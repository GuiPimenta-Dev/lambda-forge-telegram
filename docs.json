[
  {
    "url": "https://docs.lambda-forge.com/home/docs-generation/",
    "title": "Docs Generation - Lambda Forge",
    "content": "Docs Generation - Lambda Forge Skip to content Lambda Forge Docs Generation Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Docs Generation Table of contents Documenting a Lambda Function Setting Up a S3 Bucket for Documentation Setting Up Documentation Endpoints Configuring the Pipelines to Generate the Docs Configuring the Staging Pipeline Configuring the Production Pipeline Deploying the Docs Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Documenting a Lambda Function Setting Up a S3 Bucket for Documentation Setting Up Documentation Endpoints Configuring the Pipelines to Generate the Docs Configuring the Staging Pipeline Configuring the Production Pipeline Deploying the Docs Generating Docs With Swagger and ReDoc Documenting a Lambda Function Inside each main.py file, you should include the Input and Output dataclasses that are going to be the entrypoint for generating the docs. Case you have an endpoint that's expecting a path parameter, you can also include it in the Path dataclass. The code snippet below demonstrates all the types of data you can expect to work with, including simple data types, lists, custom objects, optional fields, and literal types, offering a clear understanding of the input and output contracts for the API. from dataclasses import dataclass from typing import List, Optional, Literal # Define a dataclass for path parameters, useful for API endpoints requiring parameters within the URL path. @dataclass class Path: id: str # A custom object class that represents a complex type with multiple fields. @dataclass class Object: a_string: str an_int: int # The input data class represents the expected structure of the request payload. @dataclass class Input: a_string: str # A simple string input an_int: int # A simple integer input a_boolean: bool # A boolean value a_list: List[str] # A list of strings an_object: Object # An instance of the custom 'Object' class defined above a_list_of_object: List[Object] # A list containing instances of 'Object' a_literal: Literal[\"a\", \"b\", \"c\"] # A literal type, restricting values to 'a', 'b', or 'c' an_optional: Optional[str] # An optional string, which can be either a string or None # The output data class represents the endpoint's output. @dataclass class Output: pass # No fields are defined, implying the output is empty. Setting Up a S3 Bucket for Documentation Create an Amazon S3 bucket to serve as the primary storage for your documentation files. Follow these steps to create your S3 bucket: Access the AWS Management Console: Open the Amazon S3 console at https://console.aws.amazon.com/s3/. Create a New Bucket: Click on the \"Create bucket\" button. It's important to note that each bucket's name must be globally unique across all of Amazon S3. Set Bucket Name: Choose a unique and descriptive name for your bucket. This name will be crucial for accessing your documentation files. Remember, once a bucket name is set, it cannot be changed. Choose a Region: Select an AWS Region for your bucket. Choose the same region defined in your cdk.json. Configure Options: You may leave the default settings or configure additional options like versioning, logging, or add tags according to your needs. Review and Create: Before creating the bucket, review your settings. Once everything is confirmed, click \"Create bucket\". Once the bucket is created, update your cdk.json file with the bucket's name as shown below: cdk.json40 41 42 43 44 45 46 47 48 49 50... \"region\": \"us-east-2\", \"account\": \"\", \"name\": \"Lambda-Forge-Demo\", \"repo\": { \"owner\": \"$GITHUB-USER\", \"name\": \"$GITHUB-REPO\" }, \"bucket\": \"$S3-BUCKET-NAME\", \"coverage\": 80, ... Setting Up Documentation Endpoints To activate docs generation, navigate to the deploy.py file located at infra/stages/deploy.py. Configure your endpoints as illustrated in the following example. infra/stages/deploy.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23import aws_cdk as cdk from constructs import Construct from infra.stacks.lambda_stack import LambdaStack class DeployStage(cdk.Stage): def __init__(self, scope: Construct, context, **kwargs): super().__init__(scope, context.stage, **kwargs) lambda_stack = LambdaStack(self, context) # Sets up a Swagger-based public endpoint at /docs lambda_stack.services.api_gateway.create_docs(authorizer=None) # Establishes a Swagger-based private endpoint at /docs/private with the 'secret' authorizer lambda_stack.services.api_gateway.create_docs(authorizer=\"secret\", endpoint=\"/docs/private\") # Configures a Redoc-based public endpoint at /docs/redoc lambda_stack.services.api_gateway.create_docs(authorizer=None, endpoint=\"/docs/redoc\", redoc=True) # Sets up a Redoc-based private endpoint at /docs/private/redoc with the 'secret' authorizer lambda_stack.services.api_gateway.create_docs(authorizer=\"secret\", endpoint=\"/docs/private/redoc\", redoc=True) This configuration enables both public and private documentation endpoints using Swagger and Redoc, making your API's documentation accessible and versatile. Configuring the Pipelines to Generate the Docs Given the development stage is designed for fast deployment, and serves as a sandbox environment, Lambda Forge does not generate documentation for the dev environment by default. However, if you wish to include docs generation in your development workflow, replicating the following steps for the dev environment should effectively enable this functionality. Because the project was started with the --no-docs flag, it currently lacks the validate_docs and generate_docs steps in both the Staging and Production pipelines. In essence, the validate_docs step ensures that all files intended for documentation are correctly configured with the necessary data classes. This step checks for completeness and accuracy in the documentation's underlying structure. On the other hand, the generate_docs step takes on the role of creating the documentation artifact itself and deploying it to the S3 bucket configured on the cdk.json file. To incorporate docs generation into your project, you'll need to modify your stack configurations. Specifically, you should enable the validate_docs and generate_docs steps within your CI/CD pipeline configurations for both Staging and Production environments. Configuring the Staging Pipeline To turn on documentation generation for the staging environment, add validate_docs in the pipeline's pre-execution phase and generate_docs post-deployment. infra/stacks/staging_stack.py39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # pre unit_tests = steps.run_unit_tests() coverage = steps.run_coverage() validate_docs = steps.validate_docs() validate_integration_tests = steps.validate_integration_tests() # post generate_docs = steps.generate_docs() integration_tests = steps.run_integration_tests() pipeline.add_stage( DeployStage(self, context), pre=[ unit_tests, coverage, validate_integration_tests, validate_docs, # Validate docs enabled ], post=[integration_tests, generate_docs], # Generate docs enabled ) The pipeline configuration will change for the staging environment. The following diagram illustrates the adjusted setup. graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> UnitTests[Unit Tests] Assets --> Coverage Assets --> ValidateDocs[Validate Docs] Assets --> ValidateIntegrationTests[Validate Integration Tests] UnitTests --> Deploy Coverage --> Deploy ValidateDocs --> Deploy ValidateIntegrationTests --> Deploy Deploy --> IntegrationTests[Integration Tests] Deploy --> GenerateDocs[Generate Docs] Configuring the Production Pipeline Similarly for the Production stack, ensure that validate_docs and generate_docs are enabled. infra/stacks/prod_stack.py43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # pre unit_tests = steps.run_unit_tests() coverage = steps.run_coverage() validate_docs = steps.validate_docs() validate_integration_tests = steps.validate_integration_tests() # post integration_tests = steps.run_integration_tests() pipeline.add_stage( DeployStage(self, context.staging), pre=[ unit_tests, coverage, validate_integration_tests, validate_docs, # Validate docs enabled ], post=[integration_tests], ) # post generate_docs = steps.generate_docs() pipeline.add_stage( DeployStage(self, context), post=[generate_docs], # Generate docs enabled ) The pipeline configuration will also change change for the staging environment. The following diagram illustrates the adjusted setup. graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> UnitTests[Unit Tests] Assets --> Coverage Assets --> ValidateDocs[Validate Docs] Assets --> ValidateIntegrationTests[Validate Integration Tests] UnitTests --> DeployStaging[Deploy Staging] Coverage --> DeployStaging ValidateDocs --> DeployStaging ValidateIntegrationTests --> DeployStaging DeployStaging --> IntegrationTests[Integration Tests] IntegrationTests --> DeployProduction[Deploy Production] DeployProduction --> GenerateDocs[Generate Docs] Deploying the Docs At this point, we have all the necessary components to automatically generate our docs. To proceed, commit your changes and push them to GitHub using the following commands: # Send your changes to stage git add . # Commit with a descriptive message git commit -m \"Activating docs for Staging and Production environments\" # Push changes to the 'dev' branch git push origin dev # Merge 'dev' into 'staging' and push git checkout staging git merge dev git push origin staging # Finally, merge 'staging' into 'main' and push git checkout main git merge staging git push origin main After the pipeline completes successfully, the documentation for your API's endpoints will be available through the URLs set up in the DeployStage class. This documentation offers detailed insights into the endpoints, including their request formats, response structures, and available query parameters. For easy access, the documentation for public endpoints in each environment is provided at: Staging Environment: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs/redoc Production Environment: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs/redoc Accessing the private endpoints, /docs/private for Swagger and /docs/private/redoc for Redoc, necessitates the inclusion of the security token generated by the secret authorizer, as specified in the authorizers section. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/stock-price-tracker/",
    "title": "Stock Price Tracker - Lambda Forge",
    "content": "Stock Price Tracker - Lambda Forge Skip to content Lambda Forge Stock Price Tracker Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Stock Price Tracker with Event Bridge, SQS, Dynamo DB and SES Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/tests-with-lambda-forge/",
    "title": "Tests with Lambda Forge - Lambda Forge",
    "content": "Tests with Lambda Forge - Lambda Forge Skip to content Lambda Forge Tests with Lambda Forge Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Effective Software Testing with Lambda Forge Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/oauth2-authentication/",
    "title": "OAuth2 Authentication - Lambda Forge",
    "content": "OAuth2 Authentication - Lambda Forge Skip to content Lambda Forge OAuth2 Authentication Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License OAuth2 Authentication with AWS Cognito Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/introduction/",
    "title": "Introduction - Lambda Forge",
    "content": "Introduction - Lambda Forge Lambda Forge Introduction Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Introduction In this guide, we'll take you on a journey through the development process with Lambda Forge, illustrating the progression of projects through a hands-on, step-by-step approach within a unified codebase. Our methodology employs an incremental build strategy, where each new feature enhances the foundation laid by preceding projects, ensuring a cohesive and scalable architecture without duplicating efforts. To keep our focus sharp on AWS resources and Lambda Forge architecture, we'll skip over the detailed discussion of unit and integration tests here. However, for those eager to dive deeper into testing methodologies, there is an insightful article called Effective Software Testing with Lambda Forge aimed at enriching your understanding about the subject. Our objective is to provide a streamlined and informative learning path, striking a balance between technical detail and approachability to keep you engaged without feeling overwhelmed. To enhance usability and the overall user experience, we've implemented a custom domain, https://api.lambda-forge.com, making our URLs succinct and memorable across various deployment stages: Dev - https://api.lambda-forge.com/dev Staging - https://api.lambda-forge.com/staging Prod - https://api.lambda-forge.com For those interested in customizing domain names within API Gateway, the How to Configure a Custom Domain Name for API Gateway guide offers a detailed tutorial on personalizing your project's URL. With that being said, let's forge some Lambdas! forge project lambda-forge-examples --repo-owner \"$GITHUB-OWNER\" --repo-name \"$GITHUB-REPO\" --bucket \"$S3-BUCKET\" Docs: https://api.lambda-forge.com/docs. Source code: https://github.com/GuiPimenta-Dev/lambda-forge-examples Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/locating-the-base-url/",
    "title": "Locating the Base URL - Lambda Forge",
    "content": "Locating the Base URL - Lambda Forge Skip to content Lambda Forge Locating the Base URL Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Finding the API Gateway Base URL This guide will walk you through the steps to locate the base URL for the API Gateway, essential for interacting with your deployed functions. Our focus will be on the function named Staging-Lambda-Forge-Demo-HelloWorld. First, navigate to the function in question. Then, access Configurations -> Triggers to uncover the URL generated upon deployment. For the purposes of our tutorial, the relevant URL is as follows: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/hello_world The BASE URL, vital for API interactions, is identified as the URL segment before the /hello_world endpoint. For our example, it's: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging With the base URL now in your possession, you're well-equipped to begin integrating your services, paving the way for seamless communication and functionality between your applications and the AWS infrastructure. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/multi-stage-environments/",
    "title": "Multi-Stage Environments - Lambda Forge",
    "content": "Multi-Stage Environments - Lambda Forge Skip to content Lambda Forge Multi-Stage Environments Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Multi-Stage Environments Table of contents Development Environment Configuring the Development Environment Development Pipeline Workflow Staging Environment Configuring the Staging Environment Deploying the Staging Environment Production Environment Configuring the Production Environment Deploying the Production Environment Overview Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Development Environment Configuring the Development Environment Development Pipeline Workflow Staging Environment Configuring the Staging Environment Deploying the Staging Environment Production Environment Configuring the Production Environment Deploying the Production Environment Overview Multi-Stage Environments With AWS CodePipeline In practical scenarios, it is highly recommended to adopt a multi-stage development approach. This strategy allows you to freely develop and test your code in isolated environments without affecting your live production environment and, consequently, the real-world users of your application. In Lambda Forge, the pipelines for development, staging, and production are meticulously organized within distinct files, found at infra/stacks/dev_stack.py, infra/stacks/staging_stack.py, and infra/stacks/prod_stack.py, respectively. Each stage is designed to operate with its own set of isolated resources, to ensure that changes in one environment do not inadvertently affect another. Note Lambda Forge provides a suggested pipeline configuration for each stage of deployment. You're encouraged to customize these pipelines to fit your project's needs. Whether adding new steps, adjusting existing ones, reordering or even removing some of them. Development Environment The Development environment is where the initial coding and feature implementation occur, allowing developers to make frequent changes and test new ideas in an isolated environment. This environment is strategically structured to facilitate rapid deployments, allowing new features to be rolled out directly without undergoing any preliminary validation steps. It functions essentially as a sandbox environment, providing developers with a space to both develop and test new features in a fast-paced and flexible setting. This approach enables immediate feedback and iterative improvements, streamlining the development process. Configuring the Development Environment This section details the setup process for the development environment. infra/stacks/dev_stack.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36import aws_cdk as cdk from aws_cdk import pipelines as pipelines from aws_cdk.pipelines import CodePipelineSource from constructs import Construct from lambda_forge import context from infra.stages.deploy import DeployStage @context(stage=\"Dev\", resources=\"dev\") class DevStack(cdk.Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs) source = CodePipelineSource.git_hub( f\"{context.repo['owner']}/{context.repo['name']}\", \"dev\" ) pipeline = pipelines.CodePipeline( self, \"Pipeline\", synth=pipelines.ShellStep( \"Synth\", input=source, install_commands=[ \"pip install lambda-forge aws-cdk-lib\", \"npm install -g aws-cdk\", ], commands=[ \"cdk synth\", ], ), pipeline_name=f\"{context.stage}-{context.name}-Pipeline\", ) pipeline.add_stage(DeployStage(self, context)) On line 10, the context decorator assigns the stage name as Dev and configures the use of resources tagged as dev in the cdk.json file. Moreover, it imports some additional configuration variables from the cdk.json file, assigning them to the argument named context. cdk.json41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 \"region\": \"us-east-2\", \"account\": \"\", \"name\": \"Lambda-Forge-Demo\", \"repo\": { \"owner\": \"$GITHUB-OWNER\", \"name\": \"$GITHUB-REPO\" }, \"bucket\": \"\", \"coverage\": 80, \"dev\": { \"arns\": {} }, \"staging\": { \"arns\": {} }, \"prod\": { \"arns\": {} } Additionally, we incorporate the source code from the dev branch hosted on GitHub into the pipeline. Subsequently, we finalize the deployment of the Lambda functions by activating the DeployStage. Development Pipeline Workflow As the deployment of the Development Environment has been covered in previous sections, we'll not revisit those steps here. However, the diagram below succinctly illustrates the pipeline configuration established within the AWS CodePipeline. graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> Deployment Staging Environment The Staging environment serves as a near-replica of the production environment, enabling thorough testing and quality assurance processes to catch any bugs or issues before they reach the end-users. Configuring the Staging Environment Let's take a deeper look in the staging configuration file. infra/stacks/staging_stack.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56import aws_cdk as cdk from aws_cdk import pipelines as pipelines from aws_cdk.pipelines import CodePipelineSource from constructs import Construct from lambda_forge import Steps, context from infra.stages.deploy import DeployStage @context(stage=\"Staging\", resources=\"staging\") class StagingStack(cdk.Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs) source = CodePipelineSource.git_hub( f\"{context.repo['owner']}/{context.repo['name']}\", \"staging\" ) pipeline = pipelines.CodePipeline( self, \"Pipeline\", synth=pipelines.ShellStep( \"Synth\", input=source, install_commands=[ \"pip install lambda-forge aws-cdk-lib\", \"npm install -g aws-cdk\", ], commands=[ \"cdk synth\", ], ), pipeline_name=f\"{context.stage}-{context.name}-Pipeline\", ) steps = Steps(self, context, source) # pre unit_tests = steps.run_unit_tests() coverage = steps.run_coverage() validate_docs = steps.validate_docs() validate_integration_tests = steps.validate_integration_tests() # post generate_docs = steps.generate_docs() integration_tests = steps.run_integration_tests() pipeline.add_stage( DeployStage(self, context), pre=[ unit_tests, coverage, validate_integration_tests, ], post=[integration_tests], ) Similar to the Dev environment, this environment is named Staging, with resources designated as staging in the cdk.json file. We also integrate the source code from the staging branch on GitHub into the pipeline. However, in contrast to Dev, the Staging environment incorporates stringent quality assurance protocols prior to deployment. Before deploying the functions, we execute all unit tests specified in the unit.py files. Additionally, we ensure that the code coverage percentage exceeds the threshold set in the cdk.json file. We also verify that every function connected to the API Gateway is subjected to at least one integration test, identified by the custom pytest.mark.integration decorator. Once all functions have been successfully deployed, we proceed to conduct integration tests as detailed in the integration.py files. Essentially, this procedure entails dispatching an HTTP request to each of the newly deployed functions and ensuring they respond with a 200 status code. Initially, the project was initiated with the --no-docs flag, resulting in the validate_docs and generate_docs steps being created but not integrated into the pipeline. We will delve into these steps in greater depth, exploring their functionality and potential benefits in the next section. Deploying the Staging Environment First let's create and push the current code to a new branch called staging. # Stage your changes git add . # Commit with a descriptive message git commit -m \"Deploying the Staging Environment\" # Create/switch to 'staging' branch. git checkout -b staging # Push 'staging' to remote. git push origin staging Next, let's deploy the staging environment with CDK, adhering to the naming conventions established by Forge: cdk deploy Staging-Lambda-Forge-Demo-Stack This command initiates the deployment process. Shortly, AWS CodePipeline will integrate a new pipeline, specifically tailored for the staging environment. The pipeline's configuration within AWS CodePipeline is depicted below, showcasing the streamlined workflow from source code to deployment: graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> UnitTests[Unit Tests] Assets --> Coverage Assets --> ValidateIntegrationTests[Validate Integration Tests] UnitTests --> Deploy Coverage --> Deploy ValidateIntegrationTests --> Deploy Deploy --> IntegrationTests[Integration Tests] The first deployment of the Staging Pipeline often results in failure, a situation that might seem alarming but is actually expected due to the sequence in which components are deployed and tested. This phenomenon occurs because the integration tests are set to execute immediately after the deployment phase. However, during the first deployment, the BASE URL hasn't been established since it's the inaugural setup of the Staging environment. Consequently, this leads to the failure of the Integration_Test phase. Note that the failure arises after the deployment phase, indicating that the Lambda functions have been successfully deployed. To address this issue, we need to set up the base URL specifically for the integration tests. Follow the guidelines provided in the Retrieving the Api Gateway Base URL article to find your base URL. Having the BASE URL, it must then be incorporated into your cdk.json configuration file under the base_url key. This adjustment ensures that all integration tests can interact with the staging environment seamlessly for automated testing. cdk.json48 49 50 \"bucket\": \"\", \"coverage\": 80, \"base_url\": \"https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging\" Once the base URL is properly configured for the integration tests, commit your changes and push the updated code to GitHub once again. Following these adjustments, the pipeline should successfully complete its run. Production Environment The Production environment represents the phase where the tested and stable version of the software is deployed. This version is accessible to end-users and operates within the live environment. It is imperative that this stage remains the most safeguarded, permitting only fully vetted and secure code to be deployed. This precaution helps in minimizing the risk of exposing end-users to bugs or undesirable functionalities, ensuring a seamless and reliable user experience. Configuring the Production Environment import aws_cdk as cdk from aws_cdk import pipelines from aws_cdk.pipelines import CodePipelineSource from constructs import Construct from lambda_forge import Steps, context, create_context from infra.stages.deploy import DeployStage @context( stage=\"Prod\", resources=\"prod\", staging=create_context(stage=\"Staging\", resources=\"staging\"), ) class ProdStack(cdk.Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs) source = CodePipelineSource.git_hub( f\"{context.repo['owner']}/{context.repo['name']}\", \"main\" ) pipeline = pipelines.CodePipeline( self, \"Pipeline\", synth=pipelines.ShellStep( \"Synth\", input=source, install_commands=[ \"pip install lambda-forge aws-cdk-lib\", \"npm install -g aws-cdk\", ], commands=[ \"cdk synth\", ], ), pipeline_name=f\"{context.stage}-{context.name}-Pipeline\", ) steps = Steps(self, context.staging, source) # pre unit_tests = steps.run_unit_tests() coverage = steps.run_coverage() validate_docs = steps.validate_docs() validate_integration_tests = steps.validate_integration_tests() # post integration_tests = steps.run_integration_tests() pipeline.add_stage( DeployStage(self, context.staging), pre=[ unit_tests, coverage, validate_integration_tests, ], post=[integration_tests], ) # post generate_docs = steps.generate_docs() pipeline.add_stage( DeployStage(self, context), post=[], ) This environment is named Prod and the resources used are provenient from the prod key in the cdk.json file. Additionally, the main branch on GitHub is being used to trigger the pipeline. Given the critical need for security and integrity in production, we replicate the staging environment, applying all tests and safeguards again before deploying the production stage. This ensures that any changes meet our high quality standards before production deployment, effectively protecting against vulnerabilities and ensuring a stable user experience. Deploying the Production Environment Firstly, commit and push your code to a new branch named main on GitHub # Stage your changes git add . # Commit with a descriptive message git commit -m \"Deploying the Production Environment\" # Create/switch to 'main' branch. git checkout -b main # Push 'main' to remote. git push origin main Following the branch setup, deploy your staging environment using the AWS CDK, adhering to the naming conventions provided by Forge. cdk deploy Prod-Lambda-Forge-Demo-Stack Executing this command initiates the creation of a new pipeline in AWS CodePipeline, designed to automate your deployment process. The following diagram visually represents the configuration established in AWS CodePipeline. graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> UnitTests[Unit Tests] Assets --> Coverage Assets --> ValidateIntegrationTests[Validate Integration Tests] UnitTests --> DeployStaging[Deploy Staging] Coverage --> DeployStaging ValidateIntegrationTests --> DeployStaging DeployStaging --> IntegrationTests[Integration Tests] IntegrationTests --> DeployProduction[Deploy Production] Upon the successful completion of the pipeline execution, you'll be able to observe a new Lambda function ready and deployed within your AWS Lambda console To verify the url created, navigate to the newly deployed Lambda function in the AWS Lambda console. Within the function, proceed to Configurations -> Triggers. Here, you'll find the URL for the new endpoint that has been activated as part of the deployment process. For this tutorial, the endpoint URL provided is: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/hello_world Overview By adhering to the instructions outlined in this tutorial, you are now equipped with three distinct CI/CD pipelines. Each pipeline corresponds to a specific stage of the development lifecycle, directly linked to the dev, staging, and main branches in your GitHub repository. These pipelines ensure that changes made in each branch are automatically integrated and deployed to the appropriate environment, streamlining the process from development through to production. Furthermore, you have deployed three unique functions, each corresponding to a different environment: Dev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/hello_world Staging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/hello_world Prod: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/hello_world Each link directs you to the corresponding function deployed within its respective environment, demonstrating the successful separation and management of development, staging, and production stages through your CI/CD workflows. Congratulations! \ud83c\udf89 You've successfully deployed your Lambda function across three different environments using Lambda Forge! \ud83d\ude80 Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/creating-s3-buckets/",
    "title": "Creating S3 Buckets - Lambda Forge",
    "content": "Creating S3 Buckets - Lambda Forge Skip to content Lambda Forge Creating S3 Buckets Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Creating S3 Buckets Table of contents Prerequisites Step 1: Open the Amazon S3 Console Step 2: Create a New Bucket Step 3: Access Your Bucket Best Practices Making Your Bucket Public Conclusion Locating the Base URL JSON Web Tokens License Table of contents Prerequisites Step 1: Open the Amazon S3 Console Step 2: Create a New Bucket Step 3: Access Your Bucket Best Practices Making Your Bucket Public Conclusion How to Create an Amazon S3 Bucket Amazon Simple Storage Service (Amazon S3) offers industry-leading scalability, data availability, security, and performance. This tutorial will guide you through the process of creating an S3 bucket, which can be used for a wide range of applications, including website hosting, data storage, and backups. Prerequisites You should already be logged into your AWS account. Familiarity with the basic concepts of AWS S3. Step 1: Open the Amazon S3 Console Once logged in to the AWS Management Console, locate the Services menu at the top of the console. Use the search bar to find S3 or navigate through the categories to locate S3 under the Storage section. Click on S3 to open the S3 console. Step 2: Create a New Bucket In the S3 console, click the Create bucket button. This action opens a wizard to guide you through the bucket creation process. Enter the following details: Bucket name: Choose a unique name for your bucket. This name must be globally unique across all existing bucket names in Amazon S3 and cannot be changed after the bucket is created. AWS Region: Choose the same region specified in your cdk.json file. (Optional) Configure additional options such as Versioning, Server Access Logging, Tags, and Default Encryption according to your requirements. If you're unsure or new to S3, you may proceed with the default settings. Review your settings, then click Create bucket. Step 3: Access Your Bucket After creation, your new bucket will be listed in the S3 console. Click on your bucket's name to start uploading files, creating folders, or setting up permissions. Best Practices Naming Convention: Adhere to a consistent naming convention for easier management, especially if you plan to create multiple buckets. Region Selection: Align the bucket's region with your other AWS resources to reduce latency and costs. Security: By default, all S3 buckets are private. Only make a bucket public if it is intended to serve static web content. Always follow the principle of least privilege when configuring bucket permissions. Making Your Bucket Public To make your S3 bucket public and accessible to the internet, follow these steps: Navigate to the S3 console. Select the bucket you want to make public. Go to the Permissions tab. Click on Bucket Policy. Add a policy to allow public access to your bucket. Here's an example policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PublicReadGetObject\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME/*\" } ] } Conclusion You have successfully created an Amazon S3 bucket and learned how to make it public to serve static web content or other purposes. Remember to always consider security implications and follow best practices when configuring bucket permissions. Explore further S3 features such as lifecycle policies, object versioning, and cross-region replication to optimize your data storage strategy. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/pre-commit-hooks/",
    "title": "Pre-Commit Hooks - Lambda Forge",
    "content": "Pre-Commit Hooks - Lambda Forge Skip to content Lambda Forge Pre-Commit Hooks Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Pre-Commit Hooks Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/custom-codepipeline-steps/",
    "title": "Custom CodePipeline Steps - Lambda Forge",
    "content": "Custom CodePipeline Steps - Lambda Forge Skip to content Lambda Forge Custom CodePipeline Steps Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Tailoring AWS CodePipeline with Custom Steps Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/custom-domain-name/",
    "title": "Custom Domain Name - Lambda Forge",
    "content": "Custom Domain Name - Lambda Forge Skip to content Lambda Forge Custom Domain Name Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Custom Domain Name Table of contents Prerequisites Step 1: Request a Certificate in AWS Certificate Manager Step 2: Validate Your Domain Step 3: Create a Custom Domain Name in API Gateway Step 4: Configure the Base Path Mapping Step 5: Update Your DNS Records Step 6: Test Your Custom Domain Conclusion Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Prerequisites Step 1: Request a Certificate in AWS Certificate Manager Step 2: Validate Your Domain Step 3: Create a Custom Domain Name in API Gateway Step 4: Configure the Base Path Mapping Step 5: Update Your DNS Records Step 6: Test Your Custom Domain Conclusion How to Configure a Custom Domain Name for API Gateway Configuring a custom domain name for your API Gateway allows you to present a professional and brand-consistent URL to your users. This step-by-step guide will walk you through the process of setting up a custom domain name for your API Gateway in AWS. Prerequisites Before you begin, make sure you have the following: An AWS account. A deployed API in API Gateway. A registered domain name. You can use Amazon Route 53 or any other domain registrar. An SSL certificate for your custom domain name in AWS Certificate Manager (ACM). This is required for HTTPS. Step 1: Request a Certificate in AWS Certificate Manager Go to the AWS Certificate Manager: In the AWS Management Console, navigate to ACM. Request a Certificate: Click on \u201cRequest a certificate\u201d and choose \u201cRequest a public certificate\u201d. Add your domain names: Enter your custom domain name. You can add multiple names if needed. Choose validation method: You can validate your domain ownership via DNS or email. DNS validation is recommended for its simplicity and speed. Review and request: Review your details and click \u201cConfirm and request\u201d. Step 2: Validate Your Domain For DNS Validation: Add the CNAME record provided by ACM to your DNS configuration. This process varies depending on your DNS provider. For Email Validation: Check the email associated with your domain registration and follow the instructions in the email from AWS. Step 3: Create a Custom Domain Name in API Gateway Navigate to API Gateway: In the AWS Management Console, go to API Gateway. Create Custom Domain Name: Click on \u201cCustom Domain Names\u201d in the sidebar, then \u201cCreate\u201d. Configure your domain name: Enter your domain name and select the ACM certificate you created earlier. Set up Endpoint Configuration: Choose the endpoint type. You can choose from an \u201cEdge-optimized\u201d (default and recommended for global clients) or \u201cRegional\u201d (if your users are primarily in one region) endpoint. Save the Custom Domain Name: Click on \u201cSave\u201d to create your custom domain name. Step 4: Configure the Base Path Mapping Select your custom domain name: From the list of custom domain names, click on the one you just created. Create a new Base Path Mapping: Click on the \u201cBase Path Mappings\u201d section and then \u201cAdd new base path\u201d. Set up the Base Path: Choose the destination API and stage for your custom domain. The base path allows you to direct traffic to different APIs or stages from the same domain. Save your Base Path Mapping: Click on \u201cSave\u201d. Step 5: Update Your DNS Records Get the API Gateway domain name: After saving your custom domain name in API Gateway, you'll get a target domain name. This is different from your custom domain. Create a CNAME record: In your domain\u2019s DNS settings, create a CNAME record pointing your custom domain to the target domain name provided by API Gateway. Step 6: Test Your Custom Domain After your DNS changes propagate, test your custom domain by sending requests to your API through the new domain name. Conclusion You have successfully configured a custom domain name for your API Gateway. This not only enhances your API's branding but also provides a more secure and professional way to present your services to the world. If you encounter any issues, AWS documentation and support forums are great resources for troubleshooting and getting additional help. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/real-time-chat/",
    "title": "Real-Time Chat - Lambda Forge",
    "content": "Real-Time Chat - Lambda Forge Skip to content Lambda Forge Real-Time Chat Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Building a Real-Time Chat Application with WebSockets and DynamoDB in a Serverless Architecture Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/deploying-external-library-as-layers/",
    "title": "Deploying External Library as Layers - Lambda Forge",
    "content": "Deploying External Library as Layers - Lambda Forge Skip to content Lambda Forge Deploying External Library as Layers Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Deploying External Library as Layers Table of contents Setting Up The Environment Preparing the Layer Package Creating and Using the Lambda Layer Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Setting Up The Environment Preparing the Layer Package Creating and Using the Lambda Layer Deploying an External Library to AWS Lambda as a Layer This guide outlines the steps to deploy an external library, such as the qrcode library, to AWS Lambda as a Layer. AWS Lambda Layers serve as a repository for managing common code or data shared across multiple functions. By deploying the qrcode library as a Layer, the library can be utilized in various Lambda functions without needing to be included in each function's deployment package. Although the qrcode library is used as an example, this method applies universally to any library you wish to deploy in this manner. Setting Up The Environment mkdir packages cd packages Initialize a virtual environment within this directory and activate it: python3 -m venv venv source venv/bin/activate Create a directory named python within the current directory and navigate into it: mkdir python cd python It is crucial to name this directory as python as it aligns with Lambda Layers' requirements. Install the qrcode library directly into this directory: pip install qrcode -t . This command uses pip to install the library into the current python directory. After installation, you can attempt to list the directory contents. You should see the installed packages. Preparing the Layer Package Remove unnecessary files, specifically those with the .dist-info extension, to conserve space. These files are not needed for the Lambda Layer: rm -rf *dist-info Return to the parent directory: cd .. Zip the python directory, naming the zip file qr-code-lambda-package.zip: zip -r qr-code-lambda-package.zip python You should now have the zip file in your current directory. Create a s3 bucket and upload your zip file to it: aws s3 cp qr-code-lambda-package.zip s3://your-s3-bucket-name/ Verify the upload by checking the S3 bucket and ensuring the zip file is present. Note the object URL for later use. Creating and Using the Lambda Layer Navigate to the AWS Lambda console and select the Layers option. Click on Create layer and input the necessary configuration details. Choose the Upload a file from Amazon S3 option and paste the URL of the bucket containing the zip file you created earlier. Finalize by clicking \"Add\". That's it! You've successfully deployed the qrcode library to AWS Lambda as a Layer. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/web-scraper/",
    "title": "Web Scraper - Lambda Forge",
    "content": "Web Scraper - Lambda Forge Skip to content Lambda Forge Web Scraper Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Web Scraper Table of contents Dynamo DB Lambda Layers Incorporating Requests and Beautiful Soup via Public Layers Developing The Web Scraper Building a Web Scraper with Pagination Handling Using a While Loop Building a Web Scraper with Pagination Handling Using SNS Configuring The Web Scraper Scheduling Executions With Event Bridge Developing an Endpoint for Data Access Launching Our Web Scraper and Data Visualization Endpoint Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Dynamo DB Lambda Layers Incorporating Requests and Beautiful Soup via Public Layers Developing The Web Scraper Building a Web Scraper with Pagination Handling Using a While Loop Building a Web Scraper with Pagination Handling Using SNS Configuring The Web Scraper Scheduling Executions With Event Bridge Developing an Endpoint for Data Access Launching Our Web Scraper and Data Visualization Endpoint Creating a Serverless Web Scraper: Integrating DynamoDB, SNS and EventBridge In this section, we will develop a serverless web scraper designed to extract informations about books from https://books.toscrape.com/ utilizing the Requests library and Beautiful Soup. The retrieved data will be stored in DynamoDB, enabling us to perform queries via an endpoint. Additionally, we will cover how to configure our Lambda function to execute daily, ensuring our dataset remains current and accurate. Dynamo DB Considering the write access to our database will be exclusively reserved for the scraper, maintaining three separate databases for each deployment stage is unnecessary. Therefore, let's just create a singular DynamoDB table designed to serve all three environments uniformly. Instead of setting up each environment's details separately in the cdk.json file, like we did to the users table, we'll make things simpler by creating a single Books table on the AWS console and placing its ARN directly into our DynamoDB class. infra/services/dynamo_db.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18class DynamoDB: def __init__(self, scope, resources: dict) -> None: self.users_table = dynamo_db.Table.from_table_arn( scope, \"UsersTable\", resources[\"arns\"][\"users_table\"], ) self.books_table = dynamo_db.Table.from_table_arn( scope, \"BooksTable\", \"$BOOKS-TABLE-ARN\", ) Lambda Layers Another essential aspect of our project involves leveraging external libraries like requests and Beautiful Soup for our web scraping tasks. Since these libraries are not built into Python's standard library, we'll need to incorporate them into our AWS Lambda functions as Lambda Layers. Incorporating Requests and Beautiful Soup via Public Layers The requests and Beautiful Soup libraries are widely used and recognized for their utility in web scraping and data extraction tasks. Fortunately, AWS Lambda offers these libraries as public layers, simplifying the process of integrating them into your projects without the need to create custom layers. For projects utilizing Python 3.9, we can leverage the specific Amazon Resource Names (ARNs) for both requests and Beautiful Soup libraries made available through Klayers. This provides an efficient way to add these libraries to your Lambda functions. You can explore the complete list of public layers for Python 3.9 in the us-east-2 region here. Here are the ARNs you'll need: Requests: arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-requests:19 Beautiful Soup 4: arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-beautifulsoup4:7 Let's add them both to our Layers class. infra/services/layers.pyfrom aws_cdk import aws_lambda as _lambda class Layers: def __init__(self, scope) -> None: self.requests_layer = _lambda.LayerVersion.from_layer_version_arn( scope, id=\"RequestsLayer\", layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-requests:19\", ) self.bs4_layer = _lambda.LayerVersion.from_layer_version_arn( scope, id=\"BS4Layer\", layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-beautifulsoup4:7\", ) Additionally, include the libraries in the requirements.txt file. requirements.txt16 17requests==2.28.1 beautifulsoup4==4.12.3 Developing The Web Scraper Our web scraper will extract the following details: upc, title, price, category, stock, description and url. Let's create it with forge. forge function scraper --description \"Web scraper to populate Dynamo with books data\" --no-api --belongs-to books Remember, although users can access the scraper's results, the scraper itself won't serve as a direct endpoint. We've included the --no-api flag in our Forge setup to signify that this function won't be connected to the API Gateway. Its primary role is to enrich our database. Additionally, the --belongs-to flag was used to organize it within the books directory, aligning it with related functions planned for the future. Here is the structure created for the books directory: functions \u251c\u2500\u2500 books \u251c\u2500\u2500 scraper \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 unit.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py Building a Web Scraper with Pagination Handling Using a While Loop Our focus is on understanding how AWS resources are integrated with Lambda Forge, not on the intricacies of developing a web scraper. Therefore, we will not cover the source code in detail. Nevertheless, we encourage you to experiment with creating your own web scraper, as the core concepts we're discussing will remain applicable. Below, you'll find the source code accompanied by comments that explain the concepts it illustrates. functions/books/scraper/main.pyimport os import re import boto3 import requests from bs4 import BeautifulSoup BASE_URL = \"https://books.toscrape.com\" def lambda_handler(event, context): # DynamoDB table name for storing books information BOOKS_TABLE_NAME = os.environ.get(\"BOOKS_TABLE_NAME\") # Initialize a DynamoDB resource dynamodb = boto3.resource(\"dynamodb\") # Reference the DynamoDB table books_table = dynamodb.Table(BOOKS_TABLE_NAME) # Determine the URL to scrape, defaulting to BASE_URL url = event.get(\"url\") or BASE_URL while url: # Fetch and parse the webpage at the given URL response = requests.get(url) soup = BeautifulSoup(response.text, \"html.parser\") for article in soup.find_all(\"article\"): # Extract book details title = article.find(\"h3\").find(\"a\").get(\"title\").title() price = article.find(\"p\", {\"class\": \"price_color\"}).get_text()[1:] # Correct the href if it doesn't contain \"catalogue/\" href = article.find(\"h3\").find(\"a\").get(\"href\") if \"catalogue/\" not in href: href = f\"catalogue/{href}\" # Fetch and parse the book detail page url = f\"{BASE_URL}/{href}\" detail_response = requests.get(url) detail_soup = BeautifulSoup(detail_response.text, \"html.parser\") # Extract additional details from the book detail page upc = detail_soup.find(\"th\", string=\"UPC\").find_next(\"td\").get_text().strip() category = ( detail_soup.find(\"ul\", {\"class\", \"breadcrumb\"}) .find_all(\"li\")[2] .text.strip() ) stock = ( detail_soup.find(\"p\", {\"class\": \"instock availability\"}).get_text().strip() ) stock = re.search(r\"\\d+\", stock)[0] description = detail_soup.find(\"div\", {\"id\": \"product_description\"}) if description: description = description.find_next(\"p\").get_text() # Construct the item to store in DynamoDB item = { \"PK\": upc, \"category\": category, \"title\": title, \"price\": price, \"description\": description, \"stock\": stock, \"url\": url, } # Store the item in DynamoDB books_table.put_item(Item=item) # Check for and process the next page next_page = soup.find(\"li\", {\"class\": \"next\"}) if next_page: next_href = next_page.find(\"a\")[\"href\"] if \"catalogue/\" not in next_href: next_href = f\"catalogue/{next_href}\" url = f\"{BASE_URL}/{next_href}\" else: url = None Due to AWS's predefined operational constraints, Lambda functions are explicitly engineered for rapid execution, with a maximum duration limit of 15 minutes. To evaluate the efficiency of our function, we will incorporate print statements that monitor execution time throughout our local testing phase. Execution time: 1024.913999080658 seconds The execution time approaches nearly 17 minutes, exceeding the maximum duration allowed for a Lambda function. Consequently, we need to seek alternative strategies to ensure our scraper remains compliant with the limitations. Utilizing a while loop within a solitary AWS Lambda function to perform book data extraction from the website is functional yet lacks efficiency and scalability. This is particularly pertinent within the AWS ecosystem, which is rich in services tailored for distributed computing and intricate task orchestration. Building a Web Scraper with Pagination Handling Using SNS Amazon Simple Notification Service (SNS) is a fully managed messaging service provided by AWS, enabling seamless communication between distributed systems. It operates on a publish-subscribe model, where messages are published to topics and subscribers receive notifications from these topics. With support for various types of subscriptions including HTTP, SQS, Lambda, email, and SMS, SNS ensures reliable and scalable message delivery across multiple AWS regions. It also offers features like message filtering, retry mechanisms, and dead-letter queues to enhance message processing and system resilience. Instead of using a while loop to process all pages in a single function, let's design a Lambda function to process a maximum of 10 pages. After completing these pages, it should dispatch a message with the URL of the next starting page to an SNS topic. This triggers another Lambda function dedicated to harvesting book information from the subsequent 10 pages. As an initial step, we have to integrate SNS into our Services class. forge service sns A new sns.py file was created on infra/services, so create a new SNS topic on the AWS console and place it's ARN on the SNS class. infra/services/sns.pyfrom aws_cdk import aws_lambda_event_sources import aws_cdk.aws_sns as sns class SNS: def __init__(self, scope, resources, stage) -> None: self.stage = stage self.books_scraper_topic = sns.Topic.from_topic_arn( scope, \"BooksScraperTopic\", topic_arn=\"$TOPIC-ARN\", ) def create_trigger(self, topic, function, stages=None): if stages and self.stage not in stages: return sns_subscription = aws_lambda_event_sources.SnsEventSource(topic) function.add_event_source(sns_subscription) Note that the SNS class contains a handy helper method, streamlining the process of establishing triggers that connect an SNS topic to a Lambda function. Now, let's revise the original code to eliminate the while loop that processes all pages and instead publish a message to SNS containing the URL of the new starting point. functions/books/scraper/main.pyimport os import re import json import time import boto3 import requests from bs4 import BeautifulSoup BASE_URL = \"https://books.toscrape.com\" def lambda_handler(event, context): # Get the DynamoDB table name and SNS topic ARN from environment variables. BOOKS_TABLE_NAME = os.environ.get(\"BOOKS_TABLE_NAME\", \"Books\") SNS_TOPIC_ARN = os.environ.get(\"SNS_TOPIC_ARN\") # Initialize the DynamoDB and SNS clients. dynamodb = boto3.resource(\"dynamodb\") sns = boto3.client(\"sns\") # Reference the DynamoDB table. books_table = dynamodb.Table(BOOKS_TABLE_NAME) # Determine the URL to scrape, defaulting to BASE_URL try: url = json.loads(event['Records'][0]['Sns']['Message'].replace(\"'\", '\"'))[\"url\"] except: url = BASE_URL # Keep track of the number of pages processed pages_processed = 0 # Maximum number of pages to process MAX_PAGES = 10 while pages_processed < MAX_PAGES: response = requests.get(url) soup = BeautifulSoup(response.text, \"html.parser\") for article in soup.find_all(\"article\"): # Extract book details title = article.find(\"h3\").find(\"a\").get(\"title\").title() price = article.find(\"p\", {\"class\": \"price_color\"}).get_text()[1:] # Correct the href if it doesn't contain \"catalogue/\" href = article.find(\"h3\").find(\"a\").get(\"href\") if \"catalogue/\" not in href: href = f\"catalogue/{href}\" # Fetch and parse the book detail page detail_url = f\"{BASE_URL}/{href}\" detail_response = requests.get(detail_url) detail_soup = BeautifulSoup(detail_response.text, \"html.parser\") # Extract additional details from the book detail page upc = detail_soup.find(\"th\", string=\"UPC\").find_next(\"td\").get_text().strip() category = ( detail_soup.find(\"ul\", {\"class\", \"breadcrumb\"}) .find_all(\"li\")[2] .text.strip() ) description = detail_soup.find(\"div\", {\"id\": \"product_description\"}) stock = ( detail_soup.find(\"p\", {\"class\": \"instock availability\"}) .get_text().strip() ) stock = re.search(r\"\\d+\", stock)[0] if description: description = description.find_next(\"p\").get_text() # Construct the item to store in DynamoDB item = { \"PK\": upc, \"category\": category, \"title\": title, \"price\": price, \"description\": description, \"stock\": stock, \"url\": detail_url, } # Store the item in DynamoDB books_table.put_item(Item=item) # Increment the number of pages processed pages_processed += 1 # Check for the next page next_page = soup.find(\"li\", {\"class\": \"next\"}) if not next_page: break # Correct the href if it doesn't contain \"catalogue/\" next_href = next_page.find(\"a\")[\"href\"] if \"catalogue/\" not in next_href: next_href = f\"catalogue/{next_href}\" # Construct the URL for the next page url = f\"{BASE_URL}/{next_href}\" if next_page: # Publish a message to the SNS topic to process the next 10 pages sns.publish( TopicArn=SNS_TOPIC_ARN, Message=str({\"url\": url}), Subject=f\"Process next {MAX_PAGES} pages of books\", ) Let's measure how long that function took to run locally: Execution time: 167.53530287742615 seconds Fantastic, it took under 3 minutes! This approach ensures that we never exceed the 15 minutes timeout limit, as each time a new message is published to SNS, the timeout counter is refreshed, allowing continuous execution without interruption. Configuring The Web Scraper Now that we have developed our function, let's proceed to configure the necessary AWS resources for its executions on the cloud. functions/books/scraper/config.pyfrom infra.services import Services class ScraperConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Scraper\", path=\"./functions/books\", description=\"Web scraper to populate Dynamo with books data\", directory=\"scraper\", timeout=5, layers=[services.layers.requests_layer, services.layers.bs4_layer], environment={ \"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name, \"SNS_TOPIC_ARN\": services.sns.books_scraper_topic.topic_arn } ) services.dynamo_db.books_table.grant_write_data(function) services.sns.create_trigger(services.sns.books_scraper_topic, function) services.sns.books_scraper_topic.grant_publish(function) This configuration file outlines the setup and permissions for a Lambda function, detailing: Timeout: Specifies a maximum duration of 5 minutes for Lambda execution. Layers: Adds the requests and bs4 layers to the Lambda function. Environment Variables: Establishes the required environment variables for operation. DynamoDB Access: Provides the Lambda function with write access to the DynamoDB Books table. SNS Trigger: Utilizes the SNS class helper method to link an SNS topic with the production Lambda function. SNS Publishing Permissions: Empowers the Lambda function to publish messages to the books topic. Scheduling Executions With Event Bridge The current configuration file equips us to execute the Lambda function as needed. However, it necessitates manual intervention for each run, which is an impractical approach for dynamic tasks like web scraping. The crux of the issue lies in the volatile nature of our target: website data, such as book prices and inventory, can change unpredictably. To mitigate this, we must ensure our web scraper operates automatically at regular intervals, thus capturing updates without manual oversight. By leveraging AWS EventBridge, we can schedule our Lambda function to run periodically, ensuring our data collection remains current with minimal effort. To integrate AWS EventBridge for scheduling tasks, we begin by creating an EventBridge class using Forge. This is achieved with the following command: forge service event_bridge After executing the command, a new file named event_bridge.py is generated within the infra/services directory. Let's explore its contents and functionalities: infra/services/event_bridge.pyimport aws_cdk.aws_events as events import aws_cdk.aws_events_targets as targets class EventBridge: def __init__(self, scope, resources, stage) -> None: self.scope = scope self.stage = stage def create_rule(self, name, expression, target, stages=None): if stages is not None and self.stage not in stages: return events.Rule( self.scope, name, schedule=events.Schedule.expression(expression), targets=[targets.LambdaFunction(handler=target)], ) This class introduces a streamlined method for creating EventBridge rules, enabling the scheduling of Lambda function executions. Before we proceed, it's crucial to acknowledge that we're operating within a multi-stage deployment environment. Our immediate task involves configuring the Scraper function to activate based on a scheduled rule. However, a pertinent question arises: Should we initiate the triggering of three distinct functions simultaneously? Of course not, especially when considering efficiency and resource management. More precisely, is there a need for creating three scrapers when, in reality, only one is enough to populate the database? Bearing this consideration in mind, it's wise to implement a few minor adjustments. Our goal is to streamline the process, thereby avoiding the creation of unnecessary scrapers. First, let's modify the LambdaStack class to send also the context to the ScraperConfig class. infra/stacks/lambda_stack.py42 43 # Books ScraperConfig(self.services, context) Now, let's modify our configuration class to accept the context as an additional argument in its constructor. By incorporating the context, we can strategically condition the creation of the function based on the deployment stage. functions/books/scraper/config.pyfrom infra.services import Services class ScraperConfig: def __init__(self, services: Services, context) -> None: if context.stage != \"Prod\": return function = services.aws_lambda.create_function( name=\"Scraper\", path=\"./functions/books\", description=\"Web scraper to populate Dynamo with books data\", directory=\"scraper\", timeout=5, layers=[services.layers.requests_layer, services.layers.bs4_layer], environment={ \"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name, \"SNS_TOPIC_ARN\": services.sns.books_scraper_topic.topic_arn } ) services.dynamo_db.books_table.grant_write_data(function) services.sns.create_trigger(services.sns.books_scraper_topic, function) services.sns.books_scraper_topic.grant_publish(function) services.event_bridge.create_rule( name=\"ScraperRule\", expression=\"cron(0 12 ? * * *)\", target=function, ) The cron expression cron(0 12 ? * * *) configures a schedule to initiate an action every day at 12 PM UTC. Now, we're streamlining our deployment by creating the Lambda function and its essential resources exclusively for the staging environment that will be actively utilized. Developing an Endpoint for Data Access Let's create an endpoint that returns all the stored data from our database or allows filtering by category, facilitating easy access and manipulation of the data. forge function list_books --method \"GET\" --description \"A function to fetch books from DynamoDB, optionally filtered by category.\" --belongs-to books --public The file has been created within the books directory, as initially planned. functions \u251c\u2500\u2500 books \u251c\u2500\u2500 list_books \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 integration.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 unit.py \u251c\u2500\u2500 scraper \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 unit.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py To optimize data retrieval by category from our DynamoDB table, we need to create a Global Secondary Index (GSI) on the Books Table. This index enables efficient querying and filtering of data based on the category attribute, without the need for scanning the entire table. Go to the DynamoDB section within the AWS Management Console and select the Books Table. Click on the Indexes tab next to the table details, then press Create index. In the creation form, set the partition key to your category column. Name your index as CategoryIndex. After configuring these details, review your settings and confirm by clicking Create index. Having established our index, we can utilize it to precisely and efficiently fetch data by category when needed, significantly optimizing our query performance. functions/books/list_books/main.pyimport json import os from dataclasses import dataclass from typing import List, Optional import boto3 from boto3.dynamodb.conditions import Key @dataclass class Input: category: Optional[str] @dataclass class Book: id: str title: str price: str category: str stock: str description: str url: str @dataclass class Output: data: List[Book] def lambda_handler(event, context): # Initialize a DynamoDB client dynamodb = boto3.resource(\"dynamodb\") # Get the name of the table from the environment variable BOOKS_TABLE_NAME = os.environ[\"BOOKS_TABLE_NAME\"] # Create a DynamoDB table resource table = dynamodb.Table(BOOKS_TABLE_NAME) # Check if a category is specified in the query string parameters category = ( event[\"queryStringParameters\"].get(\"category\") if event[\"queryStringParameters\"] else None ) processed_items = [] last_evaluated_key = None # Handle pagination while True: scan_kwargs = {} if category: scan_kwargs.update({ 'IndexName': \"CategoryIndex\", 'KeyConditionExpression': Key(\"category\").eq(category.title()) }) if last_evaluated_key: scan_kwargs['ExclusiveStartKey'] = last_evaluated_key if category: response = table.query(**scan_kwargs) else: response = table.scan(**scan_kwargs) items = response.get(\"Items\", []) # Renaming 'PK' attribute to 'id' in each item processed_items.extend( [{\"id\": item[\"PK\"], **{k: v for k, v in item.items() if k != \"PK\"}} for item in items] ) last_evaluated_key = response.get('LastEvaluatedKey') if not last_evaluated_key: break return {\"statusCode\": 200, \"body\": json.dumps({\"data\": processed_items})} Now Let's configure the function functions/books/list_books/config.pyfrom infra.services import Services class ListBooksConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"ListBooks\", path=\"./functions/books\", description=\"A function to fetch books from DynamoDB, optionally filtered by category.\", directory=\"list_books\", environment={\"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name}, ) services.api_gateway.create_endpoint(\"GET\", \"/books\", function, public=True) services.dynamo_db.books_table.grant_read_data(function) services.dynamo_db.add_query_permission(services.dynamo_db.books_table, function) In addition to the foundational setup facilitated by Forge, this configuration file plays a crucial role in further customizing our function. It specifically focuses on defining environment variables and granting read permissions to the function for accessing the Books table. Moreover, we leverage a specialized helper method within our DynamoDB class to extend Query permissions to the Lambda function. This distinction is critical as querying entails more specific privileges beyond data reading, ensuring our function has the precise access needed for optimal operation. Launching Our Web Scraper and Data Visualization Endpoint Great, we're all set to deploy our function. Now, we'll commit and push our changes to the remote repository, allowing our CI/CD pipeline to handle the deployment seamlessly. # Add changes to the staging area git add . # Commit the changes with a descriptive message git commit -m \"Deploying Web Scraper and Data Visualization Endpoint\" # Push changes to the 'dev' branch. git push origin dev # Switch to the 'staging' branch, merge changes from 'dev', and push git checkout staging git merge dev git push origin staging # Switch to the 'main' branch, merge changes from 'staging', and push git checkout main git merge staging git push origin main Once the pipeline execution concludes, expect to see a single scraper function established. Additionally, this function will be configured with two distinct triggers: an SNS trigger and an Event Bridge trigger, each serving a unique purpose in the workflow. Now we can also test new endpoints to list the scraped data. Dev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/books Staging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/books Prod: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/books Congratulations! \ud83c\udf89 You've successfully created your first web scraper using Lambda Layers, SNS, DynamoDB and Event Bridge using Lambda Forge. \ud83d\ude80 Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/json-web-tokens/",
    "title": "JSON Web Tokens - Lambda Forge",
    "content": "JSON Web Tokens - Lambda Forge Skip to content Lambda Forge JSON Web Tokens Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens JSON Web Tokens Table of contents What is JWT? Components of JWT How JWT Authentication Works Benefits of JWT Authentication Conclusion License Table of contents What is JWT? Components of JWT How JWT Authentication Works Benefits of JWT Authentication Conclusion Demystifying JSON Web Tokens (JWT) Authentication: A Comprehensive Guide JWT (JSON Web Token) authentication is a method for securely transmitting information between parties as a JSON object. It is commonly used for implementing stateless authentication mechanisms in web applications. This article provides an overview of JWT authentication, its components, and how it works. What is JWT? JWT is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It is commonly used for authentication and information exchange in web applications. Components of JWT JWTs consist of three main parts separated by dots (.): Header: Contains metadata about the type of token and the signing algorithm being used. Payload: Contains the claims. Claims are statements about an entity (typically, the user) and additional data. Signature: Used to verify that the sender of the JWT is who it says it is and to ensure that the message wasn't changed along the way. How JWT Authentication Works The client (usually a web browser) sends authentication credentials (such as username and password) to the server. The server verifies the credentials and generates a JWT if they are valid. The server generates a JWT containing user information (claims) and signs it using a secret key. The server sends the JWT back to the client as part of the authentication response (usually in the Authorization header). The client includes the JWT in the Authorization header of subsequent requests to the server. The server verifies the JWT's signature to ensure that it hasn't been tampered with. If the signature is valid, the server extracts the claims from the JWT and authorizes the user. Benefits of JWT Authentication Stateless: JWTs are self-contained, meaning the server does not need to store session state. Scalable: Since JWTs are stateless, they can be easily scaled across multiple servers. Decentralized: JWTs can be generated and verified by different services or systems without centralized coordination. Security: JWTs can be encrypted to provide an additional layer of security. Conclusion JWT authentication is a powerful and widely used method for securing web applications. By using JWTs, developers can implement stateless authentication mechanisms that are scalable, decentralized, and secure. For more information on how to implement JWT authentication in your application, refer to the documentation of your chosen programming language or framework. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/url-shortener/",
    "title": "URL Shortener - Lambda Forge",
    "content": "URL Shortener - Lambda Forge Skip to content Lambda Forge URL Shortener Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener URL Shortener Table of contents Configuring DynamoDB Tables for Each Deployment Stage Incorporating DynamoDB Into the Service Class Implementing the Shortener Function Implementing the Redirect Function Deploying the Functions Testing The Deployment Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Configuring DynamoDB Tables for Each Deployment Stage Incorporating DynamoDB Into the Service Class Implementing the Shortener Function Implementing the Redirect Function Deploying the Functions Testing The Deployment Creating a URL Shortener Service Using DynamoDB In this section, we will explore the development of a URL shortener. This utility enables users to input a lengthy URL, which the system then compresses into a more concise version. Configuring DynamoDB Tables for Each Deployment Stage To ensure our application can operate smoothly across different environments, we'll create three separate DynamoDB tables on AWS DynamoDB console, each tailored for a distinct deployment stage: Dev-URLs, Staging-URLs and Prod-URLs. Note Throughout this tutorial, we'll utilize PK as the Partition Key for all of our DynamoDB tables. Having acquired the ARNs for each stage-specific table, our next step involves integrating these ARNs into the cdk.json file. This crucial configuration enables our Cloud Development Kit (CDK) setup to correctly reference the DynamoDB tables according to the deployment stage. Here's how to update your cdk.json file to include the DynamoDB table ARNs for development, staging, and production environments: cdk.json51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 \"dev\": { \"arns\": { \"urls_table\": \"$DEV-URLS-TABLE-ARN\" } }, \"staging\": { \"arns\": { \"urls_table\": \"$STAGING-URLS-TABLE-ARN\" } }, \"prod\": { \"arns\": { \"urls_table\": \"$PROD-URLS-TABLE-ARN\" } } Incorporating DynamoDB Into the Service Class The subsequent phase in enhancing our application involves integrating the DynamoDB service within our service layer, enabling direct communication with DynamoDB tables. To accomplish this, utilize the following command: forge service dynamo_db This command creates a new service file named dynamo_db.py within the infra/services directory. infra \u251c\u2500\u2500 services \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api_gateway.py \u251c\u2500\u2500 aws_lambda.py \u251c\u2500\u2500 dynamo_db.py \u2514\u2500\u2500 layers.py Below is the updated structure of our Service class, now including the DynamoDB service, demonstrating the integration's completion: infra/services/__init__.pyfrom infra.services.dynamo_db import DynamoDB from infra.services.api_gateway import APIGateway from infra.services.aws_lambda import AWSLambda from infra.services.layers import Layers class Services: def __init__(self, scope, context) -> None: self.api_gateway = APIGateway(scope, context) self.aws_lambda = AWSLambda(scope, context) self.layers = Layers(scope) self.dynamo_db = DynamoDB(scope, context) Here is the newly established DynamoDB class: infra/services/dynamo_db.pyfrom aws_cdk import aws_dynamodb as dynamo_db from aws_cdk import aws_iam as iam class DynamoDB: def __init__(self, scope, context: dict) -> None: # self.dynamo = dynamo_db.Table.from_table_arn( # scope, # \"Dynamo\", # context.resources[\"arns\"][\"dynamo_arn\"], # ) ... @staticmethod def add_query_permission(table, function): function.add_to_role_policy( iam.PolicyStatement( actions=[\"dynamodb:Query\"], resources=[f\"{table.table_arn}/index/*\"], ) ) Forge has already laid the groundwork by providing a commented code that outlines the structure for creating a DynamoDB table and retrieving its ARN from the cdk.json file. Additionally, it's worth noting that the DynamoDB class includes a specialized helper method aimed at streamlining the task of assigning query permissions. Let's refine the class variables to directly reference our URLs table. infra/services/dynamo_db.py 5 6 7 8 9 10 11 12class DynamoDB: def __init__(self, scope, context: dict) -> None: self.urls_table = dynamo_db.Table.from_table_arn( scope, \"URLsTable\", context.resources[\"arns\"][\"urls_table\"], ) The context.resources object on line 11 contains only the resources that are pertinent to the current stage. By tapping into this, we can dynamically tweak our AWS resources according to the specific stage we're operating in. Implementing the Shortener Function To initiate, let's develop the shortener function, which serves as the primary interface for user interaction. This function is tasked with accepting a lengthy URL from the user and providing them with its shortened counterpart in response: forge function shortener --method \"POST\" --description \"Creates a new short URL entry in DynamoDB mapped to the original one\" --belongs-to urls --public --no-tests Executing the command will result in the establishment of the following directory structure: functions \u2514\u2500\u2500 urls \u251c\u2500\u2500 shortener \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 main.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py Now, let's implement it's functionality: functions/urls/shortener/main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43from dataclasses import dataclass import hashlib import json import os import boto3 @dataclass class Input: url: str @dataclass class Output: short_url: str def lambda_handler(event, context): # Retrieve DynamoDB table name and the Base URL from environment variables. URLS_TABLE_NAME = os.environ.get(\"URLS_TABLE_NAME\") BASE_URL = os.environ.get(\"BASE_URL\") # Initialize DynamoDB resource. dynamodb = boto3.resource(\"dynamodb\") # Reference the specified DynamoDB table. urls_table = dynamodb.Table(URLS_TABLE_NAME) # Parse the URL from the incoming event's body. body = json.loads(event[\"body\"]) original_url = body[\"url\"] # Generate a URL hash. hash_object = hashlib.sha256(original_url.encode()) url_id = hash_object.hexdigest()[:6] # Store the mapping in DynamoDB. urls_table.put_item(Item={\"PK\": url_id, \"original_url\": original_url}) # Construct the shortened URL. short_url = f\"{BASE_URL}/{url_id}\" # Return success response. return {\"statusCode\": 200, \"body\": json.dumps({\"short_url\": short_url})} This code is the core of our URL shortening service. It transforms long URLs into shorter, hash-based versions, and storing this information in DynamoDB for future retrieval. Since we are operating in a multi-stage environment, this function is dynamically retrieving the BASE URL from environment variables, as shown on line 20. This approach ensures stage-specific responses, enabling seamless URL customization. To make this possible, we must incorporate the base URL into the cdk.json file and implement minor modifications. These adjustments will enable the base URL to be accessible within the config.py class, thereby allowing the function to access the appropriate base URL depending on the environment it's operating in. cdk.json51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \"dev\": { \"base_url\": \"https://api.lambda-forge.com/dev\", \"arns\": { \"urls_table\": \"$DEV-URLS-TABLE-ARN\" } }, \"staging\": { \"base_url\": \"https://api.lambda-forge.com/staging\", \"arns\": { \"urls_table\": \"$STAGING-URLS-TABLE-ARN\" } }, \"prod\": { \"base_url\": \"https://api.lambda-forge.com\", \"arns\": { \"urls_table\": \"$PROD-URLS-TABLE-ARN\" } } Note Follow the article Finding the Api Gateway Base URL to locate your own base URL in each environment. Initially, the LambdaStack class sends only the self.services as argument to the ShortenerConfig class. We must update it to also send the context parameter. This change allows the config class to access base URLs and dynamically set the correct environment variables during the function definition, enhancing its adaptability. infra/stacks/lambda_stack.py19 20 21 22 23 24 25 26 27class LambdaStack(Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs) self.services = Services(self, context) # Urls ShortenerConfig(self.services, context) To conclude, we will now proceed with configuring our Lambda function. functions/urls/config.pyfrom infra.services import Services class ShortenerConfig: def __init__(self, services: Services, context) -> None: function = services.aws_lambda.create_function( name=\"Shortener\", path=\"./functions/urls\", description=\"Creates a new short URL entry in DynamoDB mapping to the original url\", directory=\"shortener\", environment={ \"URLS_TABLE_NAME\": services.dynamo_db.urls_table.table_name, \"BASE_URL\": context.resources[\"base_url\"], }, ) services.api_gateway.create_endpoint(\"POST\", \"/urls\", function, public=True) services.dynamo_db.urls_table.grant_write_data(function) In this configuration, we specify resources according to the deployment stages of the Lambda function, setting up the DynamoDB table and API Gateway base URL accordingly. It also includes permission settings, enabling the Lambda function to write to our DynamoDB table. Implementing the Redirect Function Having established the necessary components for URL shortening, we now proceed to create a new function tasked with redirecting users from the shortened URL to its original counterpart. Begin by creating a new function: forge function redirect --method \"GET\" --description \"Redirects from the short url to the original url\" --belongs-to urls --public --no-tests The revised directory structure will appear as follows: functions \u2514\u2500\u2500 urls \u251c\u2500\u2500 redirect \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u2514\u2500\u2500 main.py \u251c\u2500\u2500 shortener \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u2514\u2500\u2500 main.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py Now, let's implement the redirect functionality. functions/urls/redirect/main.pyfrom dataclasses import dataclass import json import boto3 import os @dataclass class Path: url_id: str @dataclass class Input: pass @dataclass class Output: pass def lambda_handler(event, context): # Retrieve DynamoDB table name from environment variables. URLS_TABLE_NAME = os.environ.get(\"URLS_TABLE_NAME\") # Initialize DynamoDB resource and table reference. dynamodb = boto3.resource(\"dynamodb\") urls_table = dynamodb.Table(URLS_TABLE_NAME) # Extract shortened URL identifier from path parameters. short_url = event[\"pathParameters\"][\"url_id\"] # Retrieve the original URL using the shortened identifier. response = urls_table.get_item(Key={\"PK\": short_url}) original_url = response.get(\"Item\", {}).get(\"original_url\") # Return 404 if no URL is found for the identifier. if original_url is None: return {\"statusCode\": 404, \"body\": json.dumps({\"message\": \"URL not found\"})} # Ensure URL starts with \"http://\" or \"https://\". if not original_url.startswith(\"http\"): original_url = f\"http://{original_url}\" # Redirect to the original URL with a 301 Moved Permanently response. return {\"statusCode\": 301, \"headers\": {\"Location\": original_url}} In this Lambda function, we're essentially setting up a redirect service. When a request comes in with a short URL identifier, the function looks up this identifier in the DynamoDB table to find the corresponding original URL. If found, it redirects the user to the original URL. Next, let's move on to its configuration. functions/urls/redirect/config.pyfrom infra.services import Services class RedirectConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Redirect\", path=\"./functions/urls\", description=\"Redirects from the short url to the original url\", directory=\"redirect\", environment={ \"URLS_TABLE_NAME\": services.dynamo_db.urls_table.table_name, } ) services.api_gateway.create_endpoint(\"GET\", \"/{url_id}\", function, public=True) services.dynamo_db.urls_table.grant_read_data(function) Deploying the Functions Next, we'll commit our code and push it to GitHub, following these steps: # Send your changes to stage git add . # Commit with a descriptive message git commit -m \"URL Shortener with DynamoDB integration\" # Push changes to the 'dev' branch git push origin dev # Merge 'dev' into 'staging' and push git checkout staging git merge dev git push origin staging # Finally, merge 'staging' into 'main' and push git checkout main git merge staging git push origin main This sequence ensures our code passes through development, staging, and finally, production environments, activating our three distinct deployment pipelines. After the pipelines complete, the URL Shortener feature is available across development, staging, and production stages. Testing The Deployment Let's test our URL Shortener by shortening a lengthy URL. For demonstration purposes, we'll use the production environment, but the process remains identical for development and staging, using their respective endpoints. Execute a POST request to shorten the URL: curl --request POST \\ --url https://api.lambda-forge.com/urls \\ --header 'Content-Type: application/json' \\ --header 'accept: application/json' \\ --data '{ \"url\": \"https://public-lambda-forge-logo.s3.us-east-2.amazonaws.com/wNSN2U7n9NiAKEItWlsrcdJ0RWFyZOmbNvsc6Kht84WsWVxuBz5O.png\" }' This request generates a short URL: { \"short_url\": \"https://api.lambda-forge.com/bc23d3\" } Navigating to this URL in your browser will redirect you to the original content, showcasing our URL Shortener in action. \ud83c\udf89 Success! Our URL shortener function is now deployed and operational across all environments. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/jwt-authentication/",
    "title": "JWT Authentication - Lambda Forge",
    "content": "JWT Authentication - Lambda Forge Skip to content Lambda Forge JWT Authentication Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication JWT Authentication Table of contents Setting Up the DynamoDB Tables Implementing Password Hashing with KMS Creating a JWT Secret on Secrets Manager Using the PYJWT Public Layer Implementing the SignUp Function Implementing the SignIn Functionality Creating the JWT Authorizer Creating a Private Function Deploying the Functions Testing the Functions OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Setting Up the DynamoDB Tables Implementing Password Hashing with KMS Creating a JWT Secret on Secrets Manager Using the PYJWT Public Layer Implementing the SignUp Function Implementing the SignIn Functionality Creating the JWT Authorizer Creating a Private Function Deploying the Functions Testing the Functions Implementing a Serverless Authentication System with JWT, Dynamo DB, Secrets Manager and KMS In this section, we will develop a serverless authentication system using JWT authentication. This system effectively transmits information from the client and authenticates users to gain access to endpoints containing private information. JWT authentication is a secure method for transmitting information between parties as a JSON object. To gain a deeper understanding of JWT tokens and their functionality, you can refer to the article JSON Web Tokens. Setting Up the DynamoDB Tables To get started, we must create tables to store user credentials securely. For maximum decoupling of environments, proceed to your AWS console and create three separate tables, each designated for a specific stage: Dev-Auth, Staging-Auth and Prod-Auth. Once you have obtained the ARNs for these tables, let's integrate them into the cdk.json file within the corresponding environment. cdk.json51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 \"dev\": { \"base_url\": \"https://api.lambda-forge.com/dev\", \"arns\": { \"urls_table\": \"$DEV-URLS-TABLE-ARN\", \"images_bucket\": \"$DEV-IMAGES-BUCKET-ARN\", \"auth_table\": \"$DEV-AUTH-TABLE-ARN\" } }, \"staging\": { \"base_url\": \"https://api.lambda-forge.com/staging\", \"arns\": { \"urls_table\": \"$STAGING-URLS-TABLE-ARN\", \"images_bucket\": \"$STAGING-IMAGES-BUCKET-ARN\", \"auth_table\": \"$STAGING-AUTH-TABLE-ARN\" } }, \"prod\": { \"base_url\": \"https://api.lambda-forge.com\", \"arns\": { \"urls_table\": \"$PROD-URLS-TABLE-ARN\", \"images_bucket\": \"$PROD-IMAGES-BUCKET-ARN\", \"auth_table\": \"$PROD-AUTH-TABLE-ARN\" } } Next, we'll create a new variable class within the DynamoDB class to reference our JWT tables. infra/services/dynamo_db.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18class DynamoDB: def __init__(self, scope, context: dict) -> None: self.urls_table = dynamo_db.Table.from_table_arn( scope, \"URLsTable\", context.resources[\"arns\"][\"urls_table\"], ) self.auth_table = dynamo_db.Table.from_table_arn( scope, \"AuthTable\", context.resources[\"arns\"][\"auth_table\"], ) Implementing Password Hashing with KMS As we're dealing with sensitive data such as passwords, storing them in plain text poses a significant security risk. To mitigate this risk, we'll utilize KMS (Key Management Service), an AWS resource designed for hashing passwords and other sensitive information. To create a new KMS service, execute the following command: forge service kms This command creates a new file within the infra/services directory specifically for managing KMS keys. infra \u2514\u2500\u2500 services \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api_gateway.py \u251c\u2500\u2500 aws_lambda.py \u251c\u2500\u2500 dynamo_db.py \u251c\u2500\u2500 kms.py \u251c\u2500\u2500 layers.py \u251c\u2500\u2500 s3.py \u2514\u2500\u2500 secrets_manager.py Next, navigate to your AWS KMS console on AWS and create a new key. Then, update the KMS class with the ARN of the newly generated key. infra/services/kms.pyfrom aws_cdk import aws_kms as kms class KMS: def __init__(self, scope, context) -> None: self.auth_key = kms.Key.from_key_arn( scope, \"AuthKey\", key_arn=\"$AUTH-KEY-ARN\", ) Creating a JWT Secret on Secrets Manager To validate JWT tokens securely, a secret is essential. This secret, usually a random string, acts as a key for verifying whether the token was generated from a trusted source. It ensures that only authorized parties can generate and verify tokens, preventing unauthorized access to protected resources. By storing the secret securely, you safeguard the integrity and confidentiality of your authentication system, mitigating the risk of unauthorized access and data breaches. Having that said, navigate to AWS Secrets Manager, create a new secret and save your random string there. After obtaining the secret ARN from AWS Secrets Manager, integrate it into the Secrets Manager class. infra/services/secrets_manager.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17class SecretsManager: def __init__(self, scope, resources) -> None: self.gmail_secret = secrets_manager.Secret.from_secret_complete_arn( scope, id=\"GmailSecret\", secret_complete_arn=\"$GMAIL-SECRET-ARN\", ) self.jwt_secret = secrets_manager.Secret.from_secret_complete_arn( scope, id=\"JwtSecret\", secret_complete_arn=\"$JWT-SECRET-ARN\", ) Using the PYJWT Public Layer To hash our JWT tokens, we'll leverage the widely-used Python library called pyjwt. Due to its popularity, AWS conveniently offers it as a public layer, streamlining our authentication implementation. PYJWT: arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-PyJWT:3 Let's now create a new class variable refencing the pyjwt layer. infra/services/layers.py14 15 16 17 18 19 20 21 22 23 24 25 26 self.sm_utils_layer = _lambda.LayerVersion( scope, id='SmUtilsLayer', code=_lambda.Code.from_asset(Path.layer('layers/sm_utils')), compatible_runtimes=[_lambda.Runtime.PYTHON_3_9], description='', ) self.pyjwt_layer = _lambda.LayerVersion.from_layer_version_arn( scope, id=\"JWTLayer\", layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-PyJWT:3\", ) Don't forget to add the pyjwt layer in the requirements.txt requirements.txt15jwt==1.3.1 Implementing the SignUp Function Now that we have all the necessary components set up, it's time to develop our authentication logic. We'll begin with the signup function, which is responsible for receiving an email and a password from the user. This function will then store them in the database, ensuring that the user is unique and storing a hashed version of the password for security purposes. forge function signup --method \"POST\" --description \"Securely handle user registration with unique credentials.\" --public --belongs-to auth --no-tests --endpoint signup This command generates a new function within the auth directory. functions \u2514\u2500\u2500 auth \u251c\u2500\u2500 signup \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 main.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py The signup functionality can be implemented as follows: functions/auth/signup/main.pyimport json import os from dataclasses import dataclass import boto3 @dataclass class Input: email: str password: int @dataclass class Output: pass def encrypt_with_kms(plaintext: str, kms_key_id: str) -> str: kms_client = boto3.client(\"kms\") response = kms_client.encrypt(KeyId=kms_key_id, Plaintext=plaintext.encode()) return response[\"CiphertextBlob\"] def lambda_handler(event, context): # Retrieve the DynamoDB table name and KMS key ID from environment variables. AUTH_TABLE_NAME = os.environ.get(\"AUTH_TABLE_NAME\") KMS_KEY_ID = os.environ.get(\"KMS_KEY_ID\") # Initialize a DynamoDB resource. dynamodb = boto3.resource(\"dynamodb\") # Reference the DynamoDB table. auth_table = dynamodb.Table(AUTH_TABLE_NAME) # Parse the request body to get user data. body = json.loads(event[\"body\"]) # Verify if the user already exists. user = auth_table.get_item(Key={\"PK\": body[\"email\"]}) if user.get(\"Item\"): return { \"statusCode\": 400, \"body\": json.dumps({\"message\": \"User already exists\"}), } # Encrypt the password using KMS. encrypted_password = encrypt_with_kms(body[\"password\"], KMS_KEY_ID) # Insert the new user into the DynamoDB table. auth_table.put_item(Item={\"PK\": body[\"email\"], \"password\": encrypted_password}) # Return a successful response with the newly created user ID. return {\"statusCode\": 201} This Lambda function basically handles user signup by encrypting passwords with KMS and storing them in DynamoDB, ensuring secure user registration. With our implementation ready, let's configure it to utilize AWS resources for seamless functionality. functions/auth/signup/config.pyfrom infra.services import Services class SignUpConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"SignUp\", path=\"./functions/auth\", description=\"Securely handle user registration with unique credentials.\", directory=\"signup\", environment={ \"AUTH_TABLE_NAME\": services.dynamo_db.auth_table.table_name, \"KMS_KEY_ID\": services.kms.auth_key.key_id, }, ) services.api_gateway.create_endpoint(\"POST\", \"/signup\", function, public=True) services.dynamo_db.auth_table.grant_read_write_data(function) services.kms.auth_key.grant_encrypt(function) Implementing the SignIn Functionality Now that the signup functionality is in place, let's proceed with the implementation of the signin function. This function will handle user input of email and password, verify them against existing credentials in the database, and decrypt the encrypted password to authenticate the user. forge function signin --method \"POST\" --description \"Authenticate user login by verifying email and password against stored credentials\" --public --belongs-to auth --no-tests --endpoint signin Here's our updated folder structure: functions \u2514\u2500\u2500 auth \u251c\u2500\u2500 signin \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u2514\u2500\u2500 main.py \u251c\u2500\u2500 signup \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u2514\u2500\u2500 main.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py And now, it's implementation. functions/auth/signup/main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69import json import os from dataclasses import dataclass import boto3 import jwt import sm_utils @dataclass class Input: email: str password: str @dataclass class Output: token: str def decrypt_with_kms(ciphertext_blob: bytes, kms_key_id: str) -> str: kms_client = boto3.client(\"kms\") # Then you can pass the decoded string to the decrypt method response = kms_client.decrypt(CiphertextBlob=bytes(ciphertext_blob), KeyId=kms_key_id) return response[\"Plaintext\"].decode() def lambda_handler(event, context): # Retrieve the DynamoDB table name and KMS key ID from environment variables. AUTH_TABLE_NAME = os.environ.get(\"AUTH_TABLE_NAME\") KMS_KEY_ID = os.environ.get(\"KMS_KEY_ID\") JWT_SECRET_NAME = os.environ.get(\"JWT_SECRET_NAME\") JWT_SECRET = sm_utils.get_secret(JWT_SECRET_NAME) # Parse the request body to get user credentials. body = json.loads(event[\"body\"]) email = body[\"email\"] password = body[\"password\"] # Initialize a DynamoDB resource. dynamodb = boto3.resource(\"dynamodb\") auth_table = dynamodb.Table(AUTH_TABLE_NAME) # Retrieve user data from DynamoDB. response = auth_table.get_item(Key={\"PK\": email}) user = response.get(\"Item\") # Check if user exists. if not user: return {\"statusCode\": 401, \"body\": json.dumps({\"error\": \"User not found\"})} # Check if user exists and password matches. encrypted_password = user.get(\"password\") decrypted_password = decrypt_with_kms(encrypted_password, KMS_KEY_ID) # Compare the decrypted password with the provided one. if password == decrypted_password: # Generate JWT token status_code = 200 token = jwt.encode({\"email\": email}, JWT_SECRET, algorithm=\"HS256\") body = json.dumps({\"token\": token}) else: status_code = 401 body = json.dumps({\"error\": \"Invalid credentials\"}) return {\"statusCode\": status_code, \"body\": body} Note that upon matching the input password with the encrypted password, the email is encoded within the JWT token and returned to the client, specifically on line 62. This step is crucial for facilitating retrieval of this information at a later stage. Now, let's move on to configure the signin function. functions/auth/signup/config.pyfrom infra.services import Services class SigninConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Signin\", path=\"./functions/auth\", description=\"Authenticate user login by verifying email and password against stored credentials\", directory=\"signin\", layers=[services.layers.sm_utils_layer, services.layers.pyjwt_layer], environment={ \"AUTH_TABLE_NAME\": services.dynamo_db.auth_table.table_name, \"KMS_KEY_ID\": services.kms.auth_key.key_id, \"JWT_SECRET_NAME\": services.secrets_manager.jwt_secret.secret_name, }, ) services.api_gateway.create_endpoint(\"POST\", \"/signin\", function, public=True) services.dynamo_db.auth_table.grant_read_data(function) services.kms.auth_key.grant_decrypt(function) services.secrets_manager.jwt_secret.grant_read(function) Creating the JWT Authorizer Now that we have the signin function, it returns a token to the client, typically a frontend application, which must include this token in the headers of subsequent requests protected by the JWT authorizer. The authorizer's role is to decode if the token was generated with the same hash as its creation, and if so, decode the token and pass the email to the protected functions. With that being said, let's proceed with its implementation. forge authorizer jwt --description \"A jwt authorizer for private lambda functions\" --no-tests This command creates a new jwt authorizer under the authorizers folder. authorizers \u251c\u2500\u2500 jwt \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u2514\u2500\u2500 main.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py Now, let's proceed with the implementation. authorizers/jwt/main.pyimport os import jwt import sm_utils def lambda_handler(event, context): # Extract the JWT token from the event token = event[\"headers\"].get(\"authorization\") # Retrieve the JWT secret from Secrets Manager JWT_SECRET_NAME = os.environ.get(\"JWT_SECRET_NAME\") JWT_SECRET = sm_utils.get_secret(JWT_SECRET_NAME) try: # Decode the JWT token decoded_token = jwt.decode(token, JWT_SECRET, algorithms=[\"HS256\"]) effect = \"allow\" email = decoded_token.get(\"email\") except: effect = \"deny\" email = None # Set the decoded email as context context = {\"email\": email} # Allow access with the user's email return { \"context\": context, \"policyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"execute-api:Invoke\", \"Effect\": effect, \"Resource\": event[\"methodArn\"], } ], }, } This function attempts to decode the token received in the headers under the key authorization using the same JWT secret stored in Secrets Manager that was used during its generation. If successful, it retrieves the hashed email from the signin function and passes it as context. Now, let's set up our new JWT authorizer. authorizers/jwt/config.pyfrom infra.services import Services class JwtAuthorizerConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"JwtAuthorizer\", path=\"./authorizers/jwt\", description=\"A jwt authorizer for private lambda functions\", layers=[services.layers.sm_utils_layer, services.layers.pyjwt_layer], environment={ \"JWT_SECRET_NAME\": services.secrets_manager.jwt_secret.secret_name }, ) services.api_gateway.create_authorizer(function, name=\"jwt\", default=False) services.secrets_manager.jwt_secret.grant_read(function) Creating a Private Function Now it's time to create a simple private function that can only be acessible through requests that passes the validations made through the authorizer. forge function hello --method \"GET\" --description \"A private function\" --no-tests This command creates a standalone function in the root of the functions folder. functions \u2514\u2500\u2500 hello \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 config.py \u2514\u2500\u2500 main.py Now, let's implement a very straightforward function that should simply retrieve the email decoded by the authorizer and return it to the user. functions/hello/main.pyimport json from dataclasses import dataclass @dataclass class Input: pass @dataclass class Output: message: str def lambda_handler(event, context): email = event[\"requestContext\"][\"authorizer\"][\"email\"] return {\"statusCode\": 200, \"body\": json.dumps({\"message\": f\"Hello, {email}!\"})} Finally, it's configuration. functions/hello/config.pyfrom infra.services import Services class HelloConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Hello\", path=\"./functions/hello\", description=\"A private function\", ) services.api_gateway.create_endpoint(\"GET\", \"/hello\", function, authorizer=\"jwt\") Note that because we didn't specify the JWT authorizer as default, and this function isn't marked as public, we need to explicitly pass the authorizer's name to the create_endpoint method. Deploying the Functions Next, we'll commit our code and push it to GitHub, following these steps: # Send your changes to stage git add . # Commit with a descriptive message git commit -m \"JWT Authentication System\" # Push changes to the 'dev' branch git push origin dev # Merge 'dev' into 'staging' and push git checkout staging git merge dev git push origin staging # Finally, merge 'staging' into 'main' and push git checkout main git merge staging git push origin main This sequence ensures our code passes through development, staging, and finally, production environments, activating our three distinct deployment pipelines. After the pipelines complete, the Authentication system should be available across development, staging, and production stages. Testing the Functions Let's start by testing the signup function with the credentials below: Email: tutorial@lambda-forge.com Password: 12345678 curl --request POST \\ --url https://api.lambda-forge.com/signup \\ --header 'Content-Type: application/json' \\ --data '{ \"email\": \"tutorial@lambda-forge.com\", \"password\": \"12345678\" }' The endpoint returns a status code 201. However, if we navigate to the Prod-Auth Table on the Dynamo DB console, we'll notice that the password stored isn't simply 12345678, but rather a significantly lengthy hash string: AQICAHinYrMBzzQKgEowcHc4llDo3C5gg+cRawehAsWTMZ24iwEvX3NrQs9oYi0hD2YnB28hAAAAZjBkBgkqhkiG9w0BBwagVzBVAgEAMFAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMEeMCuyCVk4C+Nr4OAgEQgCOEKlx01+tGfqKTNXSktApuxUI31EnwzLt7GdW0wdXrT+Yu+A== This showcases the robustness of the security measures in place to safeguard passwords. Now, let's utilize the same credentials to log in: curl --request POST \\ --url https://api.lambda-forge.com/signin \\ --header 'Content-Type: application/json' \\ --data '{ \"email\": \"tutorial@lambdaforge.com\", \"password\": \"12345678\" }' The signin endpoint returns a token: { \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InR1dG9yaWFsQGxhbWJkYWZvcmdlLmNvbSJ9.ppQLiYZ-6AtHdwaCb-H-vJnjTCle9ppULqq5-TqVPjk\" } Next, let's attempt a GET request to the hello function without headers: curl --request GET \\ --url https://api.lambda-forge.com/hello This returns the message: { \"Message\": \"User is not authorized to access this resource with an explicit deny\" } However, if we pass the token generated by the signin function: curl --request GET \\ --url https://api.lambda-forge.com/hello \\ --header 'authorization: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InR1dG9yaWFsQGxhbWJkYWZvcmdlLmNvbSJ9.ppQLiYZ-6AtHdwaCb-H-vJnjTCle9ppULqq5-TqVPjk' We receive the desired output: { \"message\": \"Hello, tutorial@lambda-forge.com!\" } \ud83c\udf89 Congratulations! You've successfully implemented a JWT authentication system, securing your endpoints.\ud83d\udd12 Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/guess-the-number/",
    "title": "Guess The Number - Lambda Forge",
    "content": "Guess The Number - Lambda Forge Skip to content Lambda Forge Guess The Number Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Creating a Guess the Number Game with DynamoDB Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/data-processing-pipeline/",
    "title": "Data Processing Pipeline - Lambda Forge",
    "content": "Data Processing Pipeline - Lambda Forge Skip to content Lambda Forge Data Processing Pipeline Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Data Processing Pipeline for Twitter Streams with Kinesis Firehose, S3 and ElasticSearch Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/",
    "title": "Home - Lambda Forge",
    "content": "Home - Lambda Forge Lambda Forge Home Initializing search Home Docs Example Projects Articles License Lambda Forge Working with Lambdas has never been easier! Get started Find me on Telegram Lambda Forge in Python Available on PIP Telegram Bot Interact with the Lambda Forge bot on Telegram. Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Back to top"
  },
  {
    "url": "https://docs.lambda-forge.com/home/introduction/",
    "title": "Introduction - Lambda Forge",
    "content": "Introduction - Lambda Forge Skip to content Lambda Forge Introduction Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Introduction Table of contents Welcome to Lambda Forge Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Welcome to Lambda Forge Introduction Welcome to Lambda Forge Lambda Forge is a Python framework, built on top of AWS Cloud Development Kit, that revolutionizes the AWS Lambda deployment enabling a modular and scalable architecture with an automated CI/CD pipeline for a multi-stage environment. Additionally, it includes a Command Line Interface named Forge, designed to optimize your development workflow. Forge not only speeds up and standardizes the development process but also automatically generates comprehensive documentation for your endpoints using Swagger and Redoc. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/creating-a-hello-world/",
    "title": "Creating a Hello World - Lambda Forge",
    "content": "Creating a Hello World - Lambda Forge Skip to content Lambda Forge Creating a Hello World Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Creating a Hello World Table of contents Understanding the Function Structure Implementing the Hello World Function Configuring Your Lambda Function Dependencies The Services Class Utilizing the Services Class in config.py Deploying Your Lambda Function Push Your Code To Github Deploying the Stacks Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Understanding the Function Structure Implementing the Hello World Function Configuring Your Lambda Function Dependencies The Services Class Utilizing the Services Class in config.py Deploying Your Lambda Function Push Your Code To Github Deploying the Stacks Creating a Public Hello World Function With API Gateway Creating a public \"Hello World\" function is a fantastic way to get started with Lambda Forge. This function will serve as a simple demonstration of Lambda Forge's ability to quickly deploy serverless functions accessible via an HTTP endpoint. Here's how you can create your first public Hello World function. forge function hello_world --method \"GET\" --description \"A simple hello world\" --public This command prompts Lambda Forge to initiate a new Lambda function located in the hello_world directory. The --method parameter defines the HTTP method accessible for this function.. The --description option provides a concise summary of the function\u2019s intent, and the --public flag ensures the function is openly accessible, allowing it to be invoked by anyone who has the URL. Understanding the Function Structure When you create a new function with Lambda Forge, it not only simplifies the creation process but also sets up a robust and organized file structure for your function. This structure is designed to support best practices in software development, including separation of concerns, configuration management, and testing. Let's break down the structure of the automatically generated hello_world function: functions/ \u2514\u2500\u2500 hello_world/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 config.py \u251c\u2500\u2500 integration.py \u251c\u2500\u2500 main.py \u2514\u2500\u2500 unit.py functions/ This directory is the root folder for all your Lambda functions. Each function has its own subdirectory within this folder. hello_world/ The hello_world subdirectory contains all the necessary files for your function to run, be configured, and tested. __init__.py This file marks the directory as a Python package, allowing its modules to be imported elsewhere. config.py Holds the configuration settings for the function. These might include environment variables, resource identifiers, and other parameters critical for the function's operation. integration.py Contains integration tests that simulate the interaction of your function with external services or resources. main.py This is where the core logic of your Lambda function resides. The handler function, which AWS Lambda invokes when the function is executed, is defined here. unit.py Contains unit tests for your function. Unit tests focus on testing individual parts of the function's code in isolation, ensuring that each component behaves as expected. Implementing the Hello World Function The Lambda function's implementation should be in the main.py file. Below is an example showcasing our simple HelloWorld function: functions/hello_world/main.pyimport json from dataclasses import dataclass @dataclass class Input: pass @dataclass class Output: message: str def lambda_handler(event, context): return { \"statusCode\": 200, \"body\": json.dumps({\"message\": \"Hello World!\"}) } The Input and Output data classes are the entrypoint for the documentation creation process. However, since the project was launched with the --no-docs flag, we will temporarily skip the docs generation details. Moving forward, we've successfully implemented a straightforward lambda function that outputs a basic JSON response: {\"message\": \"Hello World!\"}. Configuring Your Lambda Function Dependencies The Services Class Within the infra/services/__init__.py file, you'll find the Services class, a comprehensive resource manager designed to streamline the interaction with AWS services. This class acts as a dependency injector, enabling the easy and efficient configuration of AWS resources directly from your config.py files. infra/services/__init__.pyfrom infra.services.api_gateway import APIGateway from infra.services.aws_lambda import AWSLambda from infra.services.layers import Layers class Services: def __init__(self, scope, context) -> None: self.api_gateway = APIGateway(scope, context) self.aws_lambda = AWSLambda(scope, context) self.layers = Layers(scope) Utilizing the Services Class in config.py In our Lambda Forge projects, the config.py file plays a crucial role in defining and configuring the dependencies required by a Lambda function. By passing an instance of Services to our configuration classes, we can seamlessly create and manage resources such as Lambda functions and API Gateway endpoints. functions/hello_world/config.pyfrom infra.services import Services class HelloWorldConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"HelloWorld\", path=\"./functions/hello_world\", description=\"A simple hello world\" ) services.api_gateway.create_endpoint(\"GET\", \"/hello_world\", function, public=True) The Forge CLI has significantly simplified the setup by automatically tailoring the function to meet our specifications. Essentially, the config.py file configures a Lambda Function to be named as HelloWorld accompanied by the description A simple hello world. Additionally, it sets up the function to respond to GET requests at the /hello_world path and designates it as a public endpoint, making it accessible without authentication. Deploying Your Lambda Function To deploy your Lambda function, you should integrate the Config class within the infra/stacks/lambda_stack.py file. The Forge CLI streamlines this process by automatically incorporating it for you. infra/stacks/lambda_stack.pyfrom aws_cdk import Stack from constructs import Construct from infra.services import Services from lambda_forge import release from functions.hello_world.config import HelloWorldConfig @release class LambdaStack(Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs) self.services = Services(self, context) # HelloWorld HelloWorldConfig(self.services) Push Your Code To Github With all the required settings now in place, we're ready to upload our code to the GitHub repository. Lambda Forge is designed to support a multi-stage deployment process, automatically creating environments for Production, Staging and Development. These environments correspond to the main, staging, and dev branches, respectively. For the sake of simplicity, we'll focus on deploying only the development branch at this moment, deferring the discussion on setting up a multi-stage environment to a future session. # Initialize the Git repository git init git add . # Commit the changes git commit -m \"Initial commit\" # Set the remote repository git remote add origin git@github.com:$GITHUB_USER/$GITHUB_REPO.git # Create, checkout, and push the 'dev' branch git checkout -b dev git push -u origin dev Deploying the Stacks Lambda Forge ensures that every resource it creates on AWS follows a naming convention that integrates the deployment stage, the project name, and the resource name. This approach guarantees a consistent and clear identification methodology throughout the project. The project name is defined within the cdk.json file, linking each resource directly to its associated project and stage for easy management and recognition. cdk.json \"region\": \"us-east-2\", \"account\": \"\", \"name\": \"Lambda-Forge-Demo\", \"repo\": { \"owner\": \"$GITHUB-OWNER\", \"name\": \"$GITHUB-REPO\" }, Deploy the Dev Stack by running the following command in your terminal: cdk deploy Dev-Lambda-Forge-Demo-Stack Following a successful deployment, a new pipeline will be created with the name Dev-Lambda-Forge-Demo-Pipeline. Access your AWS CodePipeline console to view it. In a dedicated session, we'll delve into the specifics of the pipelines generated, including a closer examination of the development pipeline. By default, Lambda Forge does not incorporate any steps for code validation in the dev pipeline. Instead, it seamlessly integrates Github with AWS CodePipeline. This means that once code is pushed to GitHub, it triggers the pipeline, leading to automatic deployment upon the completion of the execution process. After the pipeline execution concludes, proceed to your AWS Lambda console and locate the Dev-Lambda-Forge-Demo-HelloWorld function. Select the function, then navigate to Configurations -> Triggers. Here, you will be presented with a link to your newly deployed Lambda function, ready for use. For this tutorial, the Lambda function is accessible via the following URL: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/hello_world Congratulations! \ud83c\udf89 You've successfully deployed your very first Hello World function using Lambda Forge! \ud83d\ude80 Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/license/license/",
    "title": "License - Lambda Forge",
    "content": "License - Lambda Forge Lambda Forge License Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License License MIT License Copyright (c) 2024 Guilherme Alves Pimenta Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/image-to-qr-code-converter/",
    "title": "Image to QR Code Converter - Lambda Forge",
    "content": "Image to QR Code Converter - Lambda Forge Skip to content Lambda Forge Image to QR Code Converter Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Image to QR Code Converter Table of contents Incorporating S3 Into the Service Class Incorporating Secrets Manager into the Services Class Using a Non-Public Library as Lambda Layer Implementing the Function to Convert Image to QR Code Implementing the Mailer Function Mitigating Security Risks in Mailer Configuration Creating a Custom Layer to Avoid Code Duplication Refactoring The Mailer Function to Use Custom Layers Deploying The Functions Testing the Image to QR Code Conversion Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Incorporating S3 Into the Service Class Incorporating Secrets Manager into the Services Class Using a Non-Public Library as Lambda Layer Implementing the Function to Convert Image to QR Code Implementing the Mailer Function Mitigating Security Risks in Mailer Configuration Creating a Custom Layer to Avoid Code Duplication Refactoring The Mailer Function to Use Custom Layers Deploying The Functions Testing the Image to QR Code Conversion Converting Image to QR Code with AWS S3, Secrets Manager and Email Notifications In this part, we're going to cover how to make a function that turns images uploaded by users into QR codes. When a user sends a request, the image gets processed, saved on Amazon S3, and then sent to them via email so they can easily check out the results. Incorporating S3 Into the Service Class Let's start creating three distinct buckets, each dedicated to a specific stage: Dev-Lambda-Forge-Images, Staging-Lambda-Forge-Images and Prod-Lambda-Forge-Images. Note Keep in mind that your bucket name must be unique across all AWS regions. Therefore, you'll need to select distinct names for your project. Now place the arns on your cdk.json. cdk.json51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 \"dev\": { \"base_url\": \"https://api.lambda-forge.com/dev\", \"arns\": { \"urls_table\": \"$DEV-URLS-TABLE-ARN\", \"images_bucket\": \"$DEV-IMAGES-BUCKET-ARN\" } }, \"staging\": { \"base_url\": \"https://api.lambda-forge.com/staging\", \"arns\": { \"urls_table\": \"$STAGING-URLS-TABLE-ARN\", \"images_bucket\": \"$STAGING-IMAGES-BUCKET-ARN\" } }, \"prod\": { \"base_url\": \"https://api.lambda-forge.com\", \"arns\": { \"urls_table\": \"$PROD-URLS-TABLE-ARN\", \"images_bucket\": \"$PROD-IMAGES-BUCKET-ARN\" } } The next step involves integrating the S3 service into our service layer, facilitating direct communication with S3 buckets. To achieve this, execute the following command: forge service s3 This command generates a new service file named s3.py within the infra/services directory, as illustrated below: infra \u251c\u2500\u2500 services \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api_gateway.py \u251c\u2500\u2500 aws_lambda.py \u251c\u2500\u2500 dynamo_db.py \u251c\u2500\u2500 layers.py \u2514\u2500\u2500 s3.py Below showcases the updated structure of our Service class, now incorporating the S3 service, indicating the successful integration: infra/services/__init__.pyfrom infra.services.s3 import S3 from infra.services.dynamo_db import DynamoDB from infra.services.api_gateway import APIGateway from infra.services.aws_lambda import AWSLambda from infra.services.layers import Layers class Services: def __init__(self, scope, context) -> None: self.api_gateway = APIGateway(scope, context) self.aws_lambda = AWSLambda(scope, context) self.layers = Layers(scope) self.dynamo_db = DynamoDB(scope, context) self.s3 = S3(scope, context) Here is the newly established S3 class: infra/services/s3from aws_cdk import aws_s3 as s3 from aws_cdk import aws_s3_notifications class S3: def __init__(self, scope, context) -> None: # self.s3 = s3.Bucket.from_bucket_arn( # scope, # \"S3\", # bucket_arn=context.resources[\"arns\"][\"s3_arn\"], # ) ... def create_trigger(self, bucket, function, stages=None): if stages and self.context.stage not in stages: return notifications = aws_s3_notifications.LambdaDestination(function) bucket.add_event_notification(s3.EventType.OBJECT_CREATED, notifications) As seen, Forge has created the class with a helper method to streamline the creation of a trigger between a bucket and a lambda function. Let's update the class variables to directly reference our recently created bucket. infra/services/s3.py 5 6 7 8 9 10 11 12class S3: def __init__(self, scope, context: dict) -> None: self.images_bucket = s3.Bucket.from_bucket_arn( scope, \"ImagesBucket\", bucket_arn=context.resources[\"arns\"][\"images_bucket\"], ) Excellent! This approach configures our framework to utilize each ARN on its designated stage effectively. Incorporating Secrets Manager into the Services Class Since we are dealing with emails, we must use usernames and passowrd. Hardcoding email credentials directly into the code exposes them to potential breaches. To mitigate this risk, we'll implement a more secure approach using AWS Secrets Manager, a service designed to safeguard sensitive information such as secret keys. To create a new secrets manager service, simply type: forge service secrets_manager Similar to the S3 class, Forge will generate the new service file within the infra/services directory and seamlessly integrate it into the Services class. infra \u251c\u2500\u2500 services \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 api_gateway.py \u251c\u2500\u2500 aws_lambda.py \u251c\u2500\u2500 dynamo_db.py \u251c\u2500\u2500 layers.py \u251c\u2500\u2500 s3.py \u2514\u2500\u2500 secrets_manager.py Here's the newly established class: infra/services/secrets_manager.pyfrom aws_cdk import aws_secretsmanager as secrets_manager class SecretsManager: def __init__(self, scope, resources) -> None: # self.secrets_manager = secrets_manager.Secret.from_secret_complete_arn( # scope, # id=\"SecretsManager\", # secret_complete_arn=resources[\"arns\"][\"secrets_manager_arn\"], # ) pass Now, head over to the AWS Secrets Manager panel in the AWS console and create a new secret. Within this secret, store both the email address and an app password. Warning Note that you shouldn't save your regular GMAIL password; instead, use an app password. Refer to Sign in with app passwords to generate your app password. Now that we have the secret ARN in hand, let's proceed to update the Secrets Manager class accordingly. infra/services/secrets_manager.py 4 5 6 7 8 9 10 11class SecretsManager: def __init__(self, scope, resources) -> None: self.gmail_secret = secrets_manager.Secret.from_secret_complete_arn( scope, id=\"GmailSecret\", secret_complete_arn=\"$GMAIL-SECRET-ARN\", ) Using a Non-Public Library as Lambda Layer To convert the image into a qr code, we are going to use an external library called qrcode. Unlike more popular layers, we're dealing with a library for which AWS doesn't provide a public layer. To seamlessly incorporate this library, refer to the article Deploying External Layers to AWS for guidance on deploying the qrcode library. Once you obtain the ARN of your deployed Lambda layer, simply add it to the Layers class. infra/services/layers.pyfrom aws_cdk import aws_lambda as _lambda from lambda_forge import Path class Layers: def __init__(self, scope) -> None: self.qrcode_layer = _lambda.LayerVersion.from_layer_version_arn( scope, id=\"QrCodeLayer\", layer_version_arn=\"$QR-CODE-LAYER-ARN\", ) It's essential to include both libraries in our requirements.txt file to ensure they are installed when deploying our application. requirements.txt15qrcode==7.4.2 Implementing the Function to Convert Image to QR Code With our layers now set up, it's time to create our new function. forge function qrcode --method \"POST\" --description \"Converts an image into a qr code\" --belongs-to \"images\" --no-tests --public --endpoint \"images/qrcode\" We now have the following directory: functions \u2514\u2500\u2500 images \u251c\u2500\u2500 qrcode \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u2514\u2500\u2500 main.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py Let's dive into implementing this function, which will handle user input consisting of a url to convert the image parameter and an email parameter for sending notification. functions/images/img_to_qrcode/main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59import hashlib import json import os from dataclasses import dataclass from io import BytesIO import boto3 import qrcode @dataclass class Input: url: str email: str @dataclass class Output: pass def lambda_handler(event, context): # Parse the input event to get the URL of the image and the S3 bucket name body = json.loads(event[\"body\"]) url = body.get(\"url\") # Retrieve the S3 bucket name from environment variables bucket_name = os.environ.get(\"BUCKET_NAME\") # Generate QR code from the image qr = qrcode.QRCode() qr.add_data(url) qr.make() # Create an image from the QR code qr_image = qr.make_image() # Convert the QR code image to bytes qr_byte_arr = BytesIO() qr_image.save(qr_byte_arr) qr_byte_arr = qr_byte_arr.getvalue() # Create the file name with a hash based on the input URL file_name = f\"{hashlib.md5(url.encode()).hexdigest()}.jpg\" # Initialize the S3 client s3_client = boto3.client(\"s3\") # Upload the QR code image to S3 s3_client.put_object( Bucket=bucket_name, Key=file_name, Body=qr_byte_arr, ContentType=\"image/png\", Metadata={\"url\": url, \"email\": body.get(\"email\")}, ) return {\"statusCode\": 200} Essentially, our function retrieves the URL from the parameters provided by the user. It then utilizes the qrcode library to convert the URL into a QR code before storing it in the S3 bucket. Additionally, the function saves the original url along with the associated email as metadata for future reference. Now, it's configuration. functions/images/qrcode/config.pyfrom infra.services import Services class QrcodeConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Qrcode\", path=\"./functions/images\", description=\"Converts an image into a qr code\", directory=\"qrcode\", layers=[services.layers.qrcode_layer], environment={ \"BUCKET_NAME\": services.s3.images_bucket.bucket_name, }, ) services.api_gateway.create_endpoint(\"POST\", \"/images/qrcode\", function, public=True) services.s3.images_bucket.grant_write(function) Implementing the Mailer Function It's worth noting that in our previous implementation, we deliberately omitted email notifications. This exemplifies one of the advantages of serverless architecture: the ability to completely decouple functions from each other and initiate notifications through events. This is precisely the approach we're taking with the mailer function. Whenever a file is uploaded to the S3 bucket, an event will be triggered to run this Lambda function. With the assistance of metadata, the mailer Lambda function will be equipped with the necessary information to determine the appropriate email recipients for notifications. forge function mailer --description \"Sends an email based on metadata when image enters the bucket\" --belongs-to \"images\" --no-api --no-tests Here's how our updated directory looks now. functions \u251c\u2500\u2500 images \u251c\u2500\u2500 mailer \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u2514\u2500\u2500 main.py \u251c\u2500\u2500 qrcode \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u2514\u2500\u2500 main.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py Let's whip up an eye-catching HTML layout to give our email a touch of elegance. functions/images/mailer/template.html<html> <head> <style> body { font-family: Arial, sans-serif; margin: 0; padding: 0; background-color: #f4f4f4; } .container { background-color: #ffffff; margin: 10px auto; padding: 20px; max-width: 600px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); } p { font-size: 16px; line-height: 1.5; color: #555555; } .logo { display: block; margin: 0 auto 20px auto; width: 100px; height: auto; } </style> </head> <body> <div class=\"container\"> <img src=\"https://public-lambda-forge-logo.s3.us-east-2.amazonaws.com/wNSN2U7n9NiAKEItWlsrcdJ0RWFyZOmbNvsc6Kht84WsWVxuBz5O.png\" alt=\"Lambda Forge Logo\" class=\"logo\" /> <h1>Your Image Is Ready!</h1> <p>Hello,</p> <p> We're excited to let you know that your image has been processed and is now attached to this email. </p> <p>Please check the attachment to view it.</p> <p> Made with \u2764\ufe0f by <b ><a href=\"https://docs.lambda-forge.com\" style=\"color: inherit; text-decoration: none;\" >Lambda Forge</a ></b > </p> </div> </body> </html> Time to implement the mailer functionality! functions/images/mailer/main.pyimport os import smtplib from email.mime.application import MIMEApplication from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText import boto3 def lambda_handler(event, context): # Initialize the S3 client s3_client = boto3.client(\"s3\") # Fetch the SMTP details from the environment variables SMTP_HOST = os.environ[\"SMTP_HOST\"] SMTP_PORT = os.environ[\"SMTP_PORT\"] SMTP_USER = os.environ[\"SMTP_USER\"] SMTP_PASS = os.environ[\"SMTP_PASS\"] # Extract the bucket name and the object key from the event record = event[\"Records\"][0] bucket_name = record[\"s3\"][\"bucket\"][\"name\"] object_key = record[\"s3\"][\"object\"][\"key\"] # Fetch the image from S3 response = s3_client.get_object(Bucket=bucket_name, Key=object_key) # Extract the receiver email from the metadata receiver = response[\"Metadata\"][\"email\"] # Create the multipart email msg = MIMEMultipart() sender_name = \"Lambda Forge\" # Set the 'From' field, including both the name and the email: msg[\"From\"] = f\"{sender_name} <{SMTP_USER}>\" msg[\"To\"] = receiver msg[\"Subject\"] = \"Image Processed Successfully!\" # Join the current directory with the filename to get the full path of the HTML file current_directory = os.path.dirname(os.path.abspath(__file__)) html_path = os.path.join(current_directory, \"template.html\") # Read the HTML content html = open(html_path).read() msg.attach(MIMEText(html, \"html\")) # Attach the image image_data = response[\"Body\"].read() file_name = object_key.split(\"/\")[-1] part = MIMEApplication(image_data, Name=file_name) part[\"Content-Disposition\"] = f'attachment; filename=\"{file_name}\"' msg.attach(part) # Send the email via Gmail's SMTP server, or use another server if not using Gmail with smtplib.SMTP_SSL(SMTP_HOST, SMTP_PORT) as server: server.login(SMTP_USER, SMTP_PASS) server.sendmail(SMTP_USER, receiver, msg.as_string()) This function fetches essential email-sending details from environment variables such as SMTP_HOST, SMTP_PORT, SMTP_USER, and SMTP_PASS. It then retrieves the recipient's email address from the bucket's metadata and sends an email with the QR code attached. The elegance of this approach lies in its flexibility. We can incorporate multiple image processors, including tasks like image resizing, applying color filters, facial recognition, and more. None of these processors need to handle email sending directly. By simply saving the processed image inside the bucket, the corresponding functionality is seamlessly applied. Now, let's configure our Mailer function. functions/images/mailer/config.pyfrom infra.services import Services class MailerConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Mailer\", path=\"./functions/images\", description=\"Sends an email when an image enters the bucket\", directory=\"mailer\", environment={ \"SMTP_HOST\": \"smtp.gmail.com\", \"SMTP_PORT\": \"465\", \"SMTP_USER\": services.secrets_manager.gmail_secret.secret_value_from_json(\"email\").unsafe_unwrap(), \"SMTP_PASS\": services.secrets_manager.gmail_secret.secret_value_from_json(\"password\").unsafe_unwrap(), }, ) services.s3.images_bucket.grant_read(function) services.s3.create_trigger(services.s3.images_bucket, function) With our existing setup, we configure the environment variables and grant read permissions to the function for accessing the bucket. Additionally, we utilize Forge's helper method to establish a trigger that activates when an object is created in the bucket, invoking the function. Mitigating Security Risks in Mailer Configuration Although the /mailer/config.py file may seem functional, its implementation poses a significant security risk. Hardcoding credentials directly into environment variables exposes them to potential breaches, as the secret will be visible on the Lambda Function panel. To mitigate this risk, let's modify our main.py file slightly. Instead of retrieving the Gmail credentials from environment variables, we'll directly retrieve them from AWS Secrets Manager. functions/images/mailer/main.py11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31def lambda_handler(event, context): # Initialize the S3 client s3_client = boto3.client(\"s3\") # Fetch the SMTP details from the environment variables SMTP_HOST = os.environ[\"SMTP_HOST\"] SMTP_PORT = os.environ[\"SMTP_PORT\"] import json # Initialize the Secrets Manager client sm_client = boto3.client('secretsmanager') secret_name = '$SECRET-NAME' # Retrieve the secret value from Secrets Manager response = sm_client.get_secret_value(SecretId=secret_name) secret = json.loads(response['SecretString']) # Extract SMTP credentials from the secret data SMTP_USER = secret[\"email\"] SMTP_PASS = secret[\"password\"] That's quite a bit of boilerplate code for such a straightforward task! \ud83d\ude30 Considering the critical importance of security, we'll probably employ this code snippet in numerous functions. Creating a Custom Layer to Avoid Code Duplication To avoid duplicating the previous code throughout our project, let's establish a new sm_utils custom layer. This approach will streamline the process, allowing all lambda functions that need to retrieve a secret from Secrets Manager to do so with just a single line of code. Check out AWS Lambda Development with Custom Layers to delve deeper into custom layers in Lambda development. To create the new custom layer, simply type: forge layer --custom sm_utils This command creates the following directory: layers \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 sm_utils \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 sm_utils.py Additionally, a new layer has been incorporated into the Layers class. infra/services/layers 5 6 7 8 9 10 11 12 13 14 15 16 17 self.qrcode_layer = _lambda.LayerVersion.from_layer_version_arn( scope, id=\"QrCodeLayer\", layer_version_arn=\"arn:aws:lambda:us-east-2:211125768252:layer:QRCode:1\", ) self.sm_utils_layer = _lambda.LayerVersion( scope, id='SmUtilsLayer', code=_lambda.Code.from_asset(Path.layer('layers/sm_utils')), compatible_runtimes=[_lambda.Runtime.PYTHON_3_9], description='', ) Now, it's time to level up the sm_utils layer by introducing a get_secret function. This handy feature will be shared across all our Lambda functions, simplifying our codebase. layers/sm_utils/sm_utils.pyimport json import boto3 def get_secret(secret_name: str): # Initialize the Secrets Manager client sm_client = boto3.client(\"secretsmanager\") # Retrieve the secret value from Secrets Manager response = sm_client.get_secret_value(SecretId=secret_name) # Handle scenarios where the secret is stored as plain text instead of JSON. try: secret = json.loads(response[\"SecretString\"]) except json.JSONDecodeError: secret = response[\"SecretString\"] return secret Refactoring The Mailer Function to Use Custom Layers Below is the updated main.py file, now leveraging the new sm_utils layer. functions/images/mailer/main.pyimport os import smtplib from email.mime.application import MIMEApplication from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText import boto3 import sm_utils def lambda_handler(event, context): # Initialize the S3 client s3_client = boto3.client(\"s3\") # Fetch the SMTP details from the environment variables SMTP_HOST = os.environ[\"SMTP_HOST\"] SMTP_PORT = os.environ[\"SMTP_PORT\"] # Get the secret name from env variable SECRET_NAME = os.environ[\"SECRET_NAME\"] # Get the secret from sm_utils layer secret = sm_utils.get_secret(SECRET_NAME) SMTP_USER = secret[\"email\"] SMTP_PASS = secret[\"password\"] # Extract the bucket name and the object key from the event record = event[\"Records\"][0] bucket_name = record[\"s3\"][\"bucket\"][\"name\"] object_key = record[\"s3\"][\"object\"][\"key\"] # Fetch the image from S3 response = s3_client.get_object(Bucket=bucket_name, Key=object_key) # Extract the receiver email from the metadata receiver = response[\"Metadata\"][\"email\"] # Create the multipart email msg = MIMEMultipart() sender_name = \"Lambda Forge\" # Set the 'From' field, including both the name and the email: msg[\"From\"] = f\"{sender_name} <{SMTP_USER}>\" msg[\"To\"] = receiver msg[\"Subject\"] = \"Image Processed Successfully!\" # Join the current directory with the filename to get the full path of the HTML file current_directory = os.path.dirname(os.path.abspath(__file__)) html_path = os.path.join(current_directory, \"template.html\") # Read the HTML content html = open(html_path).read() msg.attach(MIMEText(html, \"html\")) # Attach the image image_data = response[\"Body\"].read() file_name = object_key.split(\"/\")[-1] part = MIMEApplication(image_data, Name=file_name) part[\"Content-Disposition\"] = f'attachment; filename=\"{file_name}\"' msg.attach(part) # Send the email via Gmail's SMTP server, or use another server if not using Gmail with smtplib.SMTP_SSL(SMTP_HOST, SMTP_PORT) as server: server.login(SMTP_USER, SMTP_PASS) server.sendmail(SMTP_USER, receiver, msg.as_string()) Now, let's adjust the configuration to accommodate the changes necessary for the function. functions/images/mailer/config.pyfrom infra.services import Services class MailerConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Mailer\", path=\"./functions/images\", description=\"Sends an email when an image enters the bucket\", directory=\"mailer\", layers=[services.layers.sm_utils_layer], environment={ \"SMTP_HOST\": \"smtp.gmail.com\", \"SMTP_PORT\": \"465\", \"SECRET_NAME\": services.secrets_manager.gmail_secret.secret_name, }, ) services.s3.images_bucket.grant_read(function) services.s3.create_trigger(services.s3.images_bucket, function) services.secrets_manager.gmail_secret.grant_read(function) Deploying The Functions Next, we'll commit our code and push it to GitHub, following these steps: # Send your changes to stage git add . # Commit with a descriptive message git commit -m \"Image to QR code converter with result being sent by email\" # Push changes to the 'dev' branch git push origin dev # Merge 'dev' into 'staging' and push git checkout staging git merge dev git push origin staging # Finally, merge 'staging' into 'main' and push git checkout main git merge staging git push origin main This process guarantees that our code transitions systematically through the development, staging, and production environments. It activates our three specialized deployment pipelines, as illustrated by the pipelines running in the accompanying image. Following the successful execution of these pipelines, the Image to QR code feature becomes accessible across the development, staging, and production stages, ensuring a seamless deployment. Testing the Image to QR Code Conversion We'll walk through testing our Image to QR Code Converter, focusing on the production environment for this demonstration. The procedure remains consistent across development and staging environments, with the only difference being the specific endpoints used. To convert an image URL into a QR code, we execute the following POST request: curl --request POST \\ --url https://api.lambda-forge.com/images/qrcode \\ --header 'Content-Type: application/json' \\ --data '{ \"url\": \"https://public-lambda-forge-logo.s3.us-east-2.amazonaws.com/wNSN2U7n9NiAKEItWlsrcdJ0RWFyZOmbNvsc6Kht84WsWVxuBz5O.png\", \"email\": \"$EMAIL\" }' Shortly after the request is made, an email is dispatched to the provided address. The email contains a QR code attachment, as seen in the illustration below: Upon scanning the QR code, the original image is displayed: \ud83c\udf89 Success! The Image to QR Code Converter function is now fully deployed and operational in all environments. \ud83d\ude80\u2728 Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/securing-endpoints/",
    "title": "Securing Endpoints - Lambda Forge",
    "content": "Securing Endpoints - Lambda Forge Skip to content Lambda Forge Securing Endpoints Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Securing Endpoints Table of contents Creating an Authorizer Authorizer Structure Implementing The Authorizer Configuring The Authorizer Adding Authorizer To Lambda Stack Creating a Private Function Implementing the Function Configuring the Function as Private Deployment Process for Both Authorizer and Function Setting a Default Authorizer Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Creating an Authorizer Authorizer Structure Implementing The Authorizer Configuring The Authorizer Adding Authorizer To Lambda Stack Creating a Private Function Implementing the Function Configuring the Function as Private Deployment Process for Both Authorizer and Function Setting a Default Authorizer Securing Endpoints Through an Authorizer In this section, we will delve into securing endpoints by introducing an intermediary function known as an authorizer which will be responsible for validating incoming requests, determining if they should be allowed to access the targeted resources. By implementing an authorizer, you can ensure that only authenticated and authorized requests are processed by your endpoints, enhancing the security and privacy of your application. In fact, Lambda Forge treats all lambda functions as private by default. That's why we had to use the --public flag when creating the previous hello world function, to make it accessible without authentication. Without this flag, we would have been required to implement an authorizer for user authentication. Creating an Authorizer First, let's begin by creating a new authorizer function with the following command: forge authorizer secret --description \"An authorizer to validate requests based on a secret present on the headers\" This command instructs the forge CLI tool to create a new authorizer under the secret directory. Authorizer Structure Authorizers, while closely resembling Lambda Functions in structure, they fulfill a distinct role. Let's examine the structure of an authorizer more closely: authorizers \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 secret \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 unit.py \u2514\u2500\u2500 utils \u2514\u2500\u2500 __init__.py authorizers/ This directory serves as the central hub for all authorizer functions, analogous to how the functions/ directory houses Lambda functions. Each distinct authorizer is allocated its own subdirectory within this folder. secret/ This subdirectory is specifically designed for developing the secret authorizer. __init__.py Marks the directory as a Python package, enabling its modules to be imported elsewhere within the project. config.py Contains the configuration settings for the authorizer, such as environmental variables and access control parameters. main.py Houses the main logic for the authorizer, detailing how incoming requests are verified. unit.py Focused on unit testing for the authorizer, these tests ensure that each part of the authorizer's code operates as expected independently. utils/ Provides utility functions that are used by the authorizers, offering common functionalities or resources that can be leveraged across various authorizers. Implementing The Authorizer Forge automatically generates a basic implementation of an AWS Lambda authorizer that checks for a secret value present on the headers to decide on granting or denying access. Warning The example below is intended solely for demonstration and learning purposes and should not be used in production environemnts. It is crucial to develop a comprehensive and secure authentication mechanism suitable for your application's security needs. authorizers/secret/main.pydef lambda_handler(event, context): # ATTENTION: The example provided below is strictly for demonstration purposes and should NOT be deployed in a production environment. # It's crucial to develop and integrate your own robust authorization mechanism tailored to your application's security requirements. # To utilize the example authorizer as a temporary placeholder, ensure to include the following header in your requests: # Header: # secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8 # Remember, security is paramount. This placeholder serves as a guide to help you understand the kind of information your custom authorizer should authenticate. # Please replace it with your secure, proprietary logic before going live. Happy coding! secret = event[\"headers\"].get(\"secret\") SECRET = \"CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8\" effect = \"allow\" if secret == SECRET else \"deny\" policy = { \"policyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"execute-api:Invoke\", \"Effect\": effect, \"Resource\": event[\"methodArn\"] } ], }, } return policy The code snippet above demonstrates that the authorizer is configured to verify the presence of a header named secret in the request, as shown below: secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8 This key serves as a simple form of authentication, granting or denying access based on its presence and accuracy in the request headers. The secret mentioned is automatically generated by Forge, meaning the specific secret you encounter during your implementation will differ from the example provided. Please be mindful of this distinction as you proceed. Configuring The Authorizer Similar to lambda functions in terms of setup, authorizers diverge in their application. Instead of establishing an endpoint on API Gateway, an authorizer is configured to control access to one or more endpoints. authorizers/secret/config.py 1 2 3 4 5 6 7 8 9 10 11 12from infra.services import Services class SecretAuthorizerConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"SecretAuthorizer\", path=\"./authorizers/secret\", description=\"An authorizer to validate requests based on a secret present on the headers\" ) services.api_gateway.create_authorizer(function, name=\"secret\") The configuration detailed above establishes a new authorizer, assigning it a unique identifier secret within the API Gateway. Adding Authorizer To Lambda Stack Similarly to the functions, an authorizer needs to be initialized within the LambdaStack class. Fortunately, Forge takes care of this automatically. infra/stacks/lambda_stack.pyfrom aws_cdk import Stack from constructs import Construct from infra.services import Services from lambda_forge import release from authorizers.secret.config import SecretAuthorizerConfig from functions.hello_world.config import HelloWorldConfig @release class LambdaStack(Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs) self.services = Services(self, context) # Authorizers SecretAuthorizerConfig(self.services) # HelloWorld HelloWorldConfig(self.services) Creating a Private Function Now let's create a new private function. forge function private --method \"GET\" --description \"A private function\" Upon creating a new function using the Forge CLI, the project's function structure is expanded to include this new function alongside the existing ones. functions \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 hello_world \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 integration.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 unit.py \u2514\u2500\u2500 private \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 config.py \u251c\u2500\u2500 integration.py \u251c\u2500\u2500 main.py \u2514\u2500\u2500 unit.py Implementing the Function Let's make some adjustments to the response returned by this Lambda function: functions/private/main.py13 14 15 16 17 18def lambda_handler(event, context): return { \"statusCode\": 200, \"body\": json.dumps({\"message\": \"Hello From Private!\"}) } Rather than displaying the message Hello World!, we will now return Hello From Private!. Additionally, let's revise the unit tests to accurately represent the modifications we've implemented in our code. functions/private/unit.pyimport json from .main import lambda_handler def test_lambda_handler(): response = lambda_handler(None, None) assert response[\"body\"] == json.dumps({\"message\": \"Hello From Private!\"}) Configuring the Function as Private To configure the function as private, we must link it to the authorizer by passing the authorizer's name, established during its creation, to the create_endpoint method. functions/private/config.pyfrom infra.services import Services class PrivateConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Private\", path=\"./functions/private\", description=\"A private function\", ) services.api_gateway.create_endpoint(\"GET\", \"/private\", function, authorizer=\"secret\") This configuration file establishes a new private function that is secured with the secrets authorizer and accessible via a GET request at the /private endpoint. Deployment Process for Both Authorizer and Function As the next step, let's proceed to upload our updates to GitHub. # Add all changes to the staging area git add . # Commit the staged changes with a clear message git commit -m \"Implemented a private function with an authorizer\" # Push the committed changes to the 'dev' branch git push origin dev This operation will automatically initiate our development pipeline. After the pipeline completes successfully, the private Lambda function becomes operational: Dev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/private Direct access to these URLs through a web browser will display an unauthorized access message: { \"Message\": \"User is not authorized to access this resource with an explicit deny\" } However, access is granted when including the necessary secret in the request header. Below is how to use curl to access the Lambda function: curl --request GET \\ --url https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/private \\ --header 'secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8' Upon running the curl command, you will receive the following response: { \"message\": \"Hello From Private!\" } This validates the functionality of our authorizer, effectively securing the private Lambda function to ensure access is only available to those possessing the correct secret header. Setting a Default Authorizer Lambda Forge automatically considers all functions as private unless specified otherwise. This means functions are generally expected to require an authorizer for access control, unless they are explicitly marked as public. To facilitate easier management and to obviate the need for specifying an authorizer for each Lambda function individually, Lambda Forge allows for the designation of a default authorizer. This default authorizer is automatically applied to all non-public Lambda functions, streamlining the configuration process for securing access. To set an authorizer as the default, you can use the default=True argument in the create_authorizer method when defining your authorizer. authorizers/secret/config.py 6 7 8 9 10 11 12 function = services.aws_lambda.create_function( name=\"SecretAuthorizer\", path=\"./authorizers/secret\", description=\"An authorizer to validate requests based on a secret present on the headers\" ) services.api_gateway.create_authorizer(function, name=\"secret\", default=True) Next, we'll update the Private Function configuration to no longer directly associate it with the secrets authorizer. functions/private/config.py 6 7 8 9 10 11 12 function = services.aws_lambda.create_function( name=\"Private\", path=\"./functions/private\", description=\"A private function\", ) services.api_gateway.create_endpoint(\"GET\", \"/private\", function) Having designated the secretauthorizer as the default, any function not explicitly linked to a particular authorizer and not flagged as public, such as this one, will inherently be protected by the secret authorizer by default. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/lambda-layers/",
    "title": "Lambda Layers - Lambda Forge",
    "content": "Lambda Layers - Lambda Forge Skip to content Lambda Forge Lambda Layers Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Lambda Layers Table of contents What Are Lambda Layers? How They Work Use Cases AWS Lambda Development with Custom Layers Creating a Custom Layer Creating a Lambda Function Utilizing the Custom Layer AWS Lambda Development with External Libraries Incorporating Requests from Public Layers Creating a Lambda Function Utilizing the Requests Library Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents What Are Lambda Layers? How They Work Use Cases AWS Lambda Development with Custom Layers Creating a Custom Layer Creating a Lambda Function Utilizing the Custom Layer AWS Lambda Development with External Libraries Incorporating Requests from Public Layers Creating a Lambda Function Utilizing the Requests Library Utilizing Lambda Layers for Code Reuse and External Library Integration What Are Lambda Layers? Lambda Layers are essentially ZIP archives containing libraries, custom runtime environments, or other dependencies. You can include these layers in your Lambda function\u2019s execution environment without having to bundle them directly with your function's deployment package. This means you can use libraries or custom runtimes across multiple Lambda functions without needing to include them in each function\u2019s codebase. How They Work When you create a Lambda function, you specify which layers to include in its execution environment. During execution, AWS Lambda configures the function's environment to include the content of the specified layers. This content is available to your function's code just as if it were included in the deployment package directly. Use Cases Sharing code: Commonly used code can be placed in a layer and shared among multiple functions. Custom runtimes: You can use layers to deploy functions in languages that AWS Lambda does not natively support by including the necessary runtime in a layer. Configuration files: Layers can be used to store configuration files that multiple functions need to access. AWS Lambda Development with Custom Layers Forge streamlines the process of creating and sharing custom layers across AWS Lambda functions, significantly simplifying code reuse and management. This section walks you through creating a custom layer using Forge, integrating it into your development workflow, and utilizing it within a Lambda function. Creating a Custom Layer To begin, execute the following command to create a custom layer named my_custom_layer: forge layer --custom my_custom_layer This command sets up a specific directory structure for your layer within your project, organizing the code efficiently: layers \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 my_custom_layer \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 my_custom_layer.py Forge not only initializes the necessary structure but also populates my_custom_layer.py with a starter function. This function acts as a blueprint for your shared code: layers/my_custom_layer/my_custom_layer.pydef hello_from_layer(): return \"Hello from my_custom_layer layer!\" Additionally, Forge sets the new custom layer in the Layers class. infra/services/layers.pyfrom aws_cdk import aws_lambda as _lambda from lambda_forge import Path class Layers: def __init__(self, scope) -> None: self.my_custom_layer = _lambda.LayerVersion( scope, id='MyCustomLayer', code=_lambda.Code.from_asset(Path.layer('layers/my_custom_layer')), compatible_runtimes=[_lambda.Runtime.PYTHON_3_9], description='', ) Traditionally, working with Lambda layers introduces complexity during development. Since Lambda layers are deployed as zip files and run within the Lambda execution environment, developers usually face challenges in utilizing these layers locally. This often leads to a disconnect between development and production environments, complicating the development process. When you create a custom layer using Forge, the new layer is automatically integrated into your local virtual environment, similar to installing an external library from pip. However, to ensure that these changes are fully recognized, you may need to reload your IDE or reselect your virtual environment. Note In case you need to reinstall the custom layers into your virtual environment, use the command: forge layers --install Creating a Lambda Function Utilizing the Custom Layer Create a new Lambda function that leverages your custom layer by running: forge function custom --method \"GET\" --description \"A function that uses my_custom_layer\" --public This command simply creates a public function named custom inside the functions directory. functions \u2514\u2500\u2500 custom \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 config.py \u251c\u2500\u2500 integration.py \u251c\u2500\u2500 main.py \u2514\u2500\u2500 unit.py Now, implement the function to utilize the custom layer: functions/layers/custom/main.pyimport json from dataclasses import dataclass import my_custom_layer @dataclass class Input: pass @dataclass class Output: message: str def lambda_handler(event, context): message = my_custom_layer.hello_from_layer() return {\"statusCode\": 200, \"body\": json.dumps({\"message\": message})} Also, update the unit tests to expect the correct output message: functions/layers/custom/unit.pyimport json from .main import lambda_handler def test_lambda_handler(): response = lambda_handler(None, None) assert response[\"body\"] == json.dumps({\"message\": \"Hello from my_custom_layer layer!\"}) Finally, configure the function to make use of the my_custom_layer layer: functions/layers/custom/config.pyfrom infra.services import Services class CustomConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"Custom\", path=\"./functions/custom\", description=\"A function to make use of the custom layer\", layers=[services.layers.my_custom_layer], ) services.api_gateway.create_endpoint(\"GET\", \"/custom\", function, public=True) Once you've committed and pushed your code to GitHub and the pipeline has successfully executed, making a GET request to the generated URL should return the following response: { \"message\": \"Hello from my_custom_layer layer!\" } The URL for this tutorial is: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/custom AWS Lambda Development with External Libraries In software development, using external libraries is a common practice to extend functionality and streamline the development process. When working with AWS Lambda, incorporating these external libraries requires integrating them as layers into our Lambda functions. To illustrate this scenario, we will develop a new lambda function aimed to parsing the data retrieved from the external API https://randomuser.me/api/, a public service for generating random fake user data. Since the requests library is not inherently included in Python, it will be necessary to integrate it as a layer in our lambda function. Incorporating Requests from Public Layers The requests library is widely used and recognized for its utility. Fortunately, AWS Lambda offers this library as public layers, simplifying the process of integrating them into your projects without the need to create custom layers. For projects utilizing Python 3.9, we can leverage the specific Amazon Resource Names (ARNs) for the requests library made available through Klayers. This provides an efficient way to add these libraries to your Lambda functions. You can explore the complete list of public layers for Python 3.9 in the us-east-2 region here. Here is the ARN you'll need: Requests: arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-requests:19 Let's proceed by manually incorporating it into our Layers class. infra/services/layers.py 8 9 10 11 12 13 14 15 16 17 18 19 20 self.my_custom_layer = _lambda.LayerVersion( scope, id='MyCustomLayer', code=_lambda.Code.from_asset(Path.layer('layers/my_custom_layer')), compatible_runtimes=[_lambda.Runtime.PYTHON_3_9], description='', ) self.requests_layer = _lambda.LayerVersion.from_layer_version_arn( scope, id=\"RequestsLayer\", layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-requests:19\", ) Additionally, include it in the requirements.txt file. requirements.txt16requests==2.28.1 Creating a Lambda Function Utilizing the Requests Library To create a Lambda function that leverages the Requests library, execute the following command: forge function external --method \"GET\" --description \"A function that uses an external library\" --public This command creates a new function named external inside the functions directory. functions \u2514\u2500\u2500 external \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 config.py \u251c\u2500\u2500 integration.py \u251c\u2500\u2500 main.py \u2514\u2500\u2500 unit.py Now, implement the function to utilize the custom layer: import json from dataclasses import dataclass import requests @dataclass class Input: pass @dataclass class Name: title: str first: str last: str @dataclass class Output: name: Name gender: str email: str def lambda_handler(event, context): result = requests.get(\"https://randomuser.me/api\").json()[\"results\"][0] data = { \"name\": result[\"name\"], \"gender\": result[\"gender\"], \"email\": result[\"email\"], } return {\"statusCode\": 200, \"body\": json.dumps({\"data\": data})} Additionally, update the unit tests to expect the correct output message: functions/layers/external/unit.pyimport json from .main import lambda_handler def test_lambda_handler(): response = lambda_handler(None, None) body = json.loads(response[\"body\"]) assert [\"name\", \"gender\", \"email\"] == list(body.keys()) Finally, configure the function to make use of the requests layer: functions/layers/custom/config.pyfrom infra.services import Services class ExternalConfig: def __init__(self, services: Services) -> None: function = services.aws_lambda.create_function( name=\"External\", path=\"./functions/external\", description=\"A function that uses an external library\", layers=[services.layers.requests_layer], ) services.api_gateway.create_endpoint(\"GET\", \"/external\", function, public=True) Once you've committed and pushed your code to GitHub and the pipeline has successfully executed, making a GET request to the generated URL should return the following response: { \"name\": { \"title\": \"str\", \"first\": \"str\", \"last\": \"str\" }, \"gender\": \"str\", \"email\": \"str\" } For this tutorial, the generated URL is: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/external Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/getting-started/",
    "title": "Getting Started - Lambda Forge",
    "content": "Getting Started - Lambda Forge Skip to content Lambda Forge Getting Started Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Getting Started Table of contents Install and Configure AWS CDK Create a GitHub Personal Access Token Store the Token on AWS Secrets Manager Create a New Directory Create a New Virtual Environment Install lambda-forge Forge CLI Verify Installation Create a New Project Project Structure Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Install and Configure AWS CDK Create a GitHub Personal Access Token Store the Token on AWS Secrets Manager Create a New Directory Create a New Virtual Environment Install lambda-forge Forge CLI Verify Installation Create a New Project Project Structure Getting Started Install and Configure AWS CDK Lambda Forge is built on top of AWS Cloud Development Kit (CDK) and it's essential for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. Execute the following commands to install the AWS CDK globally and set up your AWS credentials: npm install -g aws-cdk aws configure cdk bootstrap During the configuration, you will be prompted to enter your AWS Access Key ID, Secret Access Key, default region name, and output format. Create a GitHub Personal Access Token Lambda Forge uses CodePipeline to interact with your GitHub repository. To enable this, generate a GitHub personal access token by following these steps: Navigate to \"Developer Settings\" in your GitHub account. Select \"Personal access tokens,\" then \"Tokens (classic).\" Click \"Generate new token,\" ensuring the \"repo\" scope is selected for full control of private repositories. Complete the token generation process. You can find more informations about creating a GitHub Token here. Your token will follow this format: ghp_******************************** Store the Token on AWS Secrets Manager Save this token in AWS Secrets Manager as plain text using the exact name github-token. This specific naming is vital as it corresponds to the default identifier that the CDK looks for within your AWS account. Create a New Directory mkdir lambda_forge_demo cd lambda_forge_demo Create a New Virtual Environment python3 -m venv venv source venv/bin/activate Install lambda-forge pip install lambda-forge --extra-index-url https://pypi.org/simple --extra-index-url https://test.pypi.org/simple/ Forge CLI The Forge Command Line Interface (CLI) is a powerful, versatile tool designed to streamline the development, deployment, and management of applications. It enables developers to automate repetitive tasks, manage project configurations, and interact directly with the services and infrastructure without leaving the terminal. This CLI tool simplifies complex processes through straightforward commands, significantly reducing development time and effort. Verify Installation Having successfully installed Lambda Forge, you are now ready to explore the capabilities of the Forge CLI. Begin by entering the following command to access the comprehensive list of available options and commands: forge --help Here's a concise list of the commands supported by Forge: Commands: authorizer Create a new authorizer function Create a new function layer Create a new layer project Create a new project service Create a new AWS service For a comprehensive list of configurations that each Forge command supports, you can refer to the command line help by running: forge $COMMAND --help. Later in this tutorial, we'll delve into the specifics of each command. But for now, let's kickstart by establishing the foundation of our project. Create a New Project Start a new project named lambda-forge-demo, incorporating the --no-docs flag to bypass docs generation initially as this will be covered on a dedicated section. forge project lambda-forge-demo --repo-owner \"$GITHUB-OWNER\" --repo-name \"$GITHUB-REPO\" --no-docs Make sure to replace $GITHUB-OWNER and $GITHUB-REPO with the actual GitHub owner and the name of an empty repository. Project Structure Upon creatig your project, some directories and files are automatically generated for you. This initial structure is designed to streamline the setup process and provide a solid foundation for further development. In the upcoming sections of this tutorial, we'll explore each of these components in detail. For now, familiarize yourself with the foundational structure that should resemble the following: . \u251c\u2500\u2500 authorizers \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 functions \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 infra \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 services \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 api_gateway.py \u2502 \u2502 \u251c\u2500\u2500 aws_lambda.py \u2502 \u2502 \u2514\u2500\u2500 layers.py \u2502 \u251c\u2500\u2500 stacks \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 dev_stack.py \u2502 \u2502 \u251c\u2500\u2500 lambda_stack.py \u2502 \u2502 \u251c\u2500\u2500 prod_stack.py \u2502 \u2502 \u2514\u2500\u2500 staging_stack.py \u2502 \u2514\u2500\u2500 stages \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 deploy.py \u251c\u2500\u2500 layers \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 .coveragerc \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 app.py \u251c\u2500\u2500 cdk.json \u251c\u2500\u2500 pytest.ini \u2514\u2500\u2500 requirements.txt The cdk.json file, located at the root of your directory, serves as the central configuration hub for Lambda Forge projects. When you run the forge project command, Forge automatically applies the informed settings into the cdk.json file. cdk.json41 42 43 44 45 46 47 48 49 \"region\": \"us-east-2\", \"account\": \"\", \"name\": \"Lambda-Forge-Demo\", \"repo\": { \"owner\": \"$GITHUB-OWNER\", \"name\": \"$GITHUB-REPO\" }, \"bucket\": \"\", \"coverage\": 80, Back to top \u00a9 2024 Guilherme Alves Pimenta"
  }
]