{
  "url": "https://docs.lambda-forge.com/home/docs-generation/",
  "title": "Docs Generation - Lambda Forge",
  "content": "\n\n\n\n\n\n\n\n\n\nDocs Generation - Lambda Forge\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Skip to content\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Lambda Forge\n          \n\n\n\n            \n              Docs Generation\n            \n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Initializing search\n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        \n  \n    \n  \n  Home\n\n      \n\n\n\n          \n  \n  Docs\n\n        \n\n\n\n          \n  \n  Example Projects\n\n        \n\n\n\n          \n  \n  Articles\n\n        \n\n\n\n        \n  \n    \n  \n  License\n\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Lambda Forge\n  \n\n\n\n\n    Home\n  \n\n\n\n\n\n\n    Docs\n  \n\n\n\n\n\n            Docs\n          \n\n\n\n\n    Introduction\n  \n\n\n\n\n\n    Getting Started\n  \n\n\n\n\n\n    Creating a Hello World\n  \n\n\n\n\n\n    Securing Endpoints\n  \n\n\n\n\n\n    Lambda Layers\n  \n\n\n\n\n\n    Multi-Stage Environments\n  \n\n\n\n\n\n    Custom CodePipeline Steps\n  \n\n\n\n\n\n\n    Docs Generation\n  \n\n\n\n\n    Docs Generation\n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Documenting a Lambda Function\n    \n\n\n\n\n\n      Setting Up a S3 Bucket for Documentation\n    \n\n\n\n\n\n      Setting Up Documentation Endpoints\n    \n\n\n\n\n\n      Configuring the Pipelines to Generate the Docs\n    \n\n\n\n\n\n\n      Configuring the Staging Pipeline\n    \n\n\n\n\n\n      Configuring the Production Pipeline\n    \n\n\n\n\n\n\n\n\n      Deploying the Docs\n    \n\n\n\n\n\n\n\n\n    Pre-Commit Hooks\n  \n\n\n\n\n\n\n\n\n\n    Example Projects\n  \n\n\n\n\n\n            Example Projects\n          \n\n\n\n\n    Introduction\n  \n\n\n\n\n\n\n    Begginer\n  \n\n\n\n\n\n            Begginer\n          \n\n\n\n\n    Guess The Number\n  \n\n\n\n\n\n    URL Shortener\n  \n\n\n\n\n\n    Image to QR Code Converter\n  \n\n\n\n\n\n\n\n\n\n    Intermediate\n  \n\n\n\n\n\n            Intermediate\n          \n\n\n\n\n    JWT Authentication\n  \n\n\n\n\n\n    OAuth2 Authentication\n  \n\n\n\n\n\n    Real-Time Chat\n  \n\n\n\n\n\n    Web Scraper\n  \n\n\n\n\n\n\n\n\n\n    Advanced\n  \n\n\n\n\n\n            Advanced\n          \n\n\n\n\n    Data Processing Pipeline\n  \n\n\n\n\n\n    Stock Price Tracker\n  \n\n\n\n\n\n\n\n\n\n\n\n\n    Articles\n  \n\n\n\n\n\n            Articles\n          \n\n\n\n\n    Custom Domain Name\n  \n\n\n\n\n\n    Tests with Lambda Forge\n  \n\n\n\n\n\n    Deploying External Library as Layers\n  \n\n\n\n\n\n    Creating S3 Buckets\n  \n\n\n\n\n\n    Locating the Base URL\n  \n\n\n\n\n\n    JSON Web Tokens\n  \n\n\n\n\n\n\n\n\n    License\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Documenting a Lambda Function\n    \n\n\n\n\n\n      Setting Up a S3 Bucket for Documentation\n    \n\n\n\n\n\n      Setting Up Documentation Endpoints\n    \n\n\n\n\n\n      Configuring the Pipelines to Generate the Docs\n    \n\n\n\n\n\n\n      Configuring the Staging Pipeline\n    \n\n\n\n\n\n      Configuring the Production Pipeline\n    \n\n\n\n\n\n\n\n\n      Deploying the Docs\n    \n\n\n\n\n\n\n\n\n\nGenerating Docs With Swagger and ReDoc\nDocumenting a Lambda Function\nInside each main.py file, you should include the Input and Output dataclasses that are going to be the entrypoint for generating the docs. Case you have an endpoint that's expecting a path parameter, you can also include it in the Path dataclass.\nThe code snippet below demonstrates all the types of data you can expect to work with, including simple data types, lists, custom objects, optional fields, and literal types, offering a clear understanding of the input and output contracts for the API.\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Literal\n\n# Define a dataclass for path parameters, useful for API endpoints requiring parameters within the URL path.\n@dataclass\nclass Path:\n    id: str\n\n# A custom object class that represents a complex type with multiple fields.\n@dataclass\nclass Object:\n    a_string: str\n    an_int: int\n\n# The input data class represents the expected structure of the request payload.\n@dataclass\nclass Input:\n    a_string: str  # A simple string input\n    an_int: int  # A simple integer input\n    a_boolean: bool  # A boolean value\n    a_list: List[str]  # A list of strings\n    an_object: Object  # An instance of the custom 'Object' class defined above\n    a_list_of_object: List[Object]  # A list containing instances of 'Object'\n    a_literal: Literal[\"a\", \"b\", \"c\"]  # A literal type, restricting values to 'a', 'b', or 'c'\n    an_optional: Optional[str]  # An optional string, which can be either a string or None\n\n# The output data class represents the endpoint's output.\n@dataclass\nclass Output:\n    pass # No fields are defined, implying the output is empty.\n\nSetting Up a S3 Bucket for Documentation\nCreate an Amazon S3 bucket to serve as the primary storage for your documentation files. Follow these steps to create your S3 bucket:\n\nAccess the AWS Management Console: Open the Amazon S3 console at https://console.aws.amazon.com/s3/.\nCreate a New Bucket: Click on the \"Create bucket\" button. It's important to note that each bucket's name must be globally unique across all of Amazon S3.\nSet Bucket Name: Choose a unique and descriptive name for your bucket. This name will be crucial for accessing your documentation files. Remember, once a bucket name is set, it cannot be changed.\nChoose a Region: Select an AWS Region for your bucket. Choose the same region defined in your cdk.json.\nConfigure Options: You may leave the default settings or configure additional options like versioning, logging, or add tags according to your needs.\nReview and Create: Before creating the bucket, review your settings. Once everything is confirmed, click \"Create bucket\".\n\nOnce the bucket is created, update your cdk.json file with the bucket's name as shown below:\ncdk.json40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50...\n\"region\": \"us-east-2\",\n\"account\": \"\",\n\"name\": \"Lambda-Forge-Demo\",\n\"repo\": {\n    \"owner\": \"$GITHUB-USER\",\n    \"name\": \"$GITHUB-REPO\"\n},\n\"bucket\": \"$S3-BUCKET-NAME\",\n\"coverage\": 80,\n...\n\nSetting Up Documentation Endpoints\nTo activate docs generation, navigate to the deploy.py file located at infra/stages/deploy.py. Configure your endpoints as illustrated in the following example.\ninfra/stages/deploy.py 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23import aws_cdk as cdk\nfrom constructs import Construct\n\nfrom infra.stacks.lambda_stack import LambdaStack\n\n\nclass DeployStage(cdk.Stage):\n    def __init__(self, scope: Construct, context, **kwargs):\n        super().__init__(scope, context.stage, **kwargs)\n\n        lambda_stack = LambdaStack(self, context)\n\n        # Sets up a Swagger-based public endpoint at /docs\n        lambda_stack.services.api_gateway.create_docs(authorizer=None)\n\n        # Establishes a Swagger-based private endpoint at /docs/private with the 'secret' authorizer\n        lambda_stack.services.api_gateway.create_docs(authorizer=\"secret\", endpoint=\"/docs/private\")\n\n        # Configures a Redoc-based public endpoint at /docs/redoc\n        lambda_stack.services.api_gateway.create_docs(authorizer=None, endpoint=\"/docs/redoc\", redoc=True)\n\n        # Sets up a Redoc-based private endpoint at /docs/private/redoc with the 'secret' authorizer\n        lambda_stack.services.api_gateway.create_docs(authorizer=\"secret\", endpoint=\"/docs/private/redoc\", redoc=True)\n\nThis configuration enables both public and private documentation endpoints using Swagger and Redoc, making your API's documentation accessible and versatile.\nConfiguring the Pipelines to Generate the Docs\nGiven the development stage is designed for fast deployment, and serves as a sandbox environment, Lambda Forge does not generate documentation for the dev environment by default. However, if you wish to include docs generation in your development workflow, replicating the following steps for the dev environment should effectively enable this functionality.\nBecause the project was started with the --no-docs flag, it currently lacks the validate_docs and generate_docs steps in both the Staging and Production pipelines.\nIn essence, the validate_docs step ensures that all files intended for documentation are correctly configured with the necessary data classes. This step checks for completeness and accuracy in the documentation's underlying structure. On the other hand, the generate_docs step takes on the role of creating the documentation artifact itself and deploying it to the S3 bucket configured on the cdk.json file.\nTo incorporate docs generation into your project, you'll need to modify your stack configurations. Specifically, you should enable the validate_docs and generate_docs steps within your CI/CD pipeline configurations for both Staging and Production environments.\nConfiguring the Staging Pipeline\nTo turn on documentation generation for the staging environment, add validate_docs in the pipeline's pre-execution phase and generate_docs post-deployment.\ninfra/stacks/staging_stack.py39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58    # pre\n    unit_tests = steps.run_unit_tests()\n    coverage = steps.run_coverage()\n    validate_docs = steps.validate_docs()\n    validate_integration_tests = steps.validate_integration_tests()\n\n    # post\n    generate_docs = steps.generate_docs()\n    integration_tests = steps.run_integration_tests()\n\n    pipeline.add_stage(\n        DeployStage(self, context),\n        pre=[\n            unit_tests,\n            coverage,\n            validate_integration_tests,\n            validate_docs, # Validate docs enabled\n        ],\n        post=[integration_tests, generate_docs], # Generate docs enabled\n    )\n\nThe pipeline configuration will change for the staging environment. The following diagram illustrates the adjusted setup.\n\ngraph TD;\n    Source --> Build;\n    Build --> UpdatePipeline[Update Pipeline]\n    UpdatePipeline --> Assets\n    Assets --> UnitTests[Unit Tests]\n    Assets --> Coverage\n    Assets --> ValidateDocs[Validate Docs]\n    Assets --> ValidateIntegrationTests[Validate Integration Tests]\n    UnitTests --> Deploy\n    Coverage --> Deploy\n    ValidateDocs --> Deploy\n    ValidateIntegrationTests --> Deploy\n    Deploy --> IntegrationTests[Integration Tests]\n    Deploy --> GenerateDocs[Generate Docs]\n\nConfiguring the Production Pipeline\nSimilarly for the Production stack, ensure that validate_docs and generate_docs are enabled.\ninfra/stacks/prod_stack.py43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69    # pre\n    unit_tests = steps.run_unit_tests()\n    coverage = steps.run_coverage()\n    validate_docs = steps.validate_docs()\n    validate_integration_tests = steps.validate_integration_tests()\n\n    # post\n    integration_tests = steps.run_integration_tests()\n\n    pipeline.add_stage(\n        DeployStage(self, context.staging),\n        pre=[\n            unit_tests,\n            coverage,\n            validate_integration_tests,\n            validate_docs, # Validate docs enabled\n        ],\n        post=[integration_tests],\n    )\n\n    # post\n    generate_docs = steps.generate_docs()\n\n    pipeline.add_stage(\n        DeployStage(self, context),\n        post=[generate_docs], # Generate docs enabled\n    )\n\nThe pipeline configuration will also change change for the staging environment. The following diagram illustrates the adjusted setup.\n\ngraph TD;\n    Source --> Build;\n    Build --> UpdatePipeline[Update Pipeline]\n    UpdatePipeline --> Assets\n    Assets --> UnitTests[Unit Tests]\n    Assets --> Coverage\n    Assets --> ValidateDocs[Validate Docs]\n    Assets --> ValidateIntegrationTests[Validate Integration Tests]\n    UnitTests --> DeployStaging[Deploy Staging]\n    Coverage --> DeployStaging\n    ValidateDocs --> DeployStaging\n    ValidateIntegrationTests --> DeployStaging\n    DeployStaging --> IntegrationTests[Integration Tests]\n    IntegrationTests --> DeployProduction[Deploy Production]\n    DeployProduction --> GenerateDocs[Generate Docs]\n\nDeploying the Docs\nAt this point, we have all the necessary components to automatically generate our docs.\nTo proceed, commit your changes and push them to GitHub using the following commands:\n# Send your changes to stage\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"Activating docs for Staging and Production environments\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n\nAfter the pipeline completes successfully, the documentation for your API's endpoints will be available through the URLs set up in the DeployStage class. This documentation offers detailed insights into the endpoints, including their request formats, response structures, and available query parameters.\nFor easy access, the documentation for public endpoints in each environment is provided at:\n\n\nStaging Environment:\n\n\nhttps://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs\n\n\nhttps://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs/redoc\n\n\nProduction Environment:\n\nhttps://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs\nhttps://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs/redoc\n\nAccessing the private endpoints, /docs/private for Swagger and /docs/private/redoc for Redoc, necessitates the inclusion of the security token generated by the secret authorizer, as specified in the authorizers section.\n\n\n\n\n\n\n\n  Back to top\n\n\n\n\n\n\n\n      \u00a9 2024 Guilherme Alves Pimenta\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
}