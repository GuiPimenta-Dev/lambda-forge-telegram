{
  "url": "https://docs.lambda-forge.com/examples/web-scraper/",
  "title": "Web Scraper - Lambda Forge",
  "content": "\n\n\n\n\n\n\n\n\n\nWeb Scraper - Lambda Forge\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Skip to content\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Lambda Forge\n          \n\n\n\n            \n              Web Scraper\n            \n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Initializing search\n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        \n  \n    \n  \n  Home\n\n      \n\n\n\n          \n  \n  Docs\n\n        \n\n\n\n          \n  \n  Example Projects\n\n        \n\n\n\n          \n  \n  Articles\n\n        \n\n\n\n        \n  \n    \n  \n  License\n\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Lambda Forge\n  \n\n\n\n\n    Home\n  \n\n\n\n\n\n\n    Docs\n  \n\n\n\n\n\n            Docs\n          \n\n\n\n\n    Introduction\n  \n\n\n\n\n\n    Getting Started\n  \n\n\n\n\n\n    Creating a Hello World\n  \n\n\n\n\n\n    Securing Endpoints\n  \n\n\n\n\n\n    Lambda Layers\n  \n\n\n\n\n\n    Multi-Stage Environments\n  \n\n\n\n\n\n    Custom CodePipeline Steps\n  \n\n\n\n\n\n    Docs Generation\n  \n\n\n\n\n\n    Pre-Commit Hooks\n  \n\n\n\n\n\n\n\n\n\n    Example Projects\n  \n\n\n\n\n\n            Example Projects\n          \n\n\n\n\n    Introduction\n  \n\n\n\n\n\n\n    Begginer\n  \n\n\n\n\n\n            Begginer\n          \n\n\n\n\n    Guess The Number\n  \n\n\n\n\n\n    URL Shortener\n  \n\n\n\n\n\n    Image to QR Code Converter\n  \n\n\n\n\n\n\n\n\n\n    Intermediate\n  \n\n\n\n\n\n            Intermediate\n          \n\n\n\n\n    JWT Authentication\n  \n\n\n\n\n\n    OAuth2 Authentication\n  \n\n\n\n\n\n    Real-Time Chat\n  \n\n\n\n\n\n\n    Web Scraper\n  \n\n\n\n\n    Web Scraper\n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Dynamo DB\n    \n\n\n\n\n\n      Lambda Layers\n    \n\n\n\n\n\n\n      Incorporating Requests and Beautiful Soup via Public Layers\n    \n\n\n\n\n\n\n\n\n      Developing The Web Scraper\n    \n\n\n\n\n\n\n      Building a Web Scraper with Pagination Handling Using a While Loop\n    \n\n\n\n\n\n      Building a Web Scraper with Pagination Handling Using SNS\n    \n\n\n\n\n\n\n\n\n      Configuring The Web Scraper\n    \n\n\n\n\n\n      Scheduling Executions With Event Bridge\n    \n\n\n\n\n\n      Developing an Endpoint for Data Access\n    \n\n\n\n\n\n      Launching Our Web Scraper and Data Visualization Endpoint\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    Advanced\n  \n\n\n\n\n\n            Advanced\n          \n\n\n\n\n    Data Processing Pipeline\n  \n\n\n\n\n\n    Stock Price Tracker\n  \n\n\n\n\n\n\n\n\n\n\n\n\n    Articles\n  \n\n\n\n\n\n            Articles\n          \n\n\n\n\n    Custom Domain Name\n  \n\n\n\n\n\n    Tests with Lambda Forge\n  \n\n\n\n\n\n    Deploying External Library as Layers\n  \n\n\n\n\n\n    Creating S3 Buckets\n  \n\n\n\n\n\n    Locating the Base URL\n  \n\n\n\n\n\n    JSON Web Tokens\n  \n\n\n\n\n\n\n\n\n    License\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Dynamo DB\n    \n\n\n\n\n\n      Lambda Layers\n    \n\n\n\n\n\n\n      Incorporating Requests and Beautiful Soup via Public Layers\n    \n\n\n\n\n\n\n\n\n      Developing The Web Scraper\n    \n\n\n\n\n\n\n      Building a Web Scraper with Pagination Handling Using a While Loop\n    \n\n\n\n\n\n      Building a Web Scraper with Pagination Handling Using SNS\n    \n\n\n\n\n\n\n\n\n      Configuring The Web Scraper\n    \n\n\n\n\n\n      Scheduling Executions With Event Bridge\n    \n\n\n\n\n\n      Developing an Endpoint for Data Access\n    \n\n\n\n\n\n      Launching Our Web Scraper and Data Visualization Endpoint\n    \n\n\n\n\n\n\n\n\n\nCreating a Serverless Web Scraper: Integrating DynamoDB, SNS and EventBridge\nIn this section, we will develop a serverless web scraper designed to extract informations about books from https://books.toscrape.com/ utilizing the Requests library and Beautiful Soup. The retrieved data will be stored in DynamoDB, enabling us to perform queries via an endpoint.\nAdditionally, we will cover how to configure our Lambda function to execute daily, ensuring our dataset remains current and accurate.\nDynamo DB\nConsidering the write access to our database will be exclusively reserved for the scraper, maintaining three separate databases for each deployment stage is unnecessary. Therefore, let's just create a singular DynamoDB table designed to serve all three environments uniformly.\nInstead of setting up each environment's details separately in the cdk.json file, like we did to the users table, we'll make things simpler by creating a single Books table on the AWS console and placing its ARN directly into our DynamoDB class.\ninfra/services/dynamo_db.py 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18class DynamoDB:\n  def __init__(self, scope, resources: dict) -> None:\n\n      self.users_table = dynamo_db.Table.from_table_arn(\n          scope,\n          \"UsersTable\",\n          resources[\"arns\"][\"users_table\"],\n      )\n\n      self.books_table = dynamo_db.Table.from_table_arn(\n          scope,\n          \"BooksTable\",\n          \"$BOOKS-TABLE-ARN\",\n      )\n\nLambda Layers\nAnother essential aspect of our project involves leveraging external libraries like requests and Beautiful Soup for our web scraping tasks. Since these libraries are not built into Python's standard library, we'll need to incorporate them into our AWS Lambda functions as Lambda Layers.\nIncorporating Requests and Beautiful Soup via Public Layers\nThe requests and Beautiful Soup libraries are widely used and recognized for their utility in web scraping and data extraction tasks. Fortunately, AWS Lambda offers these libraries as public layers, simplifying the process of integrating them into your projects without the need to create custom layers.\nFor projects utilizing Python 3.9, we can leverage the specific Amazon Resource Names (ARNs) for both requests and Beautiful Soup libraries made available through Klayers. This provides an efficient way to add these libraries to your Lambda functions. You can explore the complete list of public layers for Python 3.9 in the us-east-2 region here.\nHere are the ARNs you'll need:\n\n\nRequests: arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-requests:19\n\n\nBeautiful Soup 4: arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-beautifulsoup4:7\n\n\nLet's add them both to our Layers class.\ninfra/services/layers.pyfrom aws_cdk import aws_lambda as _lambda\n\n\nclass Layers:\n    def __init__(self, scope) -> None:\n\n        self.requests_layer = _lambda.LayerVersion.from_layer_version_arn(\n            scope,\n            id=\"RequestsLayer\",\n            layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-requests:19\",\n        )\n\n        self.bs4_layer = _lambda.LayerVersion.from_layer_version_arn(\n            scope,\n            id=\"BS4Layer\",\n            layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-beautifulsoup4:7\",\n        )\n\nAdditionally, include the libraries in the requirements.txt file.\nrequirements.txt16\n17requests==2.28.1\nbeautifulsoup4==4.12.3\n\nDeveloping The Web Scraper\nOur web scraper will extract the following details: upc, title, price, category, stock, description and url.\nLet's create it with forge.\nforge function scraper --description \"Web scraper to populate Dynamo with books data\" --no-api --belongs-to books\n\nRemember, although users can access the scraper's results, the scraper itself won't serve as a direct endpoint. We've included the --no-api flag in our Forge setup to signify that this function won't be connected to the API Gateway. Its primary role is to enrich our database. Additionally, the --belongs-to flag was used to organize it within the books directory, aligning it with related functions planned for the future.\nHere is the structure created for the books directory:\nfunctions\n\u251c\u2500\u2500 books\n   \u251c\u2500\u2500 scraper\n   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u251c\u2500\u2500 config.py\n   \u2502   \u251c\u2500\u2500 main.py\n   \u2502   \u2514\u2500\u2500 unit.py\n   \u2514\u2500\u2500 utils\n       \u2514\u2500\u2500 __init__.py\n\nBuilding a Web Scraper with Pagination Handling Using a While Loop\nOur focus is on understanding how AWS resources are integrated with Lambda Forge, not on the intricacies of developing a web scraper. Therefore, we will not cover the source code in detail. Nevertheless, we encourage you to experiment with creating your own web scraper, as the core concepts we're discussing will remain applicable.\nBelow, you'll find the source code accompanied by comments that explain the concepts it illustrates.\nfunctions/books/scraper/main.pyimport os\nimport re\nimport boto3\nimport requests\nfrom bs4 import BeautifulSoup\n\nBASE_URL = \"https://books.toscrape.com\"\n\ndef lambda_handler(event, context):\n\n    # DynamoDB table name for storing books information\n    BOOKS_TABLE_NAME = os.environ.get(\"BOOKS_TABLE_NAME\")\n\n    # Initialize a DynamoDB resource\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table\n    books_table = dynamodb.Table(BOOKS_TABLE_NAME)\n\n    # Determine the URL to scrape, defaulting to BASE_URL\n    url = event.get(\"url\") or BASE_URL\n\n    while url:\n        # Fetch and parse the webpage at the given URL\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, \"html.parser\")\n\n        for article in soup.find_all(\"article\"):\n            # Extract book details\n            title = article.find(\"h3\").find(\"a\").get(\"title\").title()\n            price = article.find(\"p\", {\"class\": \"price_color\"}).get_text()[1:]\n\n            # Correct the href if it doesn't contain \"catalogue/\"\n            href = article.find(\"h3\").find(\"a\").get(\"href\")\n            if \"catalogue/\" not in href:\n                href = f\"catalogue/{href}\"\n\n            # Fetch and parse the book detail page\n            url = f\"{BASE_URL}/{href}\"\n            detail_response = requests.get(url)\n            detail_soup = BeautifulSoup(detail_response.text, \"html.parser\")\n\n            # Extract additional details from the book detail page\n            upc = detail_soup.find(\"th\", string=\"UPC\").find_next(\"td\").get_text().strip()\n            category = (\n                detail_soup.find(\"ul\", {\"class\", \"breadcrumb\"})\n                .find_all(\"li\")[2]\n                .text.strip()\n            )\n            stock = (\n                detail_soup.find(\"p\", {\"class\": \"instock availability\"}).get_text().strip()\n            )\n            stock = re.search(r\"\\d+\", stock)[0]\n            description = detail_soup.find(\"div\", {\"id\": \"product_description\"})\n            if description:\n                description = description.find_next(\"p\").get_text()\n\n            # Construct the item to store in DynamoDB\n            item = {\n                \"PK\": upc,\n                \"category\": category,\n                \"title\": title,\n                \"price\": price,\n                \"description\": description,\n                \"stock\": stock,\n                \"url\": url,\n            }\n\n            # Store the item in DynamoDB\n            books_table.put_item(Item=item)\n\n        # Check for and process the next page\n        next_page = soup.find(\"li\", {\"class\": \"next\"})\n        if next_page:\n            next_href = next_page.find(\"a\")[\"href\"]\n            if \"catalogue/\" not in next_href:\n                next_href = f\"catalogue/{next_href}\"\n            url = f\"{BASE_URL}/{next_href}\"\n        else:\n            url = None\n\nDue to AWS's predefined operational constraints, Lambda functions are explicitly engineered for rapid execution, with a maximum duration limit of 15 minutes.\nTo evaluate the efficiency of our function, we will incorporate print statements that monitor execution time throughout our local testing phase.\nExecution time: 1024.913999080658 seconds\n\nThe execution time approaches nearly 17 minutes, exceeding the maximum duration allowed for a Lambda function. Consequently, we need to seek alternative strategies to ensure our scraper remains compliant with the limitations.\nUtilizing a while loop within a solitary AWS Lambda function to perform book data extraction from the website is functional yet lacks efficiency and scalability. This is particularly pertinent within the AWS ecosystem, which is rich in services tailored for distributed computing and intricate task orchestration.\nBuilding a Web Scraper with Pagination Handling Using SNS\nAmazon Simple Notification Service (SNS) is a fully managed messaging service provided by AWS, enabling seamless communication between distributed systems. It operates on a publish-subscribe model, where messages are published to topics and subscribers receive notifications from these topics. With support for various types of subscriptions including HTTP, SQS, Lambda, email, and SMS, SNS ensures reliable and scalable message delivery across multiple AWS regions. It also offers features like message filtering, retry mechanisms, and dead-letter queues to enhance message processing and system resilience.\nInstead of using a while loop to process all pages in a single function, let's design a Lambda function to process a maximum of 10 pages. After completing these pages, it should dispatch a message with the URL of the next starting page to an SNS topic. This triggers another Lambda function dedicated to harvesting book information from the subsequent 10 pages.\nAs an initial step, we have to integrate SNS into our Services class.\nforge service sns\n\nA new sns.py file was created on infra/services, so create a new SNS topic on the AWS console and place it's ARN on the SNS class.\ninfra/services/sns.pyfrom aws_cdk import aws_lambda_event_sources\nimport aws_cdk.aws_sns as sns\n\n\nclass SNS:\n    def __init__(self, scope, resources, stage) -> None:\n        self.stage = stage\n\n        self.books_scraper_topic = sns.Topic.from_topic_arn(\n            scope,\n            \"BooksScraperTopic\",\n            topic_arn=\"$TOPIC-ARN\",\n        )\n\n    def create_trigger(self, topic, function, stages=None):\n        if stages and self.stage not in stages:\n            return\n\n        sns_subscription = aws_lambda_event_sources.SnsEventSource(topic)\n        function.add_event_source(sns_subscription)\n\nNote that the SNS class contains a handy helper method, streamlining the process of establishing triggers that connect an SNS topic to a Lambda function.\nNow, let's revise the original code to eliminate the while loop that processes all pages and instead publish a message to SNS containing the URL of the new starting point.\nfunctions/books/scraper/main.pyimport os\nimport re\nimport json\nimport time\nimport boto3\nimport requests\nfrom bs4 import BeautifulSoup\n\nBASE_URL = \"https://books.toscrape.com\"\n\ndef lambda_handler(event, context):\n    # Get the DynamoDB table name and SNS topic ARN from environment variables.\n    BOOKS_TABLE_NAME = os.environ.get(\"BOOKS_TABLE_NAME\", \"Books\")\n    SNS_TOPIC_ARN = os.environ.get(\"SNS_TOPIC_ARN\")\n\n    # Initialize the DynamoDB and SNS clients.\n    dynamodb = boto3.resource(\"dynamodb\")\n    sns = boto3.client(\"sns\")\n\n    # Reference the DynamoDB table.\n    books_table = dynamodb.Table(BOOKS_TABLE_NAME)\n\n    # Determine the URL to scrape, defaulting to BASE_URL\n    try:\n        url = json.loads(event['Records'][0]['Sns']['Message'].replace(\"'\", '\"'))[\"url\"]\n    except:\n        url = BASE_URL\n\n    # Keep track of the number of pages processed\n    pages_processed = 0\n\n    # Maximum number of pages to process\n    MAX_PAGES = 10\n\n    while pages_processed < MAX_PAGES:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, \"html.parser\")\n\n        for article in soup.find_all(\"article\"):\n            # Extract book details\n            title = article.find(\"h3\").find(\"a\").get(\"title\").title()\n            price = article.find(\"p\", {\"class\": \"price_color\"}).get_text()[1:]\n\n            # Correct the href if it doesn't contain \"catalogue/\"\n            href = article.find(\"h3\").find(\"a\").get(\"href\")\n            if \"catalogue/\" not in href:\n                href = f\"catalogue/{href}\"\n\n            # Fetch and parse the book detail page\n            detail_url = f\"{BASE_URL}/{href}\"\n            detail_response = requests.get(detail_url)\n            detail_soup = BeautifulSoup(detail_response.text, \"html.parser\")\n\n            # Extract additional details from the book detail page\n            upc = detail_soup.find(\"th\", string=\"UPC\").find_next(\"td\").get_text().strip()\n            category = (\n                detail_soup.find(\"ul\", {\"class\", \"breadcrumb\"})\n                .find_all(\"li\")[2]\n                .text.strip()\n            )\n            description = detail_soup.find(\"div\", {\"id\": \"product_description\"})\n            stock = (\n                detail_soup.find(\"p\", {\"class\": \"instock availability\"})\n                .get_text().strip()\n            )\n            stock = re.search(r\"\\d+\", stock)[0]\n            if description:\n                description = description.find_next(\"p\").get_text()\n\n            # Construct the item to store in DynamoDB\n            item = {\n                \"PK\": upc,\n                \"category\": category,\n                \"title\": title,\n                \"price\": price,\n                \"description\": description,\n                \"stock\": stock,\n                \"url\": detail_url,\n            }\n\n            # Store the item in DynamoDB\n            books_table.put_item(Item=item)\n\n        # Increment the number of pages processed\n        pages_processed += 1\n\n        # Check for the next page\n        next_page = soup.find(\"li\", {\"class\": \"next\"})\n        if not next_page:\n            break\n\n        # Correct the href if it doesn't contain \"catalogue/\"\n        next_href = next_page.find(\"a\")[\"href\"]\n        if \"catalogue/\" not in next_href:\n            next_href = f\"catalogue/{next_href}\"\n\n        # Construct the URL for the next page\n        url = f\"{BASE_URL}/{next_href}\"\n\n    if next_page:\n        # Publish a message to the SNS topic to process the next 10 pages\n        sns.publish(\n            TopicArn=SNS_TOPIC_ARN,\n            Message=str({\"url\": url}),\n            Subject=f\"Process next {MAX_PAGES} pages of books\",\n        )\n\nLet's measure how long that function took to run locally:\nExecution time: 167.53530287742615 seconds\n\nFantastic, it took under 3 minutes!\nThis approach ensures that we never exceed the 15 minutes timeout limit, as each time a new message is published to SNS, the timeout counter is refreshed, allowing continuous execution without interruption.\nConfiguring The Web Scraper\nNow that we have developed our function, let's proceed to configure the necessary AWS resources for its executions on the cloud.\nfunctions/books/scraper/config.pyfrom infra.services import Services\n\n\nclass ScraperConfig:\n    def __init__(self, services: Services) -> None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Scraper\",\n            path=\"./functions/books\",\n            description=\"Web scraper to populate Dynamo with books data\",\n            directory=\"scraper\",\n            timeout=5,\n            layers=[services.layers.requests_layer, services.layers.bs4_layer],\n            environment={\n                \"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name,\n                \"SNS_TOPIC_ARN\": services.sns.books_scraper_topic.topic_arn\n            }\n        )\n\n        services.dynamo_db.books_table.grant_write_data(function)\n\n        services.sns.create_trigger(services.sns.books_scraper_topic, function)\n        services.sns.books_scraper_topic.grant_publish(function)\n\nThis configuration file outlines the setup and permissions for a Lambda function, detailing:\n\nTimeout: Specifies a maximum duration of 5 minutes for Lambda execution.\nLayers: Adds the requests and bs4 layers to the Lambda function.\nEnvironment Variables: Establishes the required environment variables for operation.\nDynamoDB Access: Provides the Lambda function with write access to the DynamoDB Books table.\nSNS Trigger: Utilizes the SNS class helper method to link an SNS topic with the production Lambda function.\nSNS Publishing Permissions: Empowers the Lambda function to publish messages to the books topic.\n\nScheduling Executions With Event Bridge\nThe current configuration file equips us to execute the Lambda function as needed. However, it necessitates manual intervention for each run, which is an impractical approach for dynamic tasks like web scraping. The crux of the issue lies in the volatile nature of our target: website data, such as book prices and inventory, can change unpredictably.\nTo mitigate this, we must ensure our web scraper operates automatically at regular intervals, thus capturing updates without manual oversight. By leveraging AWS EventBridge, we can schedule our Lambda function to run periodically, ensuring our data collection remains current with minimal effort.\nTo integrate AWS EventBridge for scheduling tasks, we begin by creating an EventBridge class using Forge. This is achieved with the following command:\nforge service event_bridge\n\nAfter executing the command, a new file named event_bridge.py is generated within the infra/services directory. Let's explore its contents and functionalities:\ninfra/services/event_bridge.pyimport aws_cdk.aws_events as events\nimport aws_cdk.aws_events_targets as targets\n\n\nclass EventBridge:\n    def __init__(self, scope, resources, stage) -> None:\n        self.scope = scope\n        self.stage = stage\n\n    def create_rule(self, name, expression, target, stages=None):\n        if stages is not None and self.stage not in stages:\n            return\n\n        events.Rule(\n            self.scope,\n            name,\n            schedule=events.Schedule.expression(expression),\n            targets=[targets.LambdaFunction(handler=target)],\n        )\n\nThis class introduces a streamlined method for creating EventBridge rules, enabling the scheduling of Lambda function executions.\nBefore we proceed, it's crucial to acknowledge that we're operating within a multi-stage deployment environment. Our immediate task involves configuring the Scraper function to activate based on a scheduled rule. However, a pertinent question arises: Should we initiate the triggering of three distinct functions simultaneously? Of course not, especially when considering efficiency and resource management. More precisely, is there a need for creating three scrapers when, in reality, only one is enough to populate the database?\nBearing this consideration in mind, it's wise to implement a few minor adjustments. Our goal is to streamline the process, thereby avoiding the creation of unnecessary scrapers.\nFirst, let's modify the LambdaStack class to send also the context to the ScraperConfig class.\ninfra/stacks/lambda_stack.py42\n43        # Books\n        ScraperConfig(self.services, context)\n\nNow, let's modify our configuration class to accept the context as an additional argument in its constructor.\nBy incorporating the context, we can strategically condition the creation of the function based on the deployment stage.\nfunctions/books/scraper/config.pyfrom infra.services import Services\n\n\nclass ScraperConfig:\n    def __init__(self, services: Services, context) -> None:\n\n        if context.stage != \"Prod\":\n            return\n\n        function = services.aws_lambda.create_function(\n            name=\"Scraper\",\n            path=\"./functions/books\",\n            description=\"Web scraper to populate Dynamo with books data\",\n            directory=\"scraper\",\n            timeout=5,\n            layers=[services.layers.requests_layer, services.layers.bs4_layer],\n            environment={\n                \"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name,\n                \"SNS_TOPIC_ARN\": services.sns.books_scraper_topic.topic_arn\n            }\n        )\n\n        services.dynamo_db.books_table.grant_write_data(function)\n\n        services.sns.create_trigger(services.sns.books_scraper_topic, function)\n        services.sns.books_scraper_topic.grant_publish(function)\n\n        services.event_bridge.create_rule(\n            name=\"ScraperRule\",\n            expression=\"cron(0 12 ? * * *)\",\n            target=function,\n        )\n\nThe cron expression cron(0 12 ? * * *) configures a schedule to initiate an action every day at 12 PM UTC.\nNow, we're streamlining our deployment by creating the Lambda function and its essential resources exclusively for the staging environment that will be actively utilized.\nDeveloping an Endpoint for Data Access\nLet's create an endpoint that returns all the stored data from our database or allows filtering by category, facilitating easy access and manipulation of the data.\nforge function list_books --method \"GET\" --description \"A function to fetch books from DynamoDB, optionally filtered by category.\" --belongs-to books --public\n\nThe file has been created within the books directory, as initially planned.\nfunctions\n\u251c\u2500\u2500 books\n    \u251c\u2500\u2500 list_books\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u251c\u2500\u2500 integration.py\n    \u2502   \u251c\u2500\u2500 main.py\n    \u2502   \u2514\u2500\u2500 unit.py\n    \u251c\u2500\u2500 scraper\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u251c\u2500\u2500 main.py\n    \u2502   \u2514\u2500\u2500 unit.py\n    \u2514\u2500\u2500 utils\n        \u2514\u2500\u2500 __init__.py\n\nTo optimize data retrieval by category from our DynamoDB table, we need to create a Global Secondary Index (GSI) on the Books Table. This index enables efficient querying and filtering of data based on the category attribute, without the need for scanning the entire table.\nGo to the DynamoDB section within the AWS Management Console and select the Books Table. Click on the Indexes tab next to the table details, then press Create index. In the creation form, set the partition key to your category column. Name your index as CategoryIndex. After configuring these details, review your settings and confirm by clicking Create index.\nHaving established our index, we can utilize it to precisely and efficiently fetch data by category when needed, significantly optimizing our query performance.\nfunctions/books/list_books/main.pyimport json\nimport os\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nimport boto3\nfrom boto3.dynamodb.conditions import Key\n\n@dataclass\nclass Input:\n    category: Optional[str]\n\n@dataclass\nclass Book:\n    id: str\n    title: str\n    price: str\n    category: str\n    stock: str\n    description: str\n    url: str\n\n@dataclass\nclass Output:\n    data: List[Book]\n\ndef lambda_handler(event, context):\n    # Initialize a DynamoDB client\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Get the name of the table from the environment variable\n    BOOKS_TABLE_NAME = os.environ[\"BOOKS_TABLE_NAME\"]\n\n    # Create a DynamoDB table resource\n    table = dynamodb.Table(BOOKS_TABLE_NAME)\n\n    # Check if a category is specified in the query string parameters\n    category = (\n        event[\"queryStringParameters\"].get(\"category\")\n        if event[\"queryStringParameters\"]\n        else None\n    )\n\n    processed_items = []\n    last_evaluated_key = None\n\n    # Handle pagination\n    while True:\n        scan_kwargs = {}\n        if category:\n            scan_kwargs.update({\n                'IndexName': \"CategoryIndex\",\n                'KeyConditionExpression': Key(\"category\").eq(category.title())\n            })\n\n        if last_evaluated_key:\n            scan_kwargs['ExclusiveStartKey'] = last_evaluated_key\n\n        if category:\n            response = table.query(**scan_kwargs)\n        else:\n            response = table.scan(**scan_kwargs)\n\n        items = response.get(\"Items\", [])\n\n        # Renaming 'PK' attribute to 'id' in each item\n        processed_items.extend(\n            [{\"id\": item[\"PK\"], **{k: v for k, v in item.items() if k != \"PK\"}} for item in items]\n        )\n\n        last_evaluated_key = response.get('LastEvaluatedKey')\n        if not last_evaluated_key:\n            break\n\n    return {\"statusCode\": 200, \"body\": json.dumps({\"data\": processed_items})}\n\nNow Let's configure the function\nfunctions/books/list_books/config.pyfrom infra.services import Services\n\n\nclass ListBooksConfig:\n    def __init__(self, services: Services) -> None:\n\n        function = services.aws_lambda.create_function(\n            name=\"ListBooks\",\n            path=\"./functions/books\",\n            description=\"A function to fetch books from DynamoDB, optionally filtered by category.\",\n            directory=\"list_books\",\n            environment={\"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name},\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/books\", function, public=True)\n\n        services.dynamo_db.books_table.grant_read_data(function)\n        services.dynamo_db.add_query_permission(services.dynamo_db.books_table, function)\n\nIn addition to the foundational setup facilitated by Forge, this configuration file plays a crucial role in further customizing our function. It specifically focuses on defining environment variables and granting read permissions to the function for accessing the Books table.\nMoreover, we leverage a specialized helper method within our DynamoDB class to extend Query permissions to the Lambda function. This distinction is critical as querying entails more specific privileges beyond data reading, ensuring our function has the precise access needed for optimal operation.\nLaunching Our Web Scraper and Data Visualization Endpoint\nGreat, we're all set to deploy our function.\nNow, we'll commit and push our changes to the remote repository, allowing our CI/CD pipeline to handle the deployment seamlessly.\n# Add changes to the staging area\ngit add .\n\n# Commit the changes with a descriptive message\ngit commit -m \"Deploying Web Scraper and Data Visualization Endpoint\"\n\n# Push changes to the 'dev' branch.\ngit push origin dev\n\n# Switch to the 'staging' branch, merge changes from 'dev', and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Switch to the 'main' branch, merge changes from 'staging', and push\ngit checkout main\ngit merge staging\ngit push origin main\n\nOnce the pipeline execution concludes, expect to see a single scraper function established.\n\nAdditionally, this function will be configured with two distinct triggers: an SNS trigger and an Event Bridge trigger, each serving a unique purpose in the workflow.\n\nNow we can also test new endpoints to list the scraped data.\n\nDev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/books\nStaging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/books\nProd: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/books\n\nCongratulations! \ud83c\udf89 You've successfully created your first web scraper using Lambda Layers, SNS, DynamoDB and Event Bridge using Lambda Forge. \ud83d\ude80\n\n\n\n\n\n\n\n  Back to top\n\n\n\n\n\n\n\n      \u00a9 2024 Guilherme Alves Pimenta\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
}