[
  {
    "url": "https://docs.lambda-forge.com/home/docs-generation/",
    "title": "Docs Generation - Lambda Forge",
    "content": "Docs Generation - Lambda Forge Skip to content Lambda Forge Docs Generation Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Docs Generation Table of contents Documenting a Lambda Function Setting Up a S3 Bucket for Documentation Setting Up Documentation Endpoints Configuring the Pipelines to Generate the Docs Configuring the Staging Pipeline Configuring the Production Pipeline Deploying the Docs Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Documenting a Lambda Function Setting Up a S3 Bucket for Documentation Setting Up Documentation Endpoints Configuring the Pipelines to Generate the Docs Configuring the Staging Pipeline Configuring the Production Pipeline Deploying the Docs Generating Docs With Swagger and ReDoc Documenting a Lambda Function Inside each main.py file, you should include the Input and Output dataclasses that are going to be the entrypoint for generating the docs. Case you have an endpoint that's expecting a path parameter, you can also include it in the Path dataclass. The code snippet below demonstrates all the types of data you can expect to work with, including simple data types, lists, custom objects, optional fields, and literal types, offering a clear understanding of the input and output contracts for the API. from dataclasses import dataclass from typing import List, Optional, Literal # Define a dataclass for path parameters, useful for API endpoints requiring parameters within the URL path. @dataclass class Path: id: str # A custom object class that represents a complex type with multiple fields. @dataclass class Object: a_string: str an_int: int # The input data class represents the expected structure of the request payload. @dataclass class Input: a_string: str # A simple string input an_int: int # A simple integer input a_boolean: bool # A boolean value a_list: List[str] # A list of strings an_object: Object # An instance of the custom 'Object' class defined above a_list_of_object: List[Object] # A list containing instances of 'Object' a_literal: Literal[\"a\", \"b\", \"c\"] # A literal type, restricting values to 'a', 'b', or 'c' an_optional: Optional[str] # An optional string, which can be either a string or None # The output data class represents the endpoint's output. @dataclass class Output: pass # No fields are defined, implying the output is empty. Setting Up a S3 Bucket for Documentation Create an Amazon S3 bucket to serve as the primary storage for your documentation files. Follow these steps to create your S3 bucket: Access the AWS Management Console: Open the Amazon S3 console at https://console.aws.amazon.com/s3/. Create a New Bucket: Click on the \"Create bucket\" button. It's important to note that each bucket's name must be globally unique across all of Amazon S3. Set Bucket Name: Choose a unique and descriptive name for your bucket. This name will be crucial for accessing your documentation files. Remember, once a bucket name is set, it cannot be changed. Choose a Region: Select an AWS Region for your bucket. Choose the same region defined in your cdk.json. Configure Options: You may leave the default settings or configure additional options like versioning, logging, or add tags according to your needs. Review and Create: Before creating the bucket, review your settings. Once everything is confirmed, click \"Create bucket\". Once the bucket is created, update your cdk.json file with the bucket's name as shown below: cdk.json40 41 42 43 44 45 46 47 48 49 50... \"region\": \"us-east-2\", \"account\": \"\", \"name\": \"Lambda-Forge-Demo\", \"repo\": { \"owner\": \"$GITHUB-USER\", \"name\": \"$GITHUB-REPO\" }, \"bucket\": \"$S3-BUCKET-NAME\", \"coverage\": 80, ... Setting Up Documentation Endpoints To activate docs generation, navigate to the deploy.py file located at infra/stages/deploy.py. Configure your endpoints as illustrated in the following example. infra/stages/deploy.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23import aws_cdk as cdk from constructs import Construct from infra.stacks.lambda_stack import LambdaStack class DeployStage(cdk.Stage): def __init__(self, scope: Construct, context, **kwargs): super().__init__(scope, context.stage, **kwargs) lambda_stack = LambdaStack(self, context) # Sets up a Swagger-based public endpoint at /docs lambda_stack.services.api_gateway.create_docs(authorizer=None) # Establishes a Swagger-based private endpoint at /docs/private with the 'secret' authorizer lambda_stack.services.api_gateway.create_docs(authorizer=\"secret\", endpoint=\"/docs/private\") # Configures a Redoc-based public endpoint at /docs/redoc lambda_stack.services.api_gateway.create_docs(authorizer=None, endpoint=\"/docs/redoc\", redoc=True) # Sets up a Redoc-based private endpoint at /docs/private/redoc with the 'secret' authorizer lambda_stack.services.api_gateway.create_docs(authorizer=\"secret\", endpoint=\"/docs/private/redoc\", redoc=True) This configuration enables both public and private documentation endpoints using Swagger and Redoc, making your API's documentation accessible and versatile. Configuring the Pipelines to Generate the Docs Given the development stage is designed for fast deployment, and serves as a sandbox environment, Lambda Forge does not generate documentation for the dev environment by default. However, if you wish to include docs generation in your development workflow, replicating the following steps for the dev environment should effectively enable this functionality. Because the project was started with the --no-docs flag, it currently lacks the validate_docs and generate_docs steps in both the Staging and Production pipelines. In essence, the validate_docs step ensures that all files intended for documentation are correctly configured with the necessary data classes. This step checks for completeness and accuracy in the documentation's underlying structure. On the other hand, the generate_docs step takes on the role of creating the documentation artifact itself and deploying it to the S3 bucket configured on the cdk.json file. To incorporate docs generation into your project, you'll need to modify your stack configurations. Specifically, you should enable the validate_docs and generate_docs steps within your CI/CD pipeline configurations for both Staging and Production environments. Configuring the Staging Pipeline To turn on documentation generation for the staging environment, add validate_docs in the pipeline's pre-execution phase and generate_docs post-deployment. infra/stacks/staging_stack.py39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # pre unit_tests = steps.run_unit_tests() coverage = steps.run_coverage() validate_docs = steps.validate_docs() validate_integration_tests = steps.validate_integration_tests() # post generate_docs = steps.generate_docs() integration_tests = steps.run_integration_tests() pipeline.add_stage( DeployStage(self, context), pre=[ unit_tests, coverage, validate_integration_tests, validate_docs, # Validate docs enabled ], post=[integration_tests, generate_docs], # Generate docs enabled ) The pipeline configuration will change for the staging environment. The following diagram illustrates the adjusted setup. graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> UnitTests[Unit Tests] Assets --> Coverage Assets --> ValidateDocs[Validate Docs] Assets --> ValidateIntegrationTests[Validate Integration Tests] UnitTests --> Deploy Coverage --> Deploy ValidateDocs --> Deploy ValidateIntegrationTests --> Deploy Deploy --> IntegrationTests[Integration Tests] Deploy --> GenerateDocs[Generate Docs] Configuring the Production Pipeline Similarly for the Production stack, ensure that validate_docs and generate_docs are enabled. infra/stacks/prod_stack.py43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # pre unit_tests = steps.run_unit_tests() coverage = steps.run_coverage() validate_docs = steps.validate_docs() validate_integration_tests = steps.validate_integration_tests() # post integration_tests = steps.run_integration_tests() pipeline.add_stage( DeployStage(self, context.staging), pre=[ unit_tests, coverage, validate_integration_tests, validate_docs, # Validate docs enabled ], post=[integration_tests], ) # post generate_docs = steps.generate_docs() pipeline.add_stage( DeployStage(self, context), post=[generate_docs], # Generate docs enabled ) The pipeline configuration will also change change for the staging environment. The following diagram illustrates the adjusted setup. graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> UnitTests[Unit Tests] Assets --> Coverage Assets --> ValidateDocs[Validate Docs] Assets --> ValidateIntegrationTests[Validate Integration Tests] UnitTests --> DeployStaging[Deploy Staging] Coverage --> DeployStaging ValidateDocs --> DeployStaging ValidateIntegrationTests --> DeployStaging DeployStaging --> IntegrationTests[Integration Tests] IntegrationTests --> DeployProduction[Deploy Production] DeployProduction --> GenerateDocs[Generate Docs] Deploying the Docs At this point, we have all the necessary components to automatically generate our docs. To proceed, commit your changes and push them to GitHub using the following commands: # Send your changes to stage git add . # Commit with a descriptive message git commit -m \"Activating docs for Staging and Production environments\" # Push changes to the 'dev' branch git push origin dev # Merge 'dev' into 'staging' and push git checkout staging git merge dev git push origin staging # Finally, merge 'staging' into 'main' and push git checkout main git merge staging git push origin main After the pipeline completes successfully, the documentation for your API's endpoints will be available through the URLs set up in the DeployStage class. This documentation offers detailed insights into the endpoints, including their request formats, response structures, and available query parameters. For easy access, the documentation for public endpoints in each environment is provided at: Staging Environment: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs/redoc Production Environment: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs/redoc Accessing the private endpoints, /docs/private for Swagger and /docs/private/redoc for Redoc, necessitates the inclusion of the security token generated by the secret authorizer, as specified in the authorizers section. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/stock-price-tracker/",
    "title": "Stock Price Tracker - Lambda Forge",
    "content": "Stock Price Tracker - Lambda Forge Skip to content Lambda Forge Stock Price Tracker Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Stock Price Tracker with Event Bridge, SQS, Dynamo DB and SES Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/tests-with-lambda-forge/",
    "title": "Tests with Lambda Forge - Lambda Forge",
    "content": "Tests with Lambda Forge - Lambda Forge Skip to content Lambda Forge Tests with Lambda Forge Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Effective Software Testing with Lambda Forge Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/oauth2-authentication/",
    "title": "OAuth2 Authentication - Lambda Forge",
    "content": "OAuth2 Authentication - Lambda Forge Skip to content Lambda Forge OAuth2 Authentication Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License OAuth2 Authentication with AWS Cognito Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/examples/introduction/",
    "title": "Introduction - Lambda Forge",
    "content": "Introduction - Lambda Forge Lambda Forge Introduction Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Introduction In this guide, we'll take you on a journey through the development process with Lambda Forge, illustrating the progression of projects through a hands-on, step-by-step approach within a unified codebase. Our methodology employs an incremental build strategy, where each new feature enhances the foundation laid by preceding projects, ensuring a cohesive and scalable architecture without duplicating efforts. To keep our focus sharp on AWS resources and Lambda Forge architecture, we'll skip over the detailed discussion of unit and integration tests here. However, for those eager to dive deeper into testing methodologies, there is an insightful article called Effective Software Testing with Lambda Forge aimed at enriching your understanding about the subject. Our objective is to provide a streamlined and informative learning path, striking a balance between technical detail and approachability to keep you engaged without feeling overwhelmed. To enhance usability and the overall user experience, we've implemented a custom domain, https://api.lambda-forge.com, making our URLs succinct and memorable across various deployment stages: Dev - https://api.lambda-forge.com/dev Staging - https://api.lambda-forge.com/staging Prod - https://api.lambda-forge.com For those interested in customizing domain names within API Gateway, the How to Configure a Custom Domain Name for API Gateway guide offers a detailed tutorial on personalizing your project's URL. With that being said, let's forge some Lambdas! forge project lambda-forge-examples --repo-owner \"$GITHUB-OWNER\" --repo-name \"$GITHUB-REPO\" --bucket \"$S3-BUCKET\" Docs: https://api.lambda-forge.com/docs. Source code: https://github.com/GuiPimenta-Dev/lambda-forge-examples Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/locating-the-base-url/",
    "title": "Locating the Base URL - Lambda Forge",
    "content": "Locating the Base URL - Lambda Forge Skip to content Lambda Forge Locating the Base URL Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Finding the API Gateway Base URL This guide will walk you through the steps to locate the base URL for the API Gateway, essential for interacting with your deployed functions. Our focus will be on the function named Staging-Lambda-Forge-Demo-HelloWorld. First, navigate to the function in question. Then, access Configurations -> Triggers to uncover the URL generated upon deployment. For the purposes of our tutorial, the relevant URL is as follows: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/hello_world The BASE URL, vital for API interactions, is identified as the URL segment before the /hello_world endpoint. For our example, it's: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging With the base URL now in your possession, you're well-equipped to begin integrating your services, paving the way for seamless communication and functionality between your applications and the AWS infrastructure. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/multi-stage-environments/",
    "title": "Multi-Stage Environments - Lambda Forge",
    "content": "Multi-Stage Environments - Lambda Forge Skip to content Lambda Forge Multi-Stage Environments Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Multi-Stage Environments Table of contents Development Environment Configuring the Development Environment Development Pipeline Workflow Staging Environment Configuring the Staging Environment Deploying the Staging Environment Production Environment Configuring the Production Environment Deploying the Production Environment Overview Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Table of contents Development Environment Configuring the Development Environment Development Pipeline Workflow Staging Environment Configuring the Staging Environment Deploying the Staging Environment Production Environment Configuring the Production Environment Deploying the Production Environment Overview Multi-Stage Environments With AWS CodePipeline In practical scenarios, it is highly recommended to adopt a multi-stage development approach. This strategy allows you to freely develop and test your code in isolated environments without affecting your live production environment and, consequently, the real-world users of your application. In Lambda Forge, the pipelines for development, staging, and production are meticulously organized within distinct files, found at infra/stacks/dev_stack.py, infra/stacks/staging_stack.py, and infra/stacks/prod_stack.py, respectively. Each stage is designed to operate with its own set of isolated resources, to ensure that changes in one environment do not inadvertently affect another. Note Lambda Forge provides a suggested pipeline configuration for each stage of deployment. You're encouraged to customize these pipelines to fit your project's needs. Whether adding new steps, adjusting existing ones, reordering or even removing some of them. Development Environment The Development environment is where the initial coding and feature implementation occur, allowing developers to make frequent changes and test new ideas in an isolated environment. This environment is strategically structured to facilitate rapid deployments, allowing new features to be rolled out directly without undergoing any preliminary validation steps. It functions essentially as a sandbox environment, providing developers with a space to both develop and test new features in a fast-paced and flexible setting. This approach enables immediate feedback and iterative improvements, streamlining the development process. Configuring the Development Environment This section details the setup process for the development environment. infra/stacks/dev_stack.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36import aws_cdk as cdk from aws_cdk import pipelines as pipelines from aws_cdk.pipelines import CodePipelineSource from constructs import Construct from lambda_forge import context from infra.stages.deploy import DeployStage @context(stage=\"Dev\", resources=\"dev\") class DevStack(cdk.Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs) source = CodePipelineSource.git_hub( f\"{context.repo['owner']}/{context.repo['name']}\", \"dev\" ) pipeline = pipelines.CodePipeline( self, \"Pipeline\", synth=pipelines.ShellStep( \"Synth\", input=source, install_commands=[ \"pip install lambda-forge aws-cdk-lib\", \"npm install -g aws-cdk\", ], commands=[ \"cdk synth\", ], ), pipeline_name=f\"{context.stage}-{context.name}-Pipeline\", ) pipeline.add_stage(DeployStage(self, context)) On line 10, the context decorator assigns the stage name as Dev and configures the use of resources tagged as dev in the cdk.json file. Moreover, it imports some additional configuration variables from the cdk.json file, assigning them to the argument named context. cdk.json41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 \"region\": \"us-east-2\", \"account\": \"\", \"name\": \"Lambda-Forge-Demo\", \"repo\": { \"owner\": \"$GITHUB-OWNER\", \"name\": \"$GITHUB-REPO\" }, \"bucket\": \"\", \"coverage\": 80, \"dev\": { \"arns\": {} }, \"staging\": { \"arns\": {} }, \"prod\": { \"arns\": {} } Additionally, we incorporate the source code from the dev branch hosted on GitHub into the pipeline. Subsequently, we finalize the deployment of the Lambda functions by activating the DeployStage. Development Pipeline Workflow As the deployment of the Development Environment has been covered in previous sections, we'll not revisit those steps here. However, the diagram below succinctly illustrates the pipeline configuration established within the AWS CodePipeline. graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> Deployment Staging Environment The Staging environment serves as a near-replica of the production environment, enabling thorough testing and quality assurance processes to catch any bugs or issues before they reach the end-users. Configuring the Staging Environment Let's take a deeper look in the staging configuration file. infra/stacks/staging_stack.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56import aws_cdk as cdk from aws_cdk import pipelines as pipelines from aws_cdk.pipelines import CodePipelineSource from constructs import Construct from lambda_forge import Steps, context from infra.stages.deploy import DeployStage @context(stage=\"Staging\", resources=\"staging\") class StagingStack(cdk.Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs) source = CodePipelineSource.git_hub( f\"{context.repo['owner']}/{context.repo['name']}\", \"staging\" ) pipeline = pipelines.CodePipeline( self, \"Pipeline\", synth=pipelines.ShellStep( \"Synth\", input=source, install_commands=[ \"pip install lambda-forge aws-cdk-lib\", \"npm install -g aws-cdk\", ], commands=[ \"cdk synth\", ], ), pipeline_name=f\"{context.stage}-{context.name}-Pipeline\", ) steps = Steps(self, context, source) # pre unit_tests = steps.run_unit_tests() coverage = steps.run_coverage() validate_docs = steps.validate_docs() validate_integration_tests = steps.validate_integration_tests() # post generate_docs = steps.generate_docs() integration_tests = steps.run_integration_tests() pipeline.add_stage( DeployStage(self, context), pre=[ unit_tests, coverage, validate_integration_tests, ], post=[integration_tests], ) Similar to the Dev environment, this environment is named Staging, with resources designated as staging in the cdk.json file. We also integrate the source code from the staging branch on GitHub into the pipeline. However, in contrast to Dev, the Staging environment incorporates stringent quality assurance protocols prior to deployment. Before deploying the functions, we execute all unit tests specified in the unit.py files. Additionally, we ensure that the code coverage percentage exceeds the threshold set in the cdk.json file. We also verify that every function connected to the API Gateway is subjected to at least one integration test, identified by the custom pytest.mark.integration decorator. Once all functions have been successfully deployed, we proceed to conduct integration tests as detailed in the integration.py files. Essentially, this procedure entails dispatching an HTTP request to each of the newly deployed functions and ensuring they respond with a 200 status code. Initially, the project was initiated with the --no-docs flag, resulting in the validate_docs and generate_docs steps being created but not integrated into the pipeline. We will delve into these steps in greater depth, exploring their functionality and potential benefits in the next section. Deploying the Staging Environment First let's create and push the current code to a new branch called staging. # Stage your changes git add . # Commit with a descriptive message git commit -m \"Deploying the Staging Environment\" # Create/switch to 'staging' branch. git checkout -b staging # Push 'staging' to remote. git push origin staging Next, let's deploy the staging environment with CDK, adhering to the naming conventions established by Forge: cdk deploy Staging-Lambda-Forge-Demo-Stack This command initiates the deployment process. Shortly, AWS CodePipeline will integrate a new pipeline, specifically tailored for the staging environment. The pipeline's configuration within AWS CodePipeline is depicted below, showcasing the streamlined workflow from source code to deployment: graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> UnitTests[Unit Tests] Assets --> Coverage Assets --> ValidateIntegrationTests[Validate Integration Tests] UnitTests --> Deploy Coverage --> Deploy ValidateIntegrationTests --> Deploy Deploy --> IntegrationTests[Integration Tests] The first deployment of the Staging Pipeline often results in failure, a situation that might seem alarming but is actually expected due to the sequence in which components are deployed and tested. This phenomenon occurs because the integration tests are set to execute immediately after the deployment phase. However, during the first deployment, the BASE URL hasn't been established since it's the inaugural setup of the Staging environment. Consequently, this leads to the failure of the Integration_Test phase. Note that the failure arises after the deployment phase, indicating that the Lambda functions have been successfully deployed. To address this issue, we need to set up the base URL specifically for the integration tests. Follow the guidelines provided in the Retrieving the Api Gateway Base URL article to find your base URL. Having the BASE URL, it must then be incorporated into your cdk.json configuration file under the base_url key. This adjustment ensures that all integration tests can interact with the staging environment seamlessly for automated testing. cdk.json48 49 50 \"bucket\": \"\", \"coverage\": 80, \"base_url\": \"https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging\" Once the base URL is properly configured for the integration tests, commit your changes and push the updated code to GitHub once again. Following these adjustments, the pipeline should successfully complete its run. Production Environment The Production environment represents the phase where the tested and stable version of the software is deployed. This version is accessible to end-users and operates within the live environment. It is imperative that this stage remains the most safeguarded, permitting only fully vetted and secure code to be deployed. This precaution helps in minimizing the risk of exposing end-users to bugs or undesirable functionalities, ensuring a seamless and reliable user experience. Configuring the Production Environment import aws_cdk as cdk from aws_cdk import pipelines from aws_cdk.pipelines import CodePipelineSource from constructs import Construct from lambda_forge import Steps, context, create_context from infra.stages.deploy import DeployStage @context( stage=\"Prod\", resources=\"prod\", staging=create_context(stage=\"Staging\", resources=\"staging\"), ) class ProdStack(cdk.Stack): def __init__(self, scope: Construct, context, **kwargs) -> None: super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs) source = CodePipelineSource.git_hub( f\"{context.repo['owner']}/{context.repo['name']}\", \"main\" ) pipeline = pipelines.CodePipeline( self, \"Pipeline\", synth=pipelines.ShellStep( \"Synth\", input=source, install_commands=[ \"pip install lambda-forge aws-cdk-lib\", \"npm install -g aws-cdk\", ], commands=[ \"cdk synth\", ], ), pipeline_name=f\"{context.stage}-{context.name}-Pipeline\", ) steps = Steps(self, context.staging, source) # pre unit_tests = steps.run_unit_tests() coverage = steps.run_coverage() validate_docs = steps.validate_docs() validate_integration_tests = steps.validate_integration_tests() # post integration_tests = steps.run_integration_tests() pipeline.add_stage( DeployStage(self, context.staging), pre=[ unit_tests, coverage, validate_integration_tests, ], post=[integration_tests], ) # post generate_docs = steps.generate_docs() pipeline.add_stage( DeployStage(self, context), post=[], ) This environment is named Prod and the resources used are provenient from the prod key in the cdk.json file. Additionally, the main branch on GitHub is being used to trigger the pipeline. Given the critical need for security and integrity in production, we replicate the staging environment, applying all tests and safeguards again before deploying the production stage. This ensures that any changes meet our high quality standards before production deployment, effectively protecting against vulnerabilities and ensuring a stable user experience. Deploying the Production Environment Firstly, commit and push your code to a new branch named main on GitHub # Stage your changes git add . # Commit with a descriptive message git commit -m \"Deploying the Production Environment\" # Create/switch to 'main' branch. git checkout -b main # Push 'main' to remote. git push origin main Following the branch setup, deploy your staging environment using the AWS CDK, adhering to the naming conventions provided by Forge. cdk deploy Prod-Lambda-Forge-Demo-Stack Executing this command initiates the creation of a new pipeline in AWS CodePipeline, designed to automate your deployment process. The following diagram visually represents the configuration established in AWS CodePipeline. graph TD; Source --> Build; Build --> UpdatePipeline[Update Pipeline] UpdatePipeline --> Assets Assets --> UnitTests[Unit Tests] Assets --> Coverage Assets --> ValidateIntegrationTests[Validate Integration Tests] UnitTests --> DeployStaging[Deploy Staging] Coverage --> DeployStaging ValidateIntegrationTests --> DeployStaging DeployStaging --> IntegrationTests[Integration Tests] IntegrationTests --> DeployProduction[Deploy Production] Upon the successful completion of the pipeline execution, you'll be able to observe a new Lambda function ready and deployed within your AWS Lambda console To verify the url created, navigate to the newly deployed Lambda function in the AWS Lambda console. Within the function, proceed to Configurations -> Triggers. Here, you'll find the URL for the new endpoint that has been activated as part of the deployment process. For this tutorial, the endpoint URL provided is: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/hello_world Overview By adhering to the instructions outlined in this tutorial, you are now equipped with three distinct CI/CD pipelines. Each pipeline corresponds to a specific stage of the development lifecycle, directly linked to the dev, staging, and main branches in your GitHub repository. These pipelines ensure that changes made in each branch are automatically integrated and deployed to the appropriate environment, streamlining the process from development through to production. Furthermore, you have deployed three unique functions, each corresponding to a different environment: Dev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/hello_world Staging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/hello_world Prod: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/hello_world Each link directs you to the corresponding function deployed within its respective environment, demonstrating the successful separation and management of development, staging, and production stages through your CI/CD workflows. Congratulations! \ud83c\udf89 You've successfully deployed your Lambda function across three different environments using Lambda Forge! \ud83d\ude80 Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/articles/creating-s3-buckets/",
    "title": "Creating S3 Buckets - Lambda Forge",
    "content": "Creating S3 Buckets - Lambda Forge Skip to content Lambda Forge Creating S3 Buckets Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Creating S3 Buckets Table of contents Prerequisites Step 1: Open the Amazon S3 Console Step 2: Create a New Bucket Step 3: Access Your Bucket Best Practices Making Your Bucket Public Conclusion Locating the Base URL JSON Web Tokens License Table of contents Prerequisites Step 1: Open the Amazon S3 Console Step 2: Create a New Bucket Step 3: Access Your Bucket Best Practices Making Your Bucket Public Conclusion How to Create an Amazon S3 Bucket Amazon Simple Storage Service (Amazon S3) offers industry-leading scalability, data availability, security, and performance. This tutorial will guide you through the process of creating an S3 bucket, which can be used for a wide range of applications, including website hosting, data storage, and backups. Prerequisites You should already be logged into your AWS account. Familiarity with the basic concepts of AWS S3. Step 1: Open the Amazon S3 Console Once logged in to the AWS Management Console, locate the Services menu at the top of the console. Use the search bar to find S3 or navigate through the categories to locate S3 under the Storage section. Click on S3 to open the S3 console. Step 2: Create a New Bucket In the S3 console, click the Create bucket button. This action opens a wizard to guide you through the bucket creation process. Enter the following details: Bucket name: Choose a unique name for your bucket. This name must be globally unique across all existing bucket names in Amazon S3 and cannot be changed after the bucket is created. AWS Region: Choose the same region specified in your cdk.json file. (Optional) Configure additional options such as Versioning, Server Access Logging, Tags, and Default Encryption according to your requirements. If you're unsure or new to S3, you may proceed with the default settings. Review your settings, then click Create bucket. Step 3: Access Your Bucket After creation, your new bucket will be listed in the S3 console. Click on your bucket's name to start uploading files, creating folders, or setting up permissions. Best Practices Naming Convention: Adhere to a consistent naming convention for easier management, especially if you plan to create multiple buckets. Region Selection: Align the bucket's region with your other AWS resources to reduce latency and costs. Security: By default, all S3 buckets are private. Only make a bucket public if it is intended to serve static web content. Always follow the principle of least privilege when configuring bucket permissions. Making Your Bucket Public To make your S3 bucket public and accessible to the internet, follow these steps: Navigate to the S3 console. Select the bucket you want to make public. Go to the Permissions tab. Click on Bucket Policy. Add a policy to allow public access to your bucket. Here's an example policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"PublicReadGetObject\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME/*\" } ] } Conclusion You have successfully created an Amazon S3 bucket and learned how to make it public to serve static web content or other purposes. Remember to always consider security implications and follow best practices when configuring bucket permissions. Explore further S3 features such as lifecycle policies, object versioning, and cross-region replication to optimize your data storage strategy. Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/pre-commit-hooks/",
    "title": "Pre-Commit Hooks - Lambda Forge",
    "content": "Pre-Commit Hooks - Lambda Forge Skip to content Lambda Forge Pre-Commit Hooks Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Pre-Commit Hooks Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  },
  {
    "url": "https://docs.lambda-forge.com/home/custom-codepipeline-steps/",
    "title": "Custom CodePipeline Steps - Lambda Forge",
    "content": "Custom CodePipeline Steps - Lambda Forge Skip to content Lambda Forge Custom CodePipeline Steps Initializing search Home Docs Example Projects Articles License Lambda Forge Home Docs Docs Introduction Getting Started Creating a Hello World Securing Endpoints Lambda Layers Multi-Stage Environments Custom CodePipeline Steps Docs Generation Pre-Commit Hooks Example Projects Example Projects Introduction Begginer Begginer Guess The Number URL Shortener Image to QR Code Converter Intermediate Intermediate JWT Authentication OAuth2 Authentication Real-Time Chat Web Scraper Advanced Advanced Data Processing Pipeline Stock Price Tracker Articles Articles Custom Domain Name Tests with Lambda Forge Deploying External Library as Layers Creating S3 Buckets Locating the Base URL JSON Web Tokens License Tailoring AWS CodePipeline with Custom Steps Coming soon... Back to top \u00a9 2024 Guilherme Alves Pimenta"
  }
]